diff -Nur hive-rel-release-3.1.3/.arcconfig hive-rel-release-3.1.3-master/.arcconfig
--- hive-rel-release-3.1.3/.arcconfig	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.arcconfig	1970-01-01 08:00:00.000000000 +0800
@@ -1,15 +0,0 @@
-{
-  "project_id" : "hive",
-  "conduit_uri" : "https://reviews.facebook.net/",
-  "copyright_holder" : "Apache Software Foundation",
-  "phutil_libraries" : {
-    "arc_jira_lib" : ".arc_jira_lib"
-  },
-  "arcanist_configuration" : "ArcJIRAConfiguration",
-  "events.listeners" : ["CommitListener"],
-  "jira_base_url" : "https://issues.apache.org/jira/secure/",
-  "jira_api_url" : "https://issues.apache.org/jira/si/",
-  "jira_project" : "HIVE",
-  "lint_engine" : "JavaLintEngine",
-  "max_line_length" : "100"
-}
diff -Nur hive-rel-release-3.1.3/.checkstyle hive-rel-release-3.1.3-master/.checkstyle
--- hive-rel-release-3.1.3/.checkstyle	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.checkstyle	1970-01-01 08:00:00.000000000 +0800
@@ -1,27 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one
-  or more contributor license agreements.  See the NOTICE file
-  distributed with this work for additional information
-  regarding copyright ownership.  The ASF licenses this file
-  to you under the Apache License, Version 2.0 (the
-  "License"); you may not use this file except in compliance
-  with the License.  You may obtain a copy of the License at
- 
-      http://www.apache.org/licenses/LICENSE-2.0
- 
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
---> 
-
-<fileset-config file-format-version="1.2.0" simple-config="false">
-  <local-check-config name="Hive Checkstyle" location="checkstyle/checkstyle.xml" type="project" description="">
-    <additional-data name="protect-config-file" value="true"/>
-  </local-check-config>
-  <fileset name="Hive Java Source Files" enabled="true" check-config-name="Hive Checkstyle" local="true">
-    <file-match-pattern match-pattern="^(?!(build|ant)).*java$" include-pattern="true"/>
-  </fileset>
-</fileset-config>
diff -Nur hive-rel-release-3.1.3/errata.txt hive-rel-release-3.1.3-master/errata.txt
--- hive-rel-release-3.1.3/errata.txt	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/errata.txt	1970-01-01 08:00:00.000000000 +0800
@@ -1,99 +0,0 @@
-Commits with the wrong or no JIRA referenced:
-
-git commit                               branch     jira       url
-7981d38cbf6ef43384f5fb51b1560d7cf5add729 branch-3   HIVE-19826 https://issues.apache.org/jira/browse/HIVE-19826
-233884620af67e6af72b60629f799a69f5823eb2 master     HIVE-18627 https://issues.apache.org/jira/browse/HIVE-18627
-eb0034c0cdcc5f10fd5d7382e2caf787a8003e7a master     HIVE-17420 https://issues.apache.org/jira/browse/HIVE-17420
-f1aae85f197de09d4b86143f7f13d5aa21d2eb85 master     HIVE-16431 https://issues.apache.org/jira/browse/HIVE-16431
-cbab5b29f26ceb3d4633ade9647ce8bcb2f020a0 master     HIVE-16422 https://issues.apache.org/jira/browse/HIVE-16422
-e6143de2b0c3f53d32db8a743119e3a8080d4f85 master     HIVE-16425 https://issues.apache.org/jira/browse/HIVE-16425
-3f90794d872e90c29a068f16cdf3f45b1cf52c74 master     HIVE-15579 https://issues.apache.org/jira/browse/HIVE-15579
-5a576b6fbf1680ab4dd8f275cad484a2614ef2c1 master     HIVE-10391 https://issues.apache.org/jira/browse/HIVE-10391
-582f4e1bc39b9605d11f762480b29561a44688ae llap       HIVE-10217 https://issues.apache.org/jira/browse/HIVE-10217
-8981f365bf0cf921bc0ac2ff8914df44ca2f7de7 master     HIVE-10500 https://issues.apache.org/jira/browse/HIVE-10500
-09100831adff7589ee48e735a4beac6ebb25cb3e master     HIVE-10885 https://issues.apache.org/jira/browse/HIVE-10885
-f3ab5fda6af57afff31c29ad048d906fd095d5fb branch-1.2 HIVE-10885 https://issues.apache.org/jira/browse/HIVE-10885
-dcf21cd6fa98fb5db01ef661bb3b9f94d9ca2d15 master     HIVE-10021 https://issues.apache.org/jira/browse/HIVE-10021
-9763c9dd31bd5939db3ca50e75bb97955b411f6d master     HIVE-11536 https://issues.apache.org/jira/browse/HIVE-11536
-52a934f911c63fda5d69cb6036cb4e917c799259 llap       HIVE-11871 https://issues.apache.org/jira/browse/HIVE-11871
-1d0881e04e1aa9dd10dde8425427f29a53bee97a llap       HIVE-12125 https://issues.apache.org/jira/browse/HIVE-12125
-7f21a4254dff893ff6d882ab66e018075a37d484 llap       HIVE-12126 https://issues.apache.org/jira/browse/HIVE-12126
-a4e32580f7dd0d7cda08695af0a1feae6b175709 llap       HIVE-12127 https://issues.apache.org/jira/browse/HIVE-12127
-aebb82888e53676312a46587577ac67ad0b78579 llap       HIVE-12128 https://issues.apache.org/jira/browse/HIVE-12128
-b0860a48b75069dd24a413dca701a2685577c1cf llap       HIVE-12129 https://issues.apache.org/jira/browse/HIVE-12129
-ebb8fec397e098702e85ae68919af63f6cad2ac0 llap       HIVE-12130 https://issues.apache.org/jira/browse/HIVE-12130
-b7c53456dfa49ec08952f2f1237dffb59bd3e8a9 llap       HIVE-12131 https://issues.apache.org/jira/browse/HIVE-12131
-00045de70f85f7e8c843bc7bee7846339c9781b4 llap       HIVE-12132 https://issues.apache.org/jira/browse/HIVE-12132
-3dc2dd9c195e5985712e4fba968f12fdb1c5ec2e llap       HIVE-12133 https://issues.apache.org/jira/browse/HIVE-12133
-ac52a81f72c8ce26f13554a682ee7ae41a6a7015 llap       HIVE-12134 https://issues.apache.org/jira/browse/HIVE-12134
-b4df77b013d7e5b33c4a3eddee0c1d009e2f117a llap       HIVE-12135 https://issues.apache.org/jira/browse/HIVE-12135
-d4db62fb5f6779d9989f9a8153f1771895255982 llap       HIVE-12136 https://issues.apache.org/jira/browse/HIVE-12136
-a12191237578abbaafb35934d094dbf1278d1412 llap       HIVE-12137 https://issues.apache.org/jira/browse/HIVE-12137
-255b3a6fc621b96f32fad68437b3d8caf04823ec llap       HIVE-12138 https://issues.apache.org/jira/browse/HIVE-12138
-29cd5c8d7817eb0618e4f115329882d2c4a20417 llap       HIVE-12139 https://issues.apache.org/jira/browse/HIVE-12139
-f314e577c1f411ee5b434ba8d81e30e937e40c68 llap       HIVE-12140 https://issues.apache.org/jira/browse/HIVE-12140
-4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4 llap       HIVE-12141 https://issues.apache.org/jira/browse/HIVE-12141
-289863694d242a16cdd5e8ed82bc8b4ef460bfdc llap       HIVE-12142 https://issues.apache.org/jira/browse/HIVE-12142
-c4b2a13a5e9bccadf1ca430c2a110cbe5d68a66b llap       HIVE-12143 https://issues.apache.org/jira/browse/HIVE-12143
-8b6c34eaa02f87806e6567a27baeb56c74f94926 llap       HIVE-12144 https://issues.apache.org/jira/browse/HIVE-12144
-7b5f80f1273f82f864ff4a36c7d640021e7d3d6a llap       HIVE-12145 https://issues.apache.org/jira/browse/HIVE-12145
-7ebf999e2fdee5de00a145653a1e58de53650602 llap       HIVE-12146 https://issues.apache.org/jira/browse/HIVE-12146
-e6b1556e39f81dc2861f612733b2ba61c17ff698 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-b8acbb6ef7b97502b772569641917e6ef973b25e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-35c18a3d9b67013bf8cb2185a27391390aacc1e4 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-a31d7c8a5346fe3f26ad241e4be17a09a41583dc llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-08969c8517861d820ed353db7b1e98e9f1799d64 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-7dc3cf966745feaffef742a2ea4d74c89d44e766 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-f11222583c2b62248832010fbb7181eee369fbca llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-a7f77a73d89ee6503d5671258f74f5d7183d805e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-35a314bc1bb2e7e7b29232cb63d1c5adbe26234e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-5feb58db2c99627cb41a747a097a0ec4b019d60c llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-2dadf56692fe33e1a67f162e57ba9d36bd26b84a llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-e6965be3df0c74061c44e0a6aee5f74ce9d7c113 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-76432fbe2e463c20b0230839366f8e35a0948f0f llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-42acf2b77b0f160629d9457774f5b109bc0b1fbe llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-0737a4a5aed16374e1ee504f147c96ecc6636f6a llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-d487f800a09936562dd41b6d8a039904c14dfaff llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-ccb63a31cb5f9003221341bad592080918627565 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-74c4bdfeb03f2119e01a439cc86e384ddd2bfcde llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-371c2ba38cfd90feca4be2878daf030cf8a85bfb llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-52589882b8a69577f38dbe64a1b64e51bb5f6b52 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-f9bb03441c2e50c31d29582083d467e32bc5e088 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-e5bea303829174a4999b03bbcee5b0ad57a3bcf3 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-94f3b0590ac749b2f13c2841d0b3c47c16c5d8b7 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147
-69886819281100327ffb9527a001a7956ffc8daf llap       HIVE-12148 https://issues.apache.org/jira/browse/HIVE-12148
-055ed8dc679d0f59645f2cf1b118ab125e24d4f5 llap       HIVE-12148 https://issues.apache.org/jira/browse/HIVE-12148
-05630792e1adf24e1ead0a3b03fcf0d4af689909 llap       HIVE-12149 https://issues.apache.org/jira/browse/HIVE-12149
-744dc9c36dd50d2c7ef8b54a76aba1d4109f1b23 llap       HIVE-12149 https://issues.apache.org/jira/browse/HIVE-12149
-7775f7cbad687ee39b78538c38bb0a5c0329e076 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150
-541fcbe720df8c62e3bd4e00311c9a8c95bb12a4 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150
-53094ba7190b326d32be5e43ed4d992823c5dd4e llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150
-0f556fe3723ebb67dc22793fbfa4cc0e2e248f35 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150
-4104b2c35eaac2669e862f6703dc003e94aba0f6 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-ac4baea04ffc801bd2c972d7628deba0eb9ae4a8 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-200749619c929474333c5d540eadd3751d7ecb19 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-f27bcd9ca3a4296625079e2caf7408e855a197db llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-9b3756902e5d70f36540d11b50234c3d9a2adb39 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-7d1ea695819ccdcaa86efe8d095323b5007df7f1 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-2dfd8457b7ee415f1b28c5de2650b3f2457f20ea llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-fc6be8faf5c97901ccad33edca8f8f80023b308a llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-20acdb661a12f1bc472633d89428917275b6364d llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151
-3a2e8ee7e47bd31745dfc5f6a29c602e09747f24 llap       HIVE-12152 https://issues.apache.org/jira/browse/HIVE-12152
-8ed270cb9d8a9c49cccf99402ca92e3df3304d9f llap       HIVE-12152 https://issues.apache.org/jira/browse/HIVE-12152
-c6565f5d65da9ed5cb452db7e313d0ce7abc1105 llap       HIVE-9729  https://issues.apache.org/jira/browse/HIVE-9729
-d8298e1c85a515150562b0df68af89c18c468638 llap       HIVE-9418  https://issues.apache.org/jira/browse/HIVE-9418
-034280ce070d812f1eb312567a974a8720943647 master     HIVE-12272 https://issues.apache.org/jira/browse/HIVE-12272
-36e855084da833915dfe6c34f74e19352b64fde9 master     HIVE-12826 https://issues.apache.org/jira/browse/HIVE-12826
-9cab4414caf1bba2eb1852536a9d3676ba7eab21 master     HIVE-12827 https://issues.apache.org/jira/browse/HIVE-12827
-3734d5b674b4e8de9c0cc751650aee3194bfb93a branch-1   HIVE-12827 https://issues.apache.org/jira/browse/HIVE-12827
-22df7a8441ca85ad7f64e5191d4675f2f36a0664 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182
-223350894fe5aa653668e9f39e43218e514f2b24 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182
-5c58dceeaf662b6314eedb9afa01a2896657ef77 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182
-d16d4f1bcc43d6ebcab0eaf5bc635fb88b60be5f master     HIVE-9423  https://issues.apache.org/jira/browse/HIVE-9423
-130617443bb05d79c18420c0c4e903a76da3651c master     HIVE-14909 https://issues.apache.org/jira/browse/HIVE-14909
-6dace60af4b6ab4d5200310a0ad94c4530c2bec3 master     HIVE-13335 https://issues.apache.org/jira/browse/HIVE-13335
-5facfbb863366d7a661c21c57011b8dbe43f52e0 master     HIVE-16307 https://issues.apache.org/jira/browse/HIVE-16307
-1c3039333ba71665e8b954fbee88188757bb4050 master     HIVE-16743 https://issues.apache.org/jira/browse/HIVE-16743
-e7081035bb9768bc014f0aba11417418ececbaf0 master     HIVE-17109 https://issues.apache.org/jira/browse/HIVE-17109
-f33db1f68c68b552b9888988f818c03879749461 master     HIVE-18617 https://issues.apache.org/jira/browse/HIVE-18617
-1eea5a80ded2df33d57b2296b3bed98cb18383fd master     HIVE-19157 https://issues.apache.org/jira/browse/HIVE-19157
diff -Nur hive-rel-release-3.1.3/.gitattributes hive-rel-release-3.1.3-master/.gitattributes
--- hive-rel-release-3.1.3/.gitattributes	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.gitattributes	1970-01-01 08:00:00.000000000 +0800
@@ -1,24 +0,0 @@
-# Auto detect text files and perform LF normalization
-*        text=auto
-
-*.cs     text diff=csharp
-*.java   text diff=java
-*.html   text diff=html
-*.py     text diff=python
-*.pl     text diff=perl
-*.pm     text diff=perl
-*.css    text
-*.js     text
-*.sql    text
-*.q      text
-*.q.out  text diff
-
-*.sh     text eol=lf
-
-#test files, use lf so that size is same on windows as well
-data/files/*.dat    text eol=lf
-
-*.bat    text eol=crlf
-*.cmd    text eol=crlf
-*.csproj text merge=union eol=crlf
-*.sln    text merge=union eol=crlf
diff -Nur hive-rel-release-3.1.3/.gitignore hive-rel-release-3.1.3-master/.gitignore
--- hive-rel-release-3.1.3/.gitignore	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.gitignore	1970-01-01 08:00:00.000000000 +0800
@@ -1,33 +0,0 @@
-build
-build-eclipse
-.arc_jira_lib
-.classpath*
-.externalToolBuilders
-.project
-.settings
-*.launch
-*~
-metastore_db
-common/src/gen
-.idea
-*.iml
-*.ipr
-*.iws
-*.swp
-derby.log
-datanucleus.log
-.arc
-TempStatsStore/
-target/
-ql/TempStatsStore
-hcatalog/hcatalog-pig-adapter/target
-hcatalog/server-extensions/target
-hcatalog/core/target
-hcatalog/webhcat/java-client/target
-hcatalog/storage-handlers/hbase/target
-hcatalog/webhcat/svr/target
-conf/hive-default.xml.template
-itests/hive-blobstore/src/test/resources/blobstore-conf.xml
-.DS_Store
-patchprocess
-standalone-metastore/src/gen/version
diff -Nur hive-rel-release-3.1.3/Jenkinsfile hive-rel-release-3.1.3-master/Jenkinsfile
--- hive-rel-release-3.1.3/Jenkinsfile	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/Jenkinsfile	1970-01-01 08:00:00.000000000 +0800
@@ -1,219 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-properties([
-    // max 5 build/branch/day
-    rateLimitBuilds(throttle: [count: 5, durationName: 'day', userBoost: true]),
-    // do not run multiple testruns on the same branch
-    disableConcurrentBuilds(),
-    parameters([
-        string(name: 'SPLIT', defaultValue: '20', description: 'Number of buckets to split tests into.'),
-        string(name: 'OPTS', defaultValue: '', description: 'additional maven opts'),
-    ])
-])
-
-this.prHead = null;
-def checkPrHead() {
-  if(env.CHANGE_ID) {
-    println("checkPrHead - prHead:" + prHead)
-    println("checkPrHead - prHead2:" + pullRequest.head)
-    if (prHead == null) {
-      prHead = pullRequest.head;
-    } else {
-      if(prHead != pullRequest.head) {
-        currentBuild.result = 'ABORTED'
-        error('Found new changes on PR; aborting current build')
-      }
-    }
-  }
-}
-checkPrHead()
-
-def setPrLabel(String prLabel) {
-  if (env.CHANGE_ID) {
-   def mapping=[
-    "SUCCESS":"tests passed",
-    "UNSTABLE":"tests unstable",
-    "FAILURE":"tests failed",
-    "PENDING":"tests pending",
-   ]
-   def newLabels = []
-   for( String l : pullRequest.labels )
-     newLabels.add(l)
-   for( String l : mapping.keySet() )
-     newLabels.remove(mapping[l])
-   newLabels.add(mapping[prLabel])
-   echo ('' +newLabels)
-   pullRequest.labels=newLabels
-  }
-}
-
-setPrLabel("PENDING");
-
-def executorNode(run) {
-  hdbPodTemplate {
-    timeout(time: 24, unit: 'HOURS') {
-      node(POD_LABEL) {
-        container('hdb') {
-          run()
-        }
-      }
-    }
-  }
-}
-
-def buildHive(args) {
-  configFileProvider([configFile(fileId: 'artifactory', variable: 'SETTINGS')]) {
-    withEnv(["MULTIPLIER=$params.MULTIPLIER","M_OPTS=$params.OPTS"]) {
-      sh '''#!/bin/bash -e
-ls -l
-set -x
-. /etc/profile.d/confs.sh
-export USER="`whoami`"
-export MAVEN_OPTS="-Xmx2g"
-export -n HIVE_CONF_DIR
-cp $SETTINGS .git/settings.xml
-OPTS=" -s $PWD/.git/settings.xml -B -Dtest.groups= "
-OPTS+=" -Pitests,qsplits,dist"
-OPTS+=" -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugin.surefire.SurefirePlugin=INFO"
-OPTS+=" -Dmaven.repo.local=$PWD/.git/m2"
-git config extra.mavenOpts "$OPTS"
-OPTS=" $M_OPTS -Dmaven.test.failure.ignore "
-if [ -s inclusions.txt ]; then OPTS+=" -Dsurefire.includesFile=$PWD/inclusions.txt";fi
-if [ -s exclusions.txt ]; then OPTS+=" -Dsurefire.excludesFile=$PWD/exclusions.txt";fi
-mvn $OPTS '''+args+'''
-du -h --max-depth=1
-df -h
-'''
-    }
-  }
-}
-
-def hdbPodTemplate(closure) {
-  podTemplate(
-  containers: [
-    containerTemplate(name: 'hdb', image: 'kgyrtkirk/hive-dev-box:executor', ttyEnabled: true, command: 'cat',
-        alwaysPullImage: true,
-        resourceRequestCpu: '1800m',
-        resourceLimitCpu: '8000m',
-        resourceRequestMemory: '6400Mi',
-        resourceLimitMemory: '12000Mi'
-    ),
-  ], yaml:'''
-spec:
-  securityContext:
-    fsGroup: 1000
-  tolerations:
-    - key: "type"
-      operator: "Equal"
-      value: "slave"
-      effect: "PreferNoSchedule"
-    - key: "type"
-      operator: "Equal"
-      value: "slave"
-      effect: "NoSchedule"
-  nodeSelector:
-    type: slave
-''') {
-    closure();
-  }
-}
-
-def jobWrappers(closure) {
-  def finalLabel="FAILURE";
-  try {
-    // allocate 1 precommit token for the execution
-    lock(label:'hive-precommit', quantity:1, variable: 'LOCKED_RESOURCE')  {
-      timestamps {
-        echo env.LOCKED_RESOURCE
-        checkPrHead()
-        closure()
-      }
-    }
-    finalLabel=currentBuild.currentResult
-  } finally {
-    setPrLabel(finalLabel)
-  }
-}
-
-def saveWS() {
-  sh '''#!/bin/bash -e
-    tar --exclude=archive.tar -cf archive.tar .
-    ls -l archive.tar
-    rsync -rltDq --stats archive.tar rsync://rsync/data/$LOCKED_RESOURCE'''
-}
-
-def loadWS() {
-  sh '''#!/bin/bash -e
-    rsync -rltDq --stats rsync://rsync/data/$LOCKED_RESOURCE archive.tar
-    tar -xf archive.tar'''
-}
-
-jobWrappers {
-
-  def splits
-  executorNode {
-    container('hdb') {
-      stage('Checkout') {
-        checkout scm
-      }
-      stage('Compile') {
-        buildHive("install -Dtest=noMatches")
-      }
-      checkPrHead()
-      stage('Upload') {
-        saveWS()
-        sh '''#!/bin/bash -e
-            # make parallel-test-execution plugins source scanner happy ~ better results for 1st run
-            find . -name '*.java'|grep /Test|grep -v src/test/java|grep org/apache|while read f;do t="`echo $f|sed 's|.*org/apache|happy/src/test/java/org/apache|'`";mkdir -p  "${t%/*}";touch "$t";done
-        '''
-        splits = splitTests parallelism: count(Integer.parseInt(params.SPLIT)), generateInclusions: true, estimateTestsFromFiles: true
-      }
-    }
-  }
-
-  stage('Testing') {
-
-    def branches = [:]
-    for (int i = 0; i < splits.size(); i++) {
-      def num = i
-      def split = splits[num]
-      def splitName=String.format("split-%02d",num+1)
-      branches[splitName] = {
-        executorNode {
-          stage('Prepare') {
-              loadWS();
-              writeFile file: (split.includes ? "inclusions.txt" : "exclusions.txt"), text: split.list.join("\n")
-              writeFile file: (split.includes ? "exclusions.txt" : "inclusions.txt"), text: ''
-              sh '''echo "@INC";cat inclusions.txt;echo "@EXC";cat exclusions.txt;echo "@END"'''
-          }
-          try {
-            stage('Test') {
-              buildHive("org.apache.maven.plugins:maven-antrun-plugin:run@{define-classpath,setup-test-dirs,setup-metastore-scripts} org.apache.maven.plugins:maven-surefire-plugin:test -q")
-            }
-          } finally {
-            stage('Archive') {
-              junit '**/TEST-*.xml'
-            }
-          }
-        }
-      }
-    }
-    parallel branches
-  }
-}
diff -Nur hive-rel-release-3.1.3/LICENSE hive-rel-release-3.1.3-master/LICENSE
--- hive-rel-release-3.1.3/LICENSE	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/LICENSE	1970-01-01 08:00:00.000000000 +0800
@@ -1,407 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2017
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability contains
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
-
-APACHE HIVE SUBCOMPONENTS:
-
-The Apache Hive project contains subcomponents with separate copyright
-notices and license terms. Your use of the source code for the these
-subcomponents is subject to the terms and conditions of the following
-licenses.
-
-
-For the SQLLine package:
-
-Copyright (c) 2002, 2003, 2004, 2005 Marc Prud'hommeaux
-
-From: http://sqlline.sourceforge.net/#license
-
-"SQLLine is distributed under the BSD License, meaning that you are free to redistribute, modify, or sell the software with almost no restrictions."
-
-Statement from Marc Prud'hommeaux regarding inconsistent licenses in some SQLLine source files:
-
-> SQLLine was once GPL, but it was changed to be BSD a few years back.
-> Any references to the GPL are vestigial. Hopefully the license
-> declaration at http://sqlline.sourceforge.net/#license is sufficiently
-> authoritative in this regard.
-
-
-For the org.apache.hive.beeline.ClassNameCompleter class:
-
-Copyright (c) 2002-2006, Marc Prud'hommeaux <mwp1@cornell.edu>
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or
-without modification, are permitted provided that the following
-conditions are met:
-
-Redistributions of source code must retain the above copyright
-notice, this list of conditions and the following disclaimer.
-
-Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer
-in the documentation and/or other materials provided with
-the distribution.
-
-Neither the name of JLine nor the names of its contributors
-may be used to endorse or promote products derived from this
-software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
-BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
-AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
-EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
-OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
-PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
-AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
-LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
-IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
-OF THE POSSIBILITY OF SUCH DAMAGE.
-
-For org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque class:
-
-The BSD 3-Clause License
-
-Copyright (c) 2007, Aviad Ben Dov
-
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without modification,
-are permitted provided that the following conditions are met:
-
-1. Redistributions of source code must retain the above copyright notice, this list
-of conditions and the following disclaimer.
-2. Redistributions in binary form must reproduce the above copyright notice, this
-list of conditions and the following disclaimer in the documentation and/or other
-materials provided with the distribution.
-3. Neither the name of Infomancers, Ltd. nor the names of its contributors may be
-used to endorse or promote products derived from this software without specific
-prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
-CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
-EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
-PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-
-For jquery.sparkline.js:
-
-License: New BSD License (3-clause)
-
-Copyright (c) 2012, Splunk Inc.
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without modification,
-are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice,
-      this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright notice,
-      this list of conditions and the following disclaimer in the documentation
-      and/or other materials provided with the distribution.
-    * Neither the name of Splunk Inc nor the names of its contributors may
-      be used to endorse or promote products derived from this software without
-      specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
-EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT
-SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
-OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
-HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-For json.human.js/json.human.css:
-
-Copyright (c) 2016 Mariano Guerra
-
-Permission is hereby granted, free of charge, to any person obtaining
-a copy of this software and associated documentation files (the
-"Software"), to deal in the Software without restriction, including
-without limitation the rights to use, copy, modify, merge, publish,
-distribute, sublicense, and/or sell copies of the Software, and to
-permit persons to whom the Software is furnished to do so, subject to
-the following conditions:
-
-The above copyright notice and this permission notice shall be
-included in all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
-EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
-MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
-NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
-LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
-OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
-WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
-
-For argparse.py:
-
-argparse is (c) 2006-2009 Steven J. Bethard <steven.bethard@gmail.com>.
-
-The argparse module was contributed to Python as of Python 2.7 and thus
-was licensed under the Python license. Same license applies to all files in
-the argparse package project.
-
-For details about the Python License, please see doc/Python-License.txt.
-
-PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2
---------------------------------------------
-
-1. This LICENSE AGREEMENT is between the Python Software Foundation
-("PSF"), and the Individual or Organization ("Licensee") accessing and
-otherwise using this software ("Python") in source or binary form and
-its associated documentation.
-
-2. Subject to the terms and conditions of this License Agreement, PSF hereby
-grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,
-analyze, test, perform and/or display publicly, prepare derivative works,
-distribute, and otherwise use Python alone or in any derivative version,
-provided, however, that PSF's License Agreement and PSF's notice of copyright,
-i.e., "Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010
-Python Software Foundation; All Rights Reserved" are retained in Python alone or
-in any derivative version prepared by Licensee.
-
-3. In the event Licensee prepares a derivative work that is based on
-or incorporates Python or any part thereof, and wants to make
-the derivative work available to others as provided herein, then
-Licensee hereby agrees to include in any such work a brief summary of
-the changes made to Python.
-
-4. PSF is making Python available to Licensee on an "AS IS"
-basis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
-IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND
-DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS
-FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT
-INFRINGE ANY THIRD PARTY RIGHTS.
-
-5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON
-FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS
-A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON,
-OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.
-
-6. This License Agreement will automatically terminate upon a material
-breach of its terms and conditions.
-
-7. Nothing in this License Agreement shall be deemed to create any
-relationship of agency, partnership, or joint venture between PSF and
-Licensee.  This License Agreement does not grant permission to use PSF
-trademarks or trade name in a trademark sense to endorse or promote
-products or services of Licensee, or any third party.
-
-8. By copying, installing or otherwise using Python, Licensee
-agrees to be bound by the terms and conditions of this License
-Agreement.
-
-
diff -Nur hive-rel-release-3.1.3/NOTICE hive-rel-release-3.1.3-master/NOTICE
--- hive-rel-release-3.1.3/NOTICE	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/NOTICE	1970-01-01 08:00:00.000000000 +0800
@@ -1,7 +0,0 @@
-Apache Hive
-Copyright 2008-2019 The Apache Software Foundation
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-This project includes software licensed under the JSON license.
diff -Nur hive-rel-release-3.1.3/ql/pom.xml hive-rel-release-3.1.3-master/ql/pom.xml
--- hive-rel-release-3.1.3/ql/pom.xml	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/ql/pom.xml	2023-06-28 15:05:04.000000000 +0800
@@ -30,6 +30,9 @@
   <properties>
     <hive.path.to.root>..</hive.path.to.root>
     <powermock.version>1.6.6</powermock.version>
+    <dep.omnidata.version>1.4.0</dep.omnidata.version>
+    <dep.hetu.version>1.6.1</dep.hetu.version>
+    <dep.json.version>2.12.4</dep.json.version>
   </properties>
 
   <dependencies>
@@ -758,6 +761,70 @@
       <version>${powermock.version}</version>
       <scope>test</scope>
     </dependency>
+
+    <!--omnidata-->
+    <dependency>
+        <groupId>com.huawei.boostkit</groupId>
+        <artifactId>boostkit-omnidata-common</artifactId>
+        <version>${dep.omnidata.version}</version>
+        <classifier>aarch64</classifier>
+    </dependency>
+    <dependency>
+        <groupId>com.huawei.boostkit</groupId>
+        <artifactId>boostkit-omnidata-client</artifactId>
+        <version>${dep.omnidata.version}</version>
+        <classifier>aarch64</classifier>
+    </dependency>
+     <dependency>
+            <groupId>io.hetu.core</groupId>
+            <artifactId>presto-spi</artifactId>
+            <version>${dep.hetu.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>io.hetu.core</groupId>
+            <artifactId>presto-main</artifactId>
+            <version>${dep.hetu.version}</version>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>slf4j-jdk14</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>log4j-over-slf4j</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+      <dependency>
+            <groupId>io.airlift</groupId>
+            <artifactId>slice</artifactId>
+            <version>0.38</version>
+        </dependency>
+      <dependency>
+            <groupId>com.fasterxml.jackson.module</groupId>
+            <artifactId>jackson-module-parameter-names</artifactId>
+            <version>${dep.json.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.fasterxml.jackson.datatype</groupId>
+            <artifactId>jackson-datatype-guava</artifactId>
+            <version>${dep.json.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.fasterxml.jackson.datatype</groupId>
+            <artifactId>jackson-datatype-jdk8</artifactId>
+            <version>${dep.json.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.fasterxml.jackson.datatype</groupId>
+            <artifactId>jackson-datatype-joda</artifactId>
+            <version>${dep.json.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.fasterxml.jackson.datatype</groupId>
+            <artifactId>jackson-datatype-jsr310</artifactId>
+            <version>${dep.json.version}</version>
+        </dependency>
   </dependencies>
 
   <profiles>
@@ -951,7 +1018,7 @@
                   <include>io.airlift:aircompressor</include>
                   <include>org.codehaus.jackson:jackson-core-asl</include>
                   <include>org.codehaus.jackson:jackson-mapper-asl</include>
-                  <include>com.google.guava:guava</include>
+                  <!--<include>com.google.guava:guava</include>-->
                   <include>net.sf.opencsv:opencsv</include>
                   <include>org.apache.hive:hive-spark-client</include>
                   <include>org.apache.hive:hive-storage-api</include>
@@ -959,6 +1026,18 @@
                   <include>org.apache.orc:orc-shims</include>
                   <include>org.apache.orc:orc-tools</include>
                   <include>joda-time:joda-time</include>
+                  <include>com.huawei.boostkit:boostkit-omnidata-common</include>
+                  <include>com.huawei.boostkit:boostkit-omnidata-client</include>
+                  <include>io.hetu.core:presto-spi</include>
+                  <include>io.hetu.core:presto-main</include>
+                  <include>org.slf4j:slf4j-jdk14</include>
+                  <include>org.slf4j:log4j-over-slf4j</include>
+                  <include>io.airlift:slice</include>
+                  <include>com.fasterxml.jackson.module:jackson-module-parameter-names</include>
+                  <include>com.fasterxml.jackson.datatype:jackson-datatype-guava</include>
+                  <include>com.fasterxml.jackson.datatype:jackson-datatype-jdk8</include>
+                  <include>com.fasterxml.jackson.datatype:jackson-datatype-joda</include>
+                  <include>com.fasterxml.jackson.datatype:jackson-datatype-jsr310</include>
                 </includes>
               </artifactSet>
               <relocations>
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordSource.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordSource.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordSource.java	2023-07-29 19:32:52.336665640 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordSource.java	2023-06-28 15:05:04.000000000 +0800
@@ -18,19 +18,38 @@
 
 package org.apache.hadoop.hive.ql.exec.tez;
 
-import java.io.IOException;
 
-import org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+
+import com.huawei.boostkit.omnidata.decode.type.DecodeType;
 import org.apache.hadoop.hive.ql.exec.AbstractMapOperator;
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
+import org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.omnidata.OmniDataUtils;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.hive.ql.omnidata.decode.PageDeserializer;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.apache.hadoop.hive.ql.omnidata.reader.OmniDataReader;
+import org.apache.hadoop.hive.ql.omnidata.serialize.NdpSerializationUtils;
+import org.apache.hadoop.hive.ql.plan.TableScanDesc;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.split.TezGroupedSplit;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.tez.mapreduce.lib.MRReader;
+import org.apache.tez.mapreduce.lib.MRReaderMapred;
 import org.apache.tez.runtime.library.api.KeyValueReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.*;
 
 /**
  * Process input from tez LogicalInput and write output - for a map plan Just pump the records
@@ -39,89 +58,178 @@
 
 public class MapRecordSource implements RecordSource {
 
-  public static final Logger LOG = LoggerFactory.getLogger(MapRecordSource.class);
-  private ExecMapperContext execContext = null;
-  private AbstractMapOperator mapOp = null;
-  private KeyValueReader reader = null;
-  private final boolean grouped = false;
-
-  void init(JobConf jconf, AbstractMapOperator mapOp, KeyValueReader reader) throws IOException {
-    execContext = mapOp.getExecContext();
-    this.mapOp = mapOp;
-    if (reader instanceof KeyValueInputMerger) {
-      KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;
-      kvMerger.setIOCxt(execContext.getIoCxt());
-    }
-    this.reader = reader;
-  }
-
-  @Override
-  public final boolean isGrouped() {
-    return grouped;
-  }
-
-  @Override
-  public boolean pushRecord() throws HiveException {
-    execContext.resetRow();
-
-    try {
-      if (reader.next()) {
-        Object value;
+    private final Logger LOG = LoggerFactory.getLogger(MapRecordSource.class);
+
+    /**
+     * Maximum number of concurrent connections in the omnidata thread pool
+     */
+    public static final int MAX_THREAD_NUMS = 16;
+
+    private ExecMapperContext execContext;
+
+    private AbstractMapOperator mapOp;
+
+    private KeyValueReader reader;
+
+    private final boolean grouped = false;
+
+    private TypeInfo[] rowColumnTypeInfos;
+
+    private ArrayList<FileSplit> fileSplits = new ArrayList<>();
+
+    private NdpPredicateInfo ndpPredicateInfo;
+
+    private boolean isOmniDataPushDown = false;
+
+    private JobConf jconf;
+
+    private ExecutorService executorService;
+
+    void init(JobConf jconf, AbstractMapOperator mapOp, KeyValueReader reader) throws IOException {
+        execContext = mapOp.getExecContext();
+        this.mapOp = mapOp;
+        if (reader instanceof KeyValueInputMerger) {
+            KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;
+            kvMerger.setIOCxt(execContext.getIoCxt());
+        }
+        this.reader = reader;
+        if (mapOp.getChildOperators().get(0).getConf() instanceof TableScanDesc) {
+            TableScanDesc tableScanDesc = (TableScanDesc) mapOp.getChildOperators().get(0).getConf();
+            this.ndpPredicateInfo = NdpSerializationUtils.deserializeNdpPredicateInfo(
+                    tableScanDesc.getNdpPredicateInfoStr());
+            this.isOmniDataPushDown = ndpPredicateInfo.getIsPushDown();
+            if (isOmniDataPushDown) {
+                this.jconf = jconf;
+                this.rowColumnTypeInfos = tableScanDesc.getRowColumnTypeInfos();
+                initOmniData();
+            }
+        }
+    }
+
+    /**
+     * Save InputSplit to fileSplits.
+     * Create a thread pool (executorService) based on parameters.
+     */
+    private void initOmniData() {
+        List<InputSplit> inputSplits = ((TezGroupedSplit) ((MRReaderMapred) reader).getSplit()).getGroupedSplits();
+        int splitSize = inputSplits.size();
+        // init fileSplits
+        inputSplits.forEach(is -> fileSplits.add((FileSplit) is));
+        int threadNums = OmniDataConf.getOmniDataOptimizedThreadNums(jconf);
+        // Need to limit the maximum number of threads to avoid out of memory.
+        threadNums = Math.min(splitSize, threadNums);
+        // init executorService
+        executorService = new ThreadPoolExecutor(threadNums, threadNums, 0L, TimeUnit.MILLISECONDS,
+                new LinkedBlockingDeque<>(),
+                new ThreadFactoryBuilder().setNameFormat("omnidata-hive-optimized-thread-%d").build());
+    }
+
+    @Override
+    public final boolean isGrouped() {
+        return grouped;
+    }
+
+    @Override
+    public boolean pushRecord() throws HiveException {
+        execContext.resetRow();
+        if (isOmniDataPushDown) {
+            return pushOmniDataRecord();
+        } else {
+            return pushRawRecord();
+        }
+    }
+
+    private boolean pushOmniDataRecord() throws HiveException {
+        // create DecodeType
+        DecodeType[] columnTypes = new DecodeType[ndpPredicateInfo.getDecodeTypes().size()];
+        for (int index = 0; index < columnTypes.length; index++) {
+            String codeType = ndpPredicateInfo.getDecodeTypes().get(index);
+            // If the returned data type is Agg, use transOmniDataAggDecodeType() method.
+            if (ndpPredicateInfo.getDecodeTypesWithAgg().get(index)) {
+                columnTypes[index] = OmniDataUtils.transOmniDataAggDecodeType(codeType);
+            } else {
+                columnTypes[index] = OmniDataUtils.transOmniDataDecodeType(codeType);
+            }
+        }
+        PageDeserializer deserializer = new PageDeserializer(columnTypes);
+        ArrayList<Future<Queue<VectorizedRowBatch>>> results = new ArrayList<>();
+        fileSplits.forEach(fs -> {
+            Future<Queue<VectorizedRowBatch>> future = executorService.submit(
+                    new OmniDataReader(jconf, fs, deserializer, ndpPredicateInfo, rowColumnTypeInfos));
+            results.add(future);
+        });
+        for (Future<Queue<VectorizedRowBatch>> future : results) {
+            try {
+                Queue<VectorizedRowBatch> rowBatches = future.get();
+                for (VectorizedRowBatch rowBatch : rowBatches) {
+                    mapOp.process(rowBatch);
+                }
+            } catch (InterruptedException | ExecutionException ex) {
+                ex.printStackTrace();
+            }
+        }
+        executorService.shutdown();
+        return false;
+    }
+
+    private boolean pushRawRecord() throws HiveException {
         try {
-          value = reader.getCurrentValue();
+            if (reader.next()) {
+                Object value;
+                try {
+                    value = reader.getCurrentValue();
+                } catch (IOException e) {
+                    closeReader();
+                    throw new HiveException(e);
+                }
+                return processRow(value);
+            }
         } catch (IOException e) {
-          closeReader();
-          throw new HiveException(e);
+            closeReader();
+            throw new HiveException(e);
         }
-        return processRow(value);
-      }
-    } catch (IOException e) {
-      closeReader();
-      throw new HiveException(e);
-    }
-    return false;
-  }
-
-  private boolean processRow(Object value) {
-    try {
-      if (mapOp.getDone()) {
-        return false; // done
-      } else {
-        // Since there is no concept of a group, we don't invoke
-        // startGroup/endGroup for a mapper
-        mapOp.process((Writable) value);
-      }
-    } catch (Throwable e) {
-      if (e instanceof OutOfMemoryError) {
-        // Don't create a new object if we are already out of memory
-        throw (OutOfMemoryError) e;
-      } else {
-        LOG.error(StringUtils.stringifyException(e));
-        closeReader();
-        throw new RuntimeException(e);
-      }
-    }
-    return true; // give me more
-  }
-
-  private void closeReader() {
-    if (!(reader instanceof MRReader)) {
-      LOG.warn("Cannot close " + (reader == null ? null : reader.getClass()));
-      return;
-    }
-    if (reader instanceof KeyValueInputMerger) {
-      // cleanup
-      KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;
-      kvMerger.clean();
-    }
-
-    LOG.info("Closing MRReader on error");
-    MRReader mrReader = (MRReader)reader;
-    try {
-      mrReader.close();
-    } catch (IOException ex) {
-      LOG.error("Failed to close the reader; ignoring", ex);
+        return false;
     }
-  }
 
-}
+    private boolean processRow(Object value) {
+        try {
+            if (mapOp.getDone()) {
+                return false; // done
+            } else {
+                // Since there is no concept of a group, we don't invoke
+                // startGroup/endGroup for a mapper
+                mapOp.process((Writable) value);
+            }
+        } catch (Throwable e) {
+            if (e instanceof OutOfMemoryError) {
+                // Don't create a new object if we are already out of memory
+                throw (OutOfMemoryError) e;
+            } else {
+                LOG.error(StringUtils.stringifyException(e));
+                closeReader();
+                throw new RuntimeException(e);
+            }
+        }
+        return true; // give me more
+    }
+
+    private void closeReader() {
+        if (!(reader instanceof MRReader)) {
+            LOG.warn("Cannot close " + (reader == null ? null : reader.getClass()));
+            return;
+        }
+        if (reader instanceof KeyValueInputMerger) {
+            // cleanup
+            KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;
+            kvMerger.clean();
+        }
+
+        LOG.info("Closing MRReader on error");
+        MRReader mrReader = (MRReader) reader;
+        try {
+            mrReader.close();
+        } catch (IOException ex) {
+            LOG.error("Failed to close the reader; ignoring", ex);
+        }
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java	2023-07-29 19:32:52.336665640 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java	2023-06-28 15:05:04.000000000 +0800
@@ -18,24 +18,17 @@
 
 package org.apache.hadoop.hive.ql.exec.tez;
 
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-import java.util.concurrent.ConcurrentHashMap;
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Multimap;
 
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveInputFormat;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.mapred.FileSplit;
@@ -45,10 +38,19 @@
 import org.apache.hadoop.mapred.split.TezGroupedSplit;
 import org.apache.hadoop.mapred.split.TezMapredSplitsGrouper;
 import org.apache.tez.dag.api.TaskLocationHint;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
-import com.google.common.collect.ArrayListMultimap;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Multimap;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
 
 /**
  * SplitGrouper is used to combine splits based on head room and locality. It
@@ -56,255 +58,267 @@
  */
 public class SplitGrouper {
 
-  private static final Logger LOG = LoggerFactory.getLogger(SplitGrouper.class);
+    private static final Logger LOG = LoggerFactory.getLogger(SplitGrouper.class);
 
-  // TODO This needs to be looked at. Map of Map to Map... Made concurrent for now since split generation
-  // can happen in parallel.
-  private static final Map<Map<Path, PartitionDesc>, Map<Path, PartitionDesc>> cache =
-      new ConcurrentHashMap<>();
-
-  private final TezMapredSplitsGrouper tezGrouper = new TezMapredSplitsGrouper();
-
-  /**
-   * group splits for each bucket separately - while evenly filling all the
-   * available slots with tasks
-   */
-  public Multimap<Integer, InputSplit> group(Configuration conf,
-      Multimap<Integer, InputSplit> bucketSplitMultimap, int availableSlots, float waves,
-                                             SplitLocationProvider splitLocationProvider)
-      throws IOException {
-
-    // figure out how many tasks we want for each bucket
-    Map<Integer, Integer> bucketTaskMap =
-        estimateBucketSizes(availableSlots, waves, bucketSplitMultimap.asMap());
-
-    // allocate map bucket id to grouped splits
-    Multimap<Integer, InputSplit> bucketGroupedSplitMultimap =
-        ArrayListMultimap.<Integer, InputSplit> create();
-
-    // use the tez grouper to combine splits once per bucket
-    for (int bucketId : bucketSplitMultimap.keySet()) {
-      Collection<InputSplit> inputSplitCollection = bucketSplitMultimap.get(bucketId);
-
-      InputSplit[] rawSplits = inputSplitCollection.toArray(new InputSplit[0]);
-      InputSplit[] groupedSplits =
-          tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId),
-              HiveInputFormat.class.getName(), new ColumnarSplitSizeEstimator(), splitLocationProvider);
-
-      LOG.info("Original split count is " + rawSplits.length + " grouped split count is "
-          + groupedSplits.length + ", for bucket: " + bucketId);
-
-      for (InputSplit inSplit : groupedSplits) {
-        bucketGroupedSplitMultimap.put(bucketId, inSplit);
-      }
-    }
-
-    return bucketGroupedSplitMultimap;
-  }
+    // TODO This needs to be looked at. Map of Map to Map... Made concurrent for now since split generation
+    // can happen in parallel.
+    private static final Map<Map<Path, PartitionDesc>, Map<Path, PartitionDesc>> cache =
+            new ConcurrentHashMap<>();
+
+    private final TezMapredSplitsGrouper tezGrouper = new TezMapredSplitsGrouper();
+
+    /**
+     * group splits for each bucket separately - while evenly filling all the
+     * available slots with tasks
+     */
+    public Multimap<Integer, InputSplit> group(Configuration conf,
+                                               Multimap<Integer, InputSplit> bucketSplitMultimap, int availableSlots, float waves,
+                                               SplitLocationProvider splitLocationProvider)
+            throws IOException {
+
+        // figure out how many tasks we want for each bucket
+        Map<Integer, Integer> bucketTaskMap =
+                estimateBucketSizes(availableSlots, waves, bucketSplitMultimap.asMap());
+
+        // allocate map bucket id to grouped splits
+        Multimap<Integer, InputSplit> bucketGroupedSplitMultimap =
+                ArrayListMultimap.<Integer, InputSplit> create();
+
+        // use the tez grouper to combine splits once per bucket
+        for (int bucketId : bucketSplitMultimap.keySet()) {
+            Collection<InputSplit> inputSplitCollection = bucketSplitMultimap.get(bucketId);
+
+            InputSplit[] rawSplits = inputSplitCollection.toArray(new InputSplit[0]);
+            InputSplit[] groupedSplits =
+                    tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId),
+                            HiveInputFormat.class.getName(), new ColumnarSplitSizeEstimator(), splitLocationProvider);
 
+            LOG.info("Original split count is " + rawSplits.length + " grouped split count is "
+                    + groupedSplits.length + ", for bucket: " + bucketId);
 
-  /**
-   * Create task location hints from a set of input splits
-   * @param splits the actual splits
-   * @param consistentLocations whether to re-order locations for each split, if it's a file split
-   * @return taskLocationHints - 1 per input split specified
-   * @throws IOException
-   */
-  public List<TaskLocationHint> createTaskLocationHints(InputSplit[] splits, boolean consistentLocations) throws IOException {
-
-    List<TaskLocationHint> locationHints = Lists.newArrayListWithCapacity(splits.length);
-
-    for (InputSplit split : splits) {
-      String rack = (split instanceof TezGroupedSplit) ? ((TezGroupedSplit) split).getRack() : null;
-      if (rack == null) {
-        String [] locations = split.getLocations();
-        if (locations != null && locations.length > 0) {
-          // Worthwhile only if more than 1 split, consistentGroupingEnabled and is a FileSplit
-          if (consistentLocations && locations.length > 1 && split instanceof FileSplit) {
-            Arrays.sort(locations);
-            FileSplit fileSplit = (FileSplit) split;
-            Path path = fileSplit.getPath();
-            long startLocation = fileSplit.getStart();
-            int hashCode = Objects.hash(path, startLocation);
-            int startIndex = hashCode % locations.length;
-            LinkedHashSet<String> locationSet = new LinkedHashSet<>(locations.length);
-            // Set up the locations starting from startIndex, and wrapping around the sorted array.
-            for (int i = 0 ; i < locations.length ; i++) {
-              int index = (startIndex + i) % locations.length;
-              locationSet.add(locations[index]);
+            for (InputSplit inSplit : groupedSplits) {
+                bucketGroupedSplitMultimap.put(bucketId, inSplit);
             }
-            locationHints.add(TaskLocationHint.createTaskLocationHint(locationSet, null));
-          } else {
-            locationHints.add(TaskLocationHint
-                .createTaskLocationHint(new LinkedHashSet<String>(Arrays.asList(split
-                    .getLocations())), null));
-          }
-        } else {
-          locationHints.add(TaskLocationHint.createTaskLocationHint(null, null));
-        }
-      } else {
-        locationHints.add(TaskLocationHint.createTaskLocationHint(null, Collections.singleton(rack)));
-      }
+        }
+
+        return bucketGroupedSplitMultimap;
     }
 
-    return locationHints;
-  }
 
-  /** Generate groups of splits, separated by schema evolution boundaries */
-  public Multimap<Integer, InputSplit> generateGroupedSplits(JobConf jobConf,
-                                                                    Configuration conf,
-                                                                    InputSplit[] splits,
-                                                                    float waves, int availableSlots,
-                                                                    SplitLocationProvider locationProvider)
-      throws Exception {
-    return generateGroupedSplits(jobConf, conf, splits, waves, availableSlots, null, true, locationProvider);
-  }
-
-  /** Generate groups of splits, separated by schema evolution boundaries */
-  public Multimap<Integer, InputSplit> generateGroupedSplits(JobConf jobConf,
-                                                                    Configuration conf,
-                                                                    InputSplit[] splits,
-                                                                    float waves, int availableSlots,
-                                                                    String inputName,
-                                                                    boolean groupAcrossFiles,
-                                                                    SplitLocationProvider locationProvider) throws
-      Exception {
-
-    MapWork work = populateMapWork(jobConf, inputName);
-    // ArrayListMultimap is important here to retain the ordering for the splits.
-    Multimap<Integer, InputSplit> bucketSplitMultiMap =
-        ArrayListMultimap.<Integer, InputSplit> create();
-
-    int i = 0;
-    InputSplit prevSplit = null;
-    for (InputSplit s : splits) {
-      // this is the bit where we make sure we don't group across partition
-      // schema boundaries
-      if (schemaEvolved(s, prevSplit, groupAcrossFiles, work)) {
-        ++i;
-        prevSplit = s;
-      }
-      bucketSplitMultiMap.put(i, s);
-    }
-    LOG.info("# Src groups for split generation: " + (i + 1));
+    /**
+     * Create task location hints from a set of input splits
+     * @param splits the actual splits
+     * @param consistentLocations whether to re-order locations for each split, if it's a file split
+     * @return taskLocationHints - 1 per input split specified
+     * @throws IOException
+     */
+    public List<TaskLocationHint> createTaskLocationHints(InputSplit[] splits, boolean consistentLocations) throws IOException {
+
+        List<TaskLocationHint> locationHints = Lists.newArrayListWithCapacity(splits.length);
+
+        for (InputSplit split : splits) {
+            String rack = (split instanceof TezGroupedSplit) ? ((TezGroupedSplit) split).getRack() : null;
+            if (rack == null) {
+                String [] locations = split.getLocations();
+                if (locations != null && locations.length > 0) {
+                    // Worthwhile only if more than 1 split, consistentGroupingEnabled and is a FileSplit
+                    if (consistentLocations && locations.length > 1 && split instanceof FileSplit) {
+                        Arrays.sort(locations);
+                        FileSplit fileSplit = (FileSplit) split;
+                        Path path = fileSplit.getPath();
+                        long startLocation = fileSplit.getStart();
+                        int hashCode = Objects.hash(path, startLocation);
+                        int startIndex = hashCode % locations.length;
+                        LinkedHashSet<String> locationSet = new LinkedHashSet<>(locations.length);
+                        // Set up the locations starting from startIndex, and wrapping around the sorted array.
+                        for (int i = 0 ; i < locations.length ; i++) {
+                            int index = (startIndex + i) % locations.length;
+                            locationSet.add(locations[index]);
+                        }
+                        locationHints.add(TaskLocationHint.createTaskLocationHint(locationSet, null));
+                    } else {
+                        locationHints.add(TaskLocationHint
+                                .createTaskLocationHint(new LinkedHashSet<String>(Arrays.asList(split
+                                        .getLocations())), null));
+                    }
+                } else {
+                    locationHints.add(TaskLocationHint.createTaskLocationHint(null, null));
+                }
+            } else {
+                locationHints.add(TaskLocationHint.createTaskLocationHint(null, Collections.singleton(rack)));
+            }
+        }
 
-    // group them into the chunks we want
-    Multimap<Integer, InputSplit> groupedSplits =
-        this.group(jobConf, bucketSplitMultiMap, availableSlots, waves, locationProvider);
-
-    return groupedSplits;
-  }
-
-
-  /**
-   * get the size estimates for each bucket in tasks. This is used to make sure
-   * we allocate the head room evenly
-   */
-  private Map<Integer, Integer> estimateBucketSizes(int availableSlots, float waves,
-                                                    Map<Integer, Collection<InputSplit>> bucketSplitMap) {
-
-    // mapping of bucket id to size of all splits in bucket in bytes
-    Map<Integer, Long> bucketSizeMap = new HashMap<Integer, Long>();
-
-    // mapping of bucket id to number of required tasks to run
-    Map<Integer, Integer> bucketTaskMap = new HashMap<Integer, Integer>();
-
-    // TODO HIVE-12255. Make use of SplitSizeEstimator.
-    // The actual task computation needs to be looked at as well.
-    // compute the total size per bucket
-    long totalSize = 0;
-    boolean earlyExit = false;
-    for (int bucketId : bucketSplitMap.keySet()) {
-      long size = 0;
-      for (InputSplit s : bucketSplitMap.get(bucketId)) {
-        // the incoming split may not be a file split when we are re-grouping TezGroupedSplits in
-        // the case of SMB join. So in this case, we can do an early exit by not doing the
-        // calculation for bucketSizeMap. Each bucket will assume it can fill availableSlots * waves
-        // (preset to 0.5) for SMB join.
-        if (!(s instanceof FileSplit)) {
-          bucketTaskMap.put(bucketId, (int) (availableSlots * waves));
-          earlyExit = true;
-          continue;
-        }
-        FileSplit fsplit = (FileSplit) s;
-        size += fsplit.getLength();
-        totalSize += fsplit.getLength();
-      }
-      bucketSizeMap.put(bucketId, size);
+        return locationHints;
     }
 
-    if (earlyExit) {
-      return bucketTaskMap;
-    }
+    /** Generate groups of splits, separated by schema evolution boundaries */
+    public Multimap<Integer, InputSplit> generateGroupedSplits(JobConf jobConf,
+                                                               Configuration conf,
+                                                               InputSplit[] splits,
+                                                               float waves, int availableSlots,
+                                                               SplitLocationProvider locationProvider)
+            throws Exception {
+        return generateGroupedSplits(jobConf, conf, splits, waves, availableSlots, null, true, locationProvider);
+    }
+
+    /** Generate groups of splits, separated by schema evolution boundaries */
+    public Multimap<Integer, InputSplit> generateGroupedSplits(JobConf jobConf,
+                                                               Configuration conf,
+                                                               InputSplit[] splits,
+                                                               float waves, int availableSlots,
+                                                               String inputName,
+                                                               boolean groupAcrossFiles,
+                                                               SplitLocationProvider locationProvider) throws
+            Exception {
+
+        MapWork work = populateMapWork(jobConf, inputName);
+        if (work != null && work.getAliasToWork().size() == 1) {
+            work.getAliasToWork().values().forEach(t -> {
+                if (t instanceof TableScanOperator) {
+                    OmniDataConf.setOmniDataAggOptimizedEnabled(jobConf,
+                            ((TableScanOperator) t).getConf().isPushDownAgg());
+                    OmniDataConf.setOmniDataFilterOptimizedEnabled(jobConf,
+                            ((TableScanOperator) t).getConf().isPushDownFilter());
+                    OmniDataConf.setOmniDataTableOptimizedSelectivity(jobConf,
+                            ((TableScanOperator) t).getConf().getOmniDataSelectivity());
+                }
+            });
+        }
+        // ArrayListMultimap is important here to retain the ordering for the splits.
+        Multimap<Integer, InputSplit> bucketSplitMultiMap =
+                ArrayListMultimap.<Integer, InputSplit> create();
+
+        int i = 0;
+        InputSplit prevSplit = null;
+        for (InputSplit s : splits) {
+            // this is the bit where we make sure we don't group across partition
+            // schema boundaries
+            if (schemaEvolved(s, prevSplit, groupAcrossFiles, work)) {
+                ++i;
+                prevSplit = s;
+            }
+            bucketSplitMultiMap.put(i, s);
+        }
+        LOG.info("# Src groups for split generation: " + (i + 1));
 
-    // compute the number of tasks
-    for (int bucketId : bucketSizeMap.keySet()) {
-      int numEstimatedTasks = 0;
-      if (totalSize != 0) {
-        // availableSlots * waves => desired slots to fill
-        // sizePerBucket/totalSize => weight for particular bucket. weights add
-        // up to 1.
-        numEstimatedTasks =
-            (int) (availableSlots * waves * bucketSizeMap.get(bucketId) / totalSize);
-      }
-
-      LOG.info("Estimated number of tasks: " + numEstimatedTasks + " for bucket " + bucketId);
-      if (numEstimatedTasks == 0) {
-        numEstimatedTasks = 1;
-      }
-      bucketTaskMap.put(bucketId, numEstimatedTasks);
+        // group them into the chunks we want
+        Multimap<Integer, InputSplit> groupedSplits =
+                this.group(jobConf, bucketSplitMultiMap, availableSlots, waves, locationProvider);
+
+        return groupedSplits;
     }
 
-    return bucketTaskMap;
-  }
 
-  private static MapWork populateMapWork(JobConf jobConf, String inputName) {
-    MapWork work = null;
-    if (inputName != null) {
-      work = (MapWork) Utilities.getMergeWork(jobConf, inputName);
-      // work can still be null if there is no merge work for this input
-    }
-    if (work == null) {
-      work = Utilities.getMapWork(jobConf);
-    }
+    /**
+     * get the size estimates for each bucket in tasks. This is used to make sure
+     * we allocate the head room evenly
+     */
+    private Map<Integer, Integer> estimateBucketSizes(int availableSlots, float waves,
+                                                      Map<Integer, Collection<InputSplit>> bucketSplitMap) {
+
+        // mapping of bucket id to size of all splits in bucket in bytes
+        Map<Integer, Long> bucketSizeMap = new HashMap<Integer, Long>();
+
+        // mapping of bucket id to number of required tasks to run
+        Map<Integer, Integer> bucketTaskMap = new HashMap<Integer, Integer>();
+
+        // TODO HIVE-12255. Make use of SplitSizeEstimator.
+        // The actual task computation needs to be looked at as well.
+        // compute the total size per bucket
+        long totalSize = 0;
+        boolean earlyExit = false;
+        for (int bucketId : bucketSplitMap.keySet()) {
+            long size = 0;
+            for (InputSplit s : bucketSplitMap.get(bucketId)) {
+                // the incoming split may not be a file split when we are re-grouping TezGroupedSplits in
+                // the case of SMB join. So in this case, we can do an early exit by not doing the
+                // calculation for bucketSizeMap. Each bucket will assume it can fill availableSlots * waves
+                // (preset to 0.5) for SMB join.
+                if (!(s instanceof FileSplit)) {
+                    bucketTaskMap.put(bucketId, (int) (availableSlots * waves));
+                    earlyExit = true;
+                    continue;
+                }
+                FileSplit fsplit = (FileSplit) s;
+                size += fsplit.getLength();
+                totalSize += fsplit.getLength();
+            }
+            bucketSizeMap.put(bucketId, size);
+        }
 
-    return work;
-  }
+        if (earlyExit) {
+            return bucketTaskMap;
+        }
+
+        // compute the number of tasks
+        for (int bucketId : bucketSizeMap.keySet()) {
+            int numEstimatedTasks = 0;
+            if (totalSize != 0) {
+                // availableSlots * waves => desired slots to fill
+                // sizePerBucket/totalSize => weight for particular bucket. weights add
+                // up to 1.
+                numEstimatedTasks =
+                        (int) (availableSlots * waves * bucketSizeMap.get(bucketId) / totalSize);
+            }
+
+            LOG.info("Estimated number of tasks: " + numEstimatedTasks + " for bucket " + bucketId);
+            if (numEstimatedTasks == 0) {
+                numEstimatedTasks = 1;
+            }
+            bucketTaskMap.put(bucketId, numEstimatedTasks);
+        }
 
-  private boolean schemaEvolved(InputSplit s, InputSplit prevSplit, boolean groupAcrossFiles,
-                                       MapWork work) throws IOException {
-    boolean retval = false;
-    Path path = ((FileSplit) s).getPath();
-    PartitionDesc pd = HiveFileFormatUtils.getFromPathRecursively(
-        work.getPathToPartitionInfo(), path, cache);
-    String currentDeserializerClass = pd.getDeserializerClassName();
-    Class<?> currentInputFormatClass = pd.getInputFileFormatClass();
-
-    Class<?> previousInputFormatClass = null;
-    String previousDeserializerClass = null;
-    if (prevSplit != null) {
-      Path prevPath = ((FileSplit) prevSplit).getPath();
-      if (!groupAcrossFiles) {
-        return !path.equals(prevPath);
-      }
-      PartitionDesc prevPD =
-          HiveFileFormatUtils.getFromPathRecursively(work.getPathToPartitionInfo(),
-              prevPath, cache);
-      previousDeserializerClass = prevPD.getDeserializerClassName();
-      previousInputFormatClass = prevPD.getInputFileFormatClass();
+        return bucketTaskMap;
     }
 
-    if ((currentInputFormatClass != previousInputFormatClass)
-        || (!currentDeserializerClass.equals(previousDeserializerClass))) {
-      retval = true;
+    private static MapWork populateMapWork(JobConf jobConf, String inputName) {
+        MapWork work = null;
+        if (inputName != null) {
+            work = (MapWork) Utilities.getMergeWork(jobConf, inputName);
+            // work can still be null if there is no merge work for this input
+        }
+        if (work == null) {
+            work = Utilities.getMapWork(jobConf);
+        }
+
+        return work;
     }
 
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Adding split " + path + " to src new group? " + retval);
+    private boolean schemaEvolved(InputSplit s, InputSplit prevSplit, boolean groupAcrossFiles,
+                                  MapWork work) throws IOException {
+        boolean retval = false;
+        Path path = ((FileSplit) s).getPath();
+        PartitionDesc pd = HiveFileFormatUtils.getFromPathRecursively(
+                work.getPathToPartitionInfo(), path, cache);
+        String currentDeserializerClass = pd.getDeserializerClassName();
+        Class<?> currentInputFormatClass = pd.getInputFileFormatClass();
+
+        Class<?> previousInputFormatClass = null;
+        String previousDeserializerClass = null;
+        if (prevSplit != null) {
+            Path prevPath = ((FileSplit) prevSplit).getPath();
+            if (!groupAcrossFiles) {
+                return !path.equals(prevPath);
+            }
+            PartitionDesc prevPD =
+                    HiveFileFormatUtils.getFromPathRecursively(work.getPathToPartitionInfo(),
+                            prevPath, cache);
+            previousDeserializerClass = prevPD.getDeserializerClassName();
+            previousInputFormatClass = prevPD.getInputFileFormatClass();
+        }
+
+        if ((currentInputFormatClass != previousInputFormatClass)
+                || (!currentDeserializerClass.equals(previousDeserializerClass))) {
+            retval = true;
+        }
+
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("Adding split " + path + " to src new group? " + retval);
+        }
+        return retval;
     }
-    return retval;
-  }
 
 
 
-}
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/config/OmniDataConf.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/config/OmniDataConf.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/config/OmniDataConf.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/config/OmniDataConf.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,411 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.config;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.exec.tez.MapRecordSource;
+
+/**
+ * OmniData hive configuration
+ *
+ * @since 2021-11-09
+ */
+public class OmniDataConf {
+    /**
+     * Whether to enable OmniData
+     */
+    public static final String OMNIDATA_HIVE_ENABLED = "omnidata.hive.enabled";
+
+    /**
+     * Whether to enable filter selectivity
+     */
+    public static final String OMNIDATA_HIVE_FILTER_SELECTIVITY_ENABLED = "omnidata.hive.filter.selectivity.enabled";
+
+    /**
+     * Threshold of table size
+     */
+    public static final String OMNIDATA_HIVE_TABLE_SIZE_THRESHOLD = "omnidata.hive.table.size.threshold";
+
+    /**
+     * Filter selectivity
+     */
+    public static final String OMNIDATA_HIVE_FILTER_SELECTIVITY = "omnidata.hive.filter.selectivity";
+
+    /**
+     * Zookeeper quorum server
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_QUORUM_SERVER = "omnidata.hive.zookeeper.quorum.server";
+
+    /**
+     * Zookeeper status node info
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_STATUS_NODE = "omnidata.hive.zookeeper.status.node";
+
+    /**
+     * Zookeeper conf path
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_CONF_PATH = "omnidata.hive.zookeeper.conf.path";
+
+    /**
+     * Whether to enable Zookeeper security
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_SECURITY_ENABLED = "omnidata.hive.zookeeper.security.enabled";
+
+    /**
+     * Zookeeper connection timeout interval
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_CONNECTION_TIMEOUT_MS =
+            "omnidata.hive.zookeeper.connection.timeoutMs";
+
+    /**
+     * Zookeeper session timeout interval
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_SESSION_TIMEOUT_MS = "omnidata.hive.zookeeper.session.timeoutMs";
+
+    /**
+     * Zookeeper retry interval
+     */
+    public static final String OMNIDATA_HIVE_ZOOKEEPER_RETRY_INTERVAL_MS = "omnidata.hive.zookeeper.retry.intervalMs";
+
+    /**
+     * OmniData optimized thread numbers
+     */
+    public static final String OMNIDATA_HIVE_OPTIMIZED_THREAD_NUMS = "omnidata.hive.optimized.thread.nums";
+
+    /**
+     * Whether to enable OmniData agg optimized
+     */
+    public static final String OMNIDATA_HIVE_AGG_OPTIMIZED_ENABLED = "omnidata.hive.agg.optimized.enabled";
+
+    /**
+     * Whether to enable OmniData filter optimized
+     */
+    public static final String OMNIDATA_HIVE_FILTER_OPTIMIZED_ENABLED = "omnidata.hive.filter.optimized.enabled";
+
+    /**
+     * Whether to enable OmniData reduce optimized
+     */
+    public static final String OMNIDATA_HIVE_REDUCE_OPTIMIZED_ENABLED = "omnidata.hive.reduce.optimized.enabled";
+
+    /**
+     * Whether there are tables that can be pushed down
+     */
+    public static final String OMNIDATA_HIVE_EXISTS_TABLE_PUSHDOWN = "omnidata.hive.exists.table.pushdown";
+
+    /**
+     * OmniData group optimized coefficient,
+     * If the filter type is string or char, the selectivity is inaccurate.
+     * You can manually modify this parameter to optimize the selectivity.
+     */
+    public static final String OMNIDATA_HIVE_GROUP_OPTIMIZED_COEFFICIENT = "omnidata.hive.group.optimized.coefficient";
+
+    /**
+     * Whether to enable OmniData group optimized
+     */
+    public static final String OMNIDATA_HIVE_GROUP_OPTIMIZED_ENABLED = "omnidata.hive.group.optimized.enabled";
+
+    /**
+     * OmniData table optimized selectivity
+     */
+    public static final String OMNIDATA_HIVE_TABLE_OPTIMIZED_SELECTIVITY = "omnidata.hive.table.optimized.selectivity";
+
+    /**
+     * Hdfs's replication, default:3
+     */
+    public static final String DFS_REPLICATION = "dfs.replication";
+
+    /**
+     *omnidata task.timeout, default:300
+     */
+    public static final String OMNIDATA_CLIENT_TASK_TIMEOUT = "omnidata.client.task.timeout";
+
+    /**
+     * get the value of a parameter: omnidata.hive.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_ENABLED, false);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.filter.selectivity.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataFilterSelectivityEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_FILTER_SELECTIVITY_ENABLED, true);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.table.size.threshold
+     *
+     * @param conf hive conf
+     * @return threshold
+     */
+    public static int getOmniDataTablesSizeThreshold(Configuration conf) {
+        return conf.getInt(OMNIDATA_HIVE_TABLE_SIZE_THRESHOLD, 102400);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.filter.selectivity
+     *
+     * @param conf hive conf
+     * @return filter selectivity
+     */
+    public static Double getOmniDataFilterSelectivity(Configuration conf) {
+        double selectivity = conf.getDouble(OMNIDATA_HIVE_FILTER_SELECTIVITY, 0.2);
+        checkArgument(selectivity >= 0 && selectivity <= 1.0,
+                String.format("The %s value must be in [0.0, 1.0].", OMNIDATA_HIVE_FILTER_SELECTIVITY));
+        return selectivity;
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.quorum.server
+     *
+     * @param conf hive conf
+     * @return Zookeeper quorum server
+     */
+    public static String getOmniDataZookeeperQuorumServer(Configuration conf) {
+        return conf.get(OMNIDATA_HIVE_ZOOKEEPER_QUORUM_SERVER);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.status.node
+     *
+     * @param conf hive conf
+     * @return status node info
+     */
+    public static String getOmniDataZookeeperStatusNode(Configuration conf) {
+        return conf.get(OMNIDATA_HIVE_ZOOKEEPER_STATUS_NODE);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.conf.path
+     *
+     * @param conf hive conf
+     * @return conf path
+     */
+    public static String getOmniDataZookeeperConfPath(Configuration conf) {
+        return conf.get(OMNIDATA_HIVE_ZOOKEEPER_CONF_PATH);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.security.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataZookeeperSecurityEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_ZOOKEEPER_SECURITY_ENABLED, true);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.connection.timeoutMs
+     *
+     * @param conf hive conf
+     * @return connection timeout interval
+     */
+    public static int getOmniDataZookeeperConnectionTimeout(Configuration conf) {
+        int timeout = conf.getInt(OMNIDATA_HIVE_ZOOKEEPER_CONNECTION_TIMEOUT_MS, 15000);
+        checkArgument(timeout > 0,
+                String.format("The %s value must be positive", OMNIDATA_HIVE_ZOOKEEPER_CONNECTION_TIMEOUT_MS));
+        return timeout;
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.session.timeoutMs
+     *
+     * @param conf hive conf
+     * @return session timeout interval
+     */
+    public static int getOmniDataZookeeperSessionTimeout(Configuration conf) {
+        int timeout = conf.getInt(OMNIDATA_HIVE_ZOOKEEPER_SESSION_TIMEOUT_MS, 60000);
+        checkArgument(timeout > 0,
+                String.format("The %s value must be positive", OMNIDATA_HIVE_ZOOKEEPER_SESSION_TIMEOUT_MS));
+        return timeout;
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.zookeeper.retry.intervalMs
+     *
+     * @param conf hive conf
+     * @return retry interval
+     */
+    public static int getOmniDataZookeeperRetryInterval(Configuration conf) {
+        int retryInterval = conf.getInt(OMNIDATA_HIVE_ZOOKEEPER_RETRY_INTERVAL_MS, 1000);
+        checkArgument(retryInterval > 0,
+                String.format("The %s value must be positive", OMNIDATA_HIVE_ZOOKEEPER_RETRY_INTERVAL_MS));
+        return retryInterval;
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.optimized.thread.nums
+     *
+     * @param conf hive conf
+     * @return MAX_THREAD_NUMS
+     */
+    public static int getOmniDataOptimizedThreadNums(Configuration conf) {
+        return conf.getInt(OMNIDATA_HIVE_OPTIMIZED_THREAD_NUMS, MapRecordSource.MAX_THREAD_NUMS);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.agg.optimized.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataAggOptimizedEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_AGG_OPTIMIZED_ENABLED, false);
+    }
+
+    /**
+     * set the value of a parameter: omnidata.hive.agg.optimized.enabled
+     *
+     * @param conf hive conf
+     * @param isOptimized true or false
+     */
+    public static void setOmniDataAggOptimizedEnabled(Configuration conf, boolean isOptimized) {
+        conf.setBoolean(OmniDataConf.OMNIDATA_HIVE_AGG_OPTIMIZED_ENABLED, isOptimized);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.filter.optimized.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataFilterOptimizedEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_FILTER_OPTIMIZED_ENABLED, false);
+    }
+
+    /**
+     * set the value of a parameter: omnidata.hive.filter.optimized.enabled
+     *
+     * @param conf hive conf
+     * @param isOptimized true or false
+     */
+    public static void setOmniDataFilterOptimizedEnabled(Configuration conf, boolean isOptimized) {
+        conf.setBoolean(OmniDataConf.OMNIDATA_HIVE_FILTER_OPTIMIZED_ENABLED, isOptimized);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.reduce.optimized.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataReduceOptimizedEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_REDUCE_OPTIMIZED_ENABLED, false);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.exists.table.pushdown
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataExistsTablePushDown(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_EXISTS_TABLE_PUSHDOWN, false);
+    }
+
+    /**
+     * set the value of a parameter: omnidata.hive.exists.table.pushdown
+     *
+     * @param conf hive conf
+     * @param isExists true or false
+     */
+    public static void setOmniDataExistsTablePushDown(Configuration conf, boolean isExists) {
+        conf.setBoolean(OmniDataConf.OMNIDATA_HIVE_EXISTS_TABLE_PUSHDOWN, isExists);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.group.optimized.enabled
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static Boolean getOmniDataGroupOptimizedEnabled(Configuration conf) {
+        return conf.getBoolean(OMNIDATA_HIVE_GROUP_OPTIMIZED_ENABLED, false);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.group.optimized.coefficient
+     *
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static double getOmniDataGroupOptimizedCoefficient(Configuration conf) {
+        return conf.getDouble(OMNIDATA_HIVE_GROUP_OPTIMIZED_COEFFICIENT, -1);
+    }
+
+    /**
+     * get the value of a parameter: omnidata.hive.table.optimized.selectivity
+     *
+     * @param conf hive conf
+     * @return optimized selectivity
+     */
+    public static Double getOmniDataTableOptimizedSelectivity(Configuration conf) {
+        double selectivity = conf.getDouble(OMNIDATA_HIVE_TABLE_OPTIMIZED_SELECTIVITY, 1.0);
+        checkArgument(selectivity >= 0 && selectivity <= 1.0,
+                String.format("The %s value must be in [0.0, 1.0].", OMNIDATA_HIVE_TABLE_OPTIMIZED_SELECTIVITY));
+        return selectivity;
+    }
+
+    /**
+     * set the value of a parameter: omnidata.hive.table.optimized.selectivity
+     *
+     * @param conf hive conf
+     * @param value optimized selectivity
+     */
+    public static void setOmniDataTableOptimizedSelectivity(Configuration conf, double value) {
+        checkArgument(value >= 0 && value <= 1.0,
+                String.format("The %s value must be in [0.0, 1.0].", OMNIDATA_HIVE_TABLE_OPTIMIZED_SELECTIVITY));
+        conf.setDouble(OmniDataConf.OMNIDATA_HIVE_TABLE_OPTIMIZED_SELECTIVITY, value);
+    }
+
+    /**
+     * get the value of a parameter: dfs.replication
+     *
+     * @param conf hive conf
+     * @return replication
+     */
+    public static int getOmniDataReplicationNum(Configuration conf) {
+        int replicationNum = conf.getInt(DFS_REPLICATION, 3);
+        checkArgument(replicationNum > 0, String.format("The %s value must be positive", DFS_REPLICATION));
+        return replicationNum;
+    }
+
+    /**
+     * get the value of omnidata client parameter: omnidata.client.task.timeout
+     *
+     * @param conf hive conf
+     * @return timeout
+     */
+    public static int getOmnidataClientTaskTimeout(Configuration conf) {
+        int timeout = conf.getInt(OMNIDATA_CLIENT_TASK_TIMEOUT, 300);
+        checkArgument(timeout > 0, String.format("The %s value must be positive", OMNIDATA_CLIENT_TASK_TIMEOUT));
+        return timeout;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDecoding.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDecoding.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDecoding.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDecoding.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,581 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.decode;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static io.airlift.slice.SizeOf.SIZE_OF_INT;
+import static java.lang.Double.longBitsToDouble;
+import static java.lang.Float.intBitsToFloat;
+
+import com.huawei.boostkit.omnidata.decode.AbstractDecoding;
+import com.huawei.boostkit.omnidata.decode.type.*;
+import io.airlift.slice.SliceInput;
+import io.airlift.slice.Slices;
+import io.prestosql.spi.type.DateType;
+
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Optional;
+
+/**
+ * PageDecoding
+ */
+public class PageDecoding extends AbstractDecoding<ColumnVector[]> {
+    private final int BATCH_SIZE = VectorizedRowBatch.DEFAULT_SIZE;
+
+    @Override
+    public ColumnVector[] decodeArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public ColumnVector[] decodeByteArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] longColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = sliceInput.readByte();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            longColumnVectors[i] = longColumnVector;
+        }
+        return longColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeBooleanArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        return decodeByteArray(type, sliceInput);
+    }
+
+    @Override
+    public ColumnVector[] decodeDictionary(Optional<DecodeType> type, SliceInput sliceInput) {
+        throw new UnsupportedOperationException();
+    }
+
+    // int
+    @Override
+    public ColumnVector[] decodeIntArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] intColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = sliceInput.readInt();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            intColumnVectors[i] = longColumnVector;
+        }
+        return intColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeInt128Array(Optional<DecodeType> type, SliceInput sliceInput) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public ColumnVector[] decodeShortArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] shortColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = sliceInput.readShort();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            shortColumnVectors[i] = longColumnVector;
+        }
+        return shortColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeLongArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] longColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = sliceInput.readLong();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            longColumnVectors[i] = longColumnVector;
+        }
+        return longColumnVectors;
+    }
+
+    // float
+    @Override
+    public ColumnVector[] decodeFloatArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] floatColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    doubleColumnVector.vector[position] = intBitsToFloat(sliceInput.readInt());
+                    doubleColumnVector.isNull[position] = false;
+                } else {
+                    doubleColumnVector.isNull[position] = true;
+                    doubleColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            floatColumnVectors[i] = doubleColumnVector;
+        }
+        return floatColumnVectors;
+    }
+
+    // double
+    @Override
+    public ColumnVector[] decodeDoubleArray(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] doubleColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    doubleColumnVector.vector[position] = longBitsToDouble(sliceInput.readLong());
+                    doubleColumnVector.isNull[position] = false;
+                } else {
+                    doubleColumnVector.isNull[position] = true;
+                    doubleColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            doubleColumnVectors[i] = doubleColumnVector;
+        }
+        return doubleColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeMap(Optional<DecodeType> type, SliceInput sliceInput) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public ColumnVector[] decodeSingleMap(Optional<DecodeType> type, SliceInput sliceInput) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public ColumnVector[] decodeVariableWidth(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+        int[] offsets = new int[positionCount + 1];
+        sliceInput.readBytes(Slices.wrappedIntArray(offsets), SIZE_OF_INT,  Math.multiplyExact(positionCount, SIZE_OF_INT));
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+        int blockSize = sliceInput.readInt();
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] bytesColumnVectors = new ColumnVector[batchCount];
+        int curOffset = offsets[0];
+        int nextOffset;
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            BytesColumnVector bytesColumnVector = new BytesColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    nextOffset = offsets[batchPosition + position + 1];
+                    int length = nextOffset - curOffset;
+                    curOffset = nextOffset;
+                    byte[] bytes = new byte[length];
+                    sliceInput.readBytes(bytes, 0, length);
+                    bytesColumnVector.vector[position] = bytes;
+                    bytesColumnVector.length[position] = length;
+                    bytesColumnVector.isNull[position] = false;
+                } else {
+                    bytesColumnVector.isNull[position] = true;
+                    bytesColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            bytesColumnVectors[i] = bytesColumnVector;
+        }
+        return bytesColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeRunLength(Optional<DecodeType> type, SliceInput sliceInput)
+            throws InvocationTargetException, IllegalAccessException {
+        // read the run length
+        int positionCount = sliceInput.readInt();
+        // decode first
+        ColumnVector[] columnVectors = decode(type, sliceInput);
+        ColumnVector[] resColumnVectors = null;
+        String decodeName = typeToDecodeRunLengthName(type);
+        if (columnVectors == null || decodeName == null) {
+            throw new IllegalAccessException();
+        }
+        checkArgument(columnVectors.length == 1, "unanticipated results");
+        //        throw new UnsupportedOperationException("Decoding run length is not supported.");
+        //TODO No test scenario is found. To be verified.
+        Map<String, Method> decompressMethods = new HashMap<>();
+        Method[] methods = PageDeRunLength.class.getDeclaredMethods();
+        for (Method method : methods) {
+            decompressMethods.put(method.getName(), method);
+        }
+        Method method = decompressMethods.get(decodeName);
+
+        PageDeRunLength pageDeRunLength = new PageDeRunLength();
+        Object objResult = method.invoke(pageDeRunLength, positionCount, columnVectors[0]);
+        if (objResult instanceof ColumnVector[]) {
+            resColumnVectors = (ColumnVector[]) objResult;
+        }
+        return resColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeRow(Optional<DecodeType> type, SliceInput sliceInput) {
+        return null;
+    }
+
+    @Override
+    public ColumnVector[] decodeDate(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] longColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = sliceInput.readInt();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            longColumnVectors[i] = longColumnVector;
+        }
+        return longColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeLongToInt(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] intColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = (int) sliceInput.readLong();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            intColumnVectors[i] = longColumnVector;
+        }
+        return intColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeLongToShort(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] shortColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = (short) sliceInput.readLong();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            shortColumnVectors[i] = longColumnVector;
+        }
+        return shortColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeLongToByte(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] longColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    longColumnVector.vector[position] = (byte) sliceInput.readLong();
+                    longColumnVector.isNull[position] = false;
+                } else {
+                    longColumnVector.isNull[position] = true;
+                    longColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            longColumnVectors[i] = longColumnVector;
+        }
+        return longColumnVectors;
+    }
+
+    @Override
+    public ColumnVector[] decodeLongToFloat(Optional<DecodeType> type, SliceInput sliceInput) {
+        int positionCount = sliceInput.readInt();
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        int batchPosition = 0;
+
+        boolean[] valueIsNull = decodeNullBits(sliceInput, positionCount).orElse(null);
+
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        ColumnVector[] floatColumnVectors = new ColumnVector[batchCount];
+        for (int i = 0; i < batchCount; i++) {
+            if (batchCount - 1 == i) {
+                loopPositionCount = remainderCount;
+            }
+            DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+            for (int position = 0; position < loopPositionCount; position++) {
+                if (valueIsNull == null || !valueIsNull[batchPosition + position]) {
+                    doubleColumnVector.vector[position] = intBitsToFloat((int) sliceInput.readLong());
+                    doubleColumnVector.isNull[position] = false;
+                } else {
+                    doubleColumnVector.isNull[position] = true;
+                    doubleColumnVector.noNulls = false;
+                }
+            }
+            batchPosition += BATCH_SIZE;
+            floatColumnVectors[i] = doubleColumnVector;
+        }
+        return floatColumnVectors;
+    }
+
+    private String typeToDecodeRunLengthName(Optional<DecodeType> optType) {
+        Class<?> javaType = null;
+        if (!optType.isPresent()) {
+            return null;
+        }
+        DecodeType type = optType.get();
+        if (type.getJavaType().isPresent()) {
+            javaType = type.getJavaType().get();
+        }
+        if (javaType == double.class) {
+            return "decompressDoubleArray";
+        }
+        if (javaType == float.class || javaType == LongToFloatDecodeType.class) {
+            return "decompressFloatArray";
+        }
+        if (javaType == int.class || javaType == LongToIntDecodeType.class || javaType == DateType.class) {
+            return "decompressIntArray";
+        }
+        if (javaType == long.class) {
+            return "decompressLongArray";
+        }
+        if (javaType == byte.class || javaType == LongToByteDecodeType.class) {
+            return "decompressByteArray";
+        }
+        if (javaType == boolean.class) {
+            return "decompressBooleanArray";
+        }
+        if (javaType == short.class || javaType == LongToShortDecodeType.class) {
+            return "decompressShortArray";
+        }
+        if (javaType == String.class) {
+            return "decompressVariableWidth";
+        }
+        return null;
+    }
+    @Override
+    public ColumnVector[] decodeDecimal(Optional<DecodeType> optional, SliceInput sliceInput, String s) {
+        return null;
+    }
+
+    @Override
+    public ColumnVector[] decodeTimestamp(Optional<DecodeType> optional, SliceInput sliceInput) {
+        return null;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeRunLength.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeRunLength.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeRunLength.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeRunLength.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,351 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.decode;
+
+import org.apache.hadoop.hive.ql.exec.vector.*;
+
+/**
+ * DeCompress RunLength
+ *
+ * @since 2021-09-27
+ */
+public class PageDeRunLength {
+
+    private final int BATCH_SIZE = VectorizedRowBatch.DEFAULT_SIZE;
+
+    /**
+     * decompress byteColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of byteArray to decompress
+     * @return decompressed byteColumnVectors
+     */
+    public ColumnVector[] decompressByteArray(int positionCount, ColumnVector tmpColumnVector) {
+        LongColumnVector tmpLongColumnVector = (LongColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpLongColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.isNull[j] = true;
+                }
+                longColumnVector.noNulls = false;
+                resColumnVectors[i] = longColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.vector[j] = tmpLongColumnVector.vector[0];
+                    longColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = longColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress booleanColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of booleanArray to decompress
+     * @return decompressed booleanColumnVectors
+     */
+    public ColumnVector[] decompressBooleanArray(int positionCount, ColumnVector tmpColumnVector) {
+        return decompressByteArray(positionCount, tmpColumnVector);
+    }
+
+    /**
+     * decompress intColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of intArray to decompress
+     * @return decompressed intColumnVectors
+     */
+    public ColumnVector[] decompressIntArray(int positionCount, ColumnVector tmpColumnVector) {
+        LongColumnVector tmpLongColumnVector = (LongColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpLongColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.isNull[j] = true;
+                }
+                longColumnVector.noNulls = false;
+                resColumnVectors[i] = longColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.vector[j] = tmpLongColumnVector.vector[0];
+                    longColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = longColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress shortColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of shortArray to decompress
+     * @return decompressed shortColumnVectors
+     */
+    public ColumnVector[] decompressShortArray(int positionCount, ColumnVector tmpColumnVector) {
+        LongColumnVector tmpLongColumnVector = (LongColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpLongColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.isNull[j] = true;
+                }
+                longColumnVector.noNulls = false;
+                resColumnVectors[i] = longColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.vector[j] = tmpLongColumnVector.vector[0];
+                    longColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = longColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress longColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of longArray to decompress
+     * @return decompressed longColumnVectors
+     */
+    public ColumnVector[] decompressLongArray(int positionCount, ColumnVector tmpColumnVector) {
+        LongColumnVector tmpLongColumnVector = (LongColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpLongColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.isNull[j] = true;
+                }
+                longColumnVector.noNulls = false;
+                resColumnVectors[i] = longColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                LongColumnVector longColumnVector = new LongColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    longColumnVector.vector[j] = tmpLongColumnVector.vector[0];
+                    longColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = longColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress floatColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of floatArray to decompress
+     * @return decompressed floatColumnVectors
+     */
+    public ColumnVector[] decompressFloatArray(int positionCount, ColumnVector tmpColumnVector) {
+        DoubleColumnVector tmpDoubleColumnVector = (DoubleColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpDoubleColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    doubleColumnVector.isNull[j] = true;
+                }
+                doubleColumnVector.noNulls = false;
+                resColumnVectors[i] = doubleColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    doubleColumnVector.vector[j] = tmpDoubleColumnVector.vector[0];
+                    doubleColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = doubleColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress doubleColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of doubleArray to decompress
+     * @return decompressed doubleColumnVectors
+     */
+    public ColumnVector[] decompressDoubleArray(int positionCount, ColumnVector tmpColumnVector) {
+        DoubleColumnVector tmpDoubleColumnVector = (DoubleColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpDoubleColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    doubleColumnVector.isNull[j] = true;
+                }
+                doubleColumnVector.noNulls = false;
+                resColumnVectors[i] = doubleColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                DoubleColumnVector doubleColumnVector = new DoubleColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    doubleColumnVector.vector[j] = tmpDoubleColumnVector.vector[0];
+                    doubleColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = doubleColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+
+    /**
+     * decompress stringColumnVector
+     *
+     * @param positionCount the positionCount to decompress
+     * @param tmpColumnVector the columnVector of string to decompress
+     * @return decompressed stringColumnVectors
+     */
+    public ColumnVector[] decompressVariableWidth(int positionCount, ColumnVector tmpColumnVector) {
+        BytesColumnVector tmpBytesColumnVector = (BytesColumnVector) tmpColumnVector;
+        int batchCount = (positionCount + BATCH_SIZE - 1) / BATCH_SIZE;
+        int remainderCount = positionCount % BATCH_SIZE;
+        if (remainderCount == 0) {
+            remainderCount = BATCH_SIZE;
+        }
+        int loopPositionCount = BATCH_SIZE;
+        ColumnVector[] resColumnVectors = new ColumnVector[batchCount];
+        if (tmpBytesColumnVector.isNull[0]) {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                BytesColumnVector bytesColumnVector = new BytesColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    bytesColumnVector.isNull[j] = true;
+                }
+                bytesColumnVector.noNulls = false;
+                resColumnVectors[i] = bytesColumnVector;
+            }
+        } else {
+            for (int i = 0; i < batchCount; i++) {
+                if (batchCount - 1 == i) {
+                    loopPositionCount = remainderCount;
+                }
+                BytesColumnVector bytesColumnVector = new BytesColumnVector(loopPositionCount);
+                for (int j = 0; j < loopPositionCount; j++) {
+                    bytesColumnVector.vector[j] = tmpBytesColumnVector.vector[0];
+                    bytesColumnVector.isNull[j] = false;
+                }
+                resColumnVectors[i] = bytesColumnVector;
+            }
+        }
+        return resColumnVectors;
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeserializer.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeserializer.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeserializer.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/decode/PageDeserializer.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,83 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.decode;
+
+import com.huawei.boostkit.omnidata.decode.Deserializer;
+
+import com.huawei.boostkit.omnidata.decode.type.DecodeType;
+import io.airlift.slice.SliceInput;
+import io.hetu.core.transport.execution.buffer.SerializedPage;
+
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Optional;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+/**
+ * PageDeserializer
+ */
+public class PageDeserializer implements Deserializer<List<ColumnVector[]>> {
+
+    private final PageDecoding decoding;
+
+    private final DecodeType[] columnTypes;
+
+    public PageDeserializer(DecodeType[] columnTypes) {
+        this.columnTypes = columnTypes;
+        decoding = new PageDecoding();
+    }
+
+    @Override
+    public List<ColumnVector[]> deserialize(SerializedPage page) {
+        if (page.isCompressed() || page.isEncrypted()) {
+            throw new UnsupportedOperationException(
+                    "unsupported HiveDeserializer isMarkerPage or compressed or encrypted page ");
+        }
+        SliceInput input = page.getSlice().getInput();
+        int numberOfBlocks = input.readInt();
+        checkArgument(numberOfBlocks >= 0, "decode failed, numberOfBlocks < 0");
+        List<ColumnVector[]> columnVectors = new ArrayList<>();
+
+        for (int i = 0; i < numberOfBlocks; i++) {
+            ColumnVector[] result = decoding.decode(Optional.of(columnTypes[i]), input);
+            if (result == null) {
+                return null;
+            }
+            columnVectors.add(result);
+        }
+        return transform(columnVectors, numberOfBlocks);
+    }
+
+    private List<ColumnVector[]> transform(List<ColumnVector[]> columnVectors, int numberOfBlocks) {
+        List<ColumnVector[]> newColumnVectors = new ArrayList<>();
+        for (int i = 0; i < columnVectors.get(0).length; i++) {
+            ColumnVector[] result = new ColumnVector[numberOfBlocks];
+            for (int j = 0; j < numberOfBlocks; j++) {
+                result[j] = columnVectors.get(j)[i];
+            }
+            newColumnVectors.add(result);
+        }
+        return newColumnVectors;
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/OmniDataUtils.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/OmniDataUtils.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/OmniDataUtils.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/OmniDataUtils.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,350 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata;
+
+import static io.airlift.slice.Slices.utf8Slice;
+import static io.prestosql.spi.type.BigintType.BIGINT;
+import static io.prestosql.spi.type.BooleanType.BOOLEAN;
+import static io.prestosql.spi.type.DateType.DATE;
+import static io.prestosql.spi.type.DoubleType.DOUBLE;
+import static io.prestosql.spi.type.IntegerType.INTEGER;
+import static io.prestosql.spi.type.RealType.REAL;
+import static io.prestosql.spi.type.SmallintType.SMALLINT;
+import static io.prestosql.spi.type.TinyintType.TINYINT;
+import static io.prestosql.spi.type.VarcharType.VARCHAR;
+import static java.lang.Float.floatToIntBits;
+
+
+import com.huawei.boostkit.omnidata.decode.type.*;
+import com.huawei.boostkit.omnidata.model.Column;
+import com.huawei.boostkit.omnidata.model.Predicate;
+
+import io.prestosql.spi.relation.ConstantExpression;
+import io.prestosql.spi.type.ArrayType;
+import io.prestosql.spi.type.CharType;
+import io.prestosql.spi.type.Type;
+import io.prestosql.spi.type.VarcharType;
+
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * OmniData tool
+ *
+ * @since 2021-11-16
+ */
+public class OmniDataUtils {
+
+    /**
+     * Converts Hive data type to OmniData Type
+     *
+     * @param dataType Hive data type
+     * @return OmniData Type
+     */
+    public static Type transOmniDataType(String dataType) {
+        String lType = dataType.toLowerCase(Locale.ENGLISH);
+        int length = -1;
+        // Keep the English letters and remove the others. like: char(11) -> char
+        if (lType.contains("char")) {
+            lType = dataType.replaceAll("[^a-z<>]", "");
+            String sLength = dataType.replaceAll("[^0-9]", "");
+            if (sLength.length() > 0) {
+                length = Integer.parseInt(sLength);
+            }
+        }
+        switch (lType) {
+            case "bigint":
+            case "long":
+                return BIGINT;
+            case "boolean":
+                return BOOLEAN;
+            case "byte":
+            case "tinyint":
+                return TINYINT;
+            case "char":
+                if (length > 0) {
+                    return CharType.createCharType(length);
+                } else {
+                    return VARCHAR;
+                }
+            case "string":
+            case "varchar":
+                return VARCHAR;
+            case "double":
+                return DOUBLE;
+            case "date":
+                return DATE;
+            case "float":
+                return REAL;
+            case "int":
+            case "integer":
+                return INTEGER;
+            case "short":
+            case "smallint":
+                return SMALLINT;
+            case "array<string>":
+                return new ArrayType<>(VARCHAR);
+            default:
+                throw new UnsupportedOperationException("OmniData Hive unsupported this type:" + lType);
+        }
+    }
+
+    /**
+     * Converts Hive UDF typeInfo to OmniData Type
+     *
+     * @param typeInfo Hive UDF typeInfo
+     * @return OmniData Type
+     */
+    public static Type transOmniDataUdfType(TypeInfo typeInfo) {
+        if (typeInfo instanceof CharTypeInfo) {
+            return CharType.createCharType(((CharTypeInfo) typeInfo).getLength());
+        } else if (typeInfo instanceof VarcharTypeInfo) {
+            return VarcharType.createVarcharType(((VarcharTypeInfo) typeInfo).getLength());
+        } else {
+            return transOmniDataType(typeInfo.getTypeName());
+        }
+    }
+
+    public static Type transAggType(Type dataType) {
+        if (BIGINT.equals(dataType) || INTEGER.equals(dataType) || SMALLINT.equals(dataType) || TINYINT.equals(
+                dataType)) {
+            return BIGINT;
+        } else if (DOUBLE.equals(dataType) || REAL.equals(dataType)) {
+            return DOUBLE;
+        } else {
+            return dataType;
+        }
+    }
+
+    /**
+     * Converts Hive Agg return type to OmniData DecodeType
+     *
+     * @param dataType Hive Agg return type
+     * @return OmniData DecodeType
+     */
+    public static DecodeType transOmniDataAggDecodeType(String dataType) {
+        String lType = dataType.toLowerCase(Locale.ENGLISH);
+        // Keep the English letters and remove the others. like: char(11) -> char
+        if (lType.contains("char")) {
+            lType = dataType.replaceAll("[^a-z<>]", "");
+        }
+        switch (lType) {
+            case "bigint":
+                return new LongDecodeType();
+            case "boolean":
+                return new BooleanDecodeType();
+            case "byte":
+            case "tinyint":
+                return new LongToByteDecodeType();
+            case "char":
+            case "string":
+            case "varchar":
+                return new VarcharDecodeType();
+            case "date":
+            case "int":
+            case "integer":
+                return new LongToIntDecodeType();
+            case "double":
+                return new DoubleDecodeType();
+            case "float":
+            case "real":
+                return new LongToFloatDecodeType();
+            case "smallint":
+                return new LongToShortDecodeType();
+            default:
+                throw new UnsupportedOperationException("OmniData Hive unsupported this type:" + lType);
+        }
+    }
+
+    /**
+     * Converts Hive return type to OmniData DecodeType
+     *
+     * @param dataType Hive return type
+     * @return OmniData DecodeType
+     */
+    public static DecodeType transOmniDataDecodeType(String dataType) {
+        String lType = dataType.toLowerCase(Locale.ENGLISH);
+        // Keep the English letters and remove the others. like: char(11) -> char
+        if (lType.contains("char")) {
+            lType = dataType.replaceAll("[^a-z<>]", "");
+        }
+        switch (lType) {
+            case "bigint":
+                return new LongDecodeType();
+            case "boolean":
+                return new BooleanDecodeType();
+            case "byte":
+            case "tinyint":
+                return new ByteDecodeType();
+            case "char":
+            case "string":
+            case "varchar":
+                return new VarcharDecodeType();
+            case "date":
+                return new DateDecodeType();
+            case "double":
+                return new DoubleDecodeType();
+            case "float":
+            case "real":
+                return new FloatDecodeType();
+            case "int":
+            case "integer":
+                return new IntDecodeType();
+            case "smallint":
+                return new ShortDecodeType();
+            default:
+                throw new UnsupportedOperationException("OmniData Hive unsupported this type:" + lType);
+        }
+    }
+
+    /**
+     * Converts OmniData Type to ConstantExpression
+     *
+     * @param value constant value
+     * @param type OmniData Type
+     * @return ConstantExpression
+     */
+    public static ConstantExpression transOmniDataConstantExpr(String value, Type type) {
+        String lType = type.toString().toLowerCase(Locale.ENGLISH);
+        if (lType.contains("char")) {
+            lType = type.toString().replaceAll("[^a-z<>]", "");
+        }
+        // check 'null' value
+        if ("null".equals(value) && !"varchar".equals(lType) && !"char".equals(lType)) {
+            return new ConstantExpression(null, type);
+        }
+        switch (lType) {
+            case "bigint":
+            case "integer":
+            case "tinyint":
+            case "smallint":
+                return new ConstantExpression(Long.parseLong(value), type);
+            case "boolean":
+                return new ConstantExpression(Boolean.parseBoolean(value), type);
+            case "char":
+                // When the Slices store the 'char' type, the trailing spaces need to be deleted.
+                return new ConstantExpression(utf8Slice(stripEnd(value, " ")), type);
+            case "date":
+                if (value.contains("-")) {
+                    String[] dateStrArray = value.split("-");
+                    long daToMillSecs = 24 * 3600 * 1000;
+                    int year = Integer.parseInt(dateStrArray[0]) - 1900;
+                    int month = Integer.parseInt(dateStrArray[1]) - 1;
+                    int day = Integer.parseInt(dateStrArray[2]);
+                    java.sql.Date date = new java.sql.Date(year, month, day);
+                    return new ConstantExpression((date.getTime() - date.getTimezoneOffset() * 60000L) / daToMillSecs,
+                            type);
+                } else {
+                    return new ConstantExpression(Long.parseLong(value), type);
+                }
+            case "double":
+                return new ConstantExpression(Double.parseDouble(value), type);
+            case "real":
+                return new ConstantExpression((long) floatToIntBits(Float.parseFloat(value)), type);
+            case "varchar":
+                return new ConstantExpression(utf8Slice(value), type);
+            default:
+                throw new UnsupportedOperationException("OmniData Hive unsupported this type:" + lType);
+        }
+    }
+
+    public static String stripEnd(String str, String stripChars) {
+        int end;
+        if (str != null && (end = str.length()) != 0) {
+            if (stripChars == null) {
+                while (end != 0 && Character.isWhitespace(str.charAt(end - 1))) {
+                    --end;
+                }
+            } else {
+                if (stripChars.isEmpty()) {
+                    return str;
+                }
+                while (end != 0 && stripChars.indexOf(str.charAt(end - 1)) != -1) {
+                    --end;
+                }
+            }
+            return str.substring(0, end);
+        } else {
+            return str;
+        }
+    }
+
+    /**
+     * Converts Hive operator name to OmniData operator
+     *
+     * @param operator Hive operator name
+     * @return OmniData operator
+     */
+    public static String transOmniDataOperator(String operator) {
+        switch (operator) {
+            case "LESS_THAN":
+                return "less_than";
+            case "LESS_THAN_EQUALS":
+                return "less_than_or_equal";
+            case "EQUALS":
+                return "equal";
+            default:
+                return operator;
+        }
+    }
+
+    public static void addPartitionValues(NdpPredicateInfo ndpPredicate, String path, String defaultPartitionValue) {
+        Predicate oldPredicate = ndpPredicate.getPredicate();
+        List<Column> newColumns = new ArrayList<>();
+        for (Column column : oldPredicate.getColumns()) {
+            if (column.isPartitionKey()) {
+                String partitionValue = getPartitionValue(path, column.getName(), defaultPartitionValue);
+                newColumns.add(
+                        new Column(column.getFieldId(), column.getName(), column.getType(), true, partitionValue));
+            } else {
+                newColumns.add(column);
+            }
+        }
+        Predicate newPredicate = new Predicate(oldPredicate.getTypes(), newColumns, oldPredicate.getFilter(),
+                oldPredicate.getProjections(), oldPredicate.getDomains(), oldPredicate.getBloomFilters(),
+                oldPredicate.getAggregations(), oldPredicate.getLimit());
+        ndpPredicate.setPredicate(newPredicate);
+    }
+
+    private static String getPartitionValue(String filePath, String columnName, String defaultPartitionValue) {
+        String[] filePathStrArray = filePath.split("\\/");
+        String partitionValue = "";
+        Pattern pn = Pattern.compile(columnName + "\\=");
+        for (String strColumn : filePathStrArray) {
+            Matcher matcher = pn.matcher(strColumn);
+            if (matcher.find()) {
+                partitionValue = strColumn.split("\\=")[1];
+                if (defaultPartitionValue.equals(partitionValue)) {
+                    partitionValue = null;
+                }
+                break;
+            }
+        }
+        return partitionValue;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/NdpArithmeticExpressionInfo.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/NdpArithmeticExpressionInfo.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/NdpArithmeticExpressionInfo.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/NdpArithmeticExpressionInfo.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.aggregation;
+
+/**
+ * Ndp Arithmetic Expression Info
+ * <p>
+ * This class obtains variables such as colId based on the Hive expression string and checks whether the value of colId is a number.
+ *
+ * @since 2022-01-22
+ */
+public class NdpArithmeticExpressionInfo {
+    private boolean[] isVal;
+
+    private int[] colId;
+
+    private String[] colType;
+
+    private String[] colValue;
+
+    public NdpArithmeticExpressionInfo(String vectorExpressionParameters) {
+        // vectorExpressionParameters : col 2:bigint, col 6:bigint
+        String[] parameters = vectorExpressionParameters.split(", ");
+        int length = parameters.length;
+        isVal = new boolean[length];
+        colId = new int[length];
+        colType = new String[length];
+        colValue = new String[length];
+        for (int j = 0; j < length; j++) {
+            if (parameters[j].startsWith("val ")) {
+                isVal[j] = true;
+                colValue[j] = parameters[j].substring("val ".length());
+            } else {
+                isVal[j] = false;
+                colId[j] = Integer.parseInt(parameters[j].split(":")[0].substring("col ".length()));
+                colType[j] = parameters[j].split(":")[1];
+            }
+        }
+    }
+
+    public boolean[] getIsVal() {
+        return isVal;
+    }
+
+    public int[] getColId() {
+        return colId;
+    }
+
+    public String[] getColType() {
+        return colType;
+    }
+
+    public String[] getColValue() {
+        return colValue;
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/OmniDataAggregation.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/OmniDataAggregation.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/OmniDataAggregation.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/aggregation/OmniDataAggregation.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,195 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.aggregation;
+
+import static io.prestosql.spi.function.FunctionKind.AGGREGATE;
+import static io.prestosql.spi.type.BigintType.BIGINT;
+
+import com.huawei.boostkit.omnidata.model.AggregationInfo;
+
+import io.prestosql.spi.connector.QualifiedObjectName;
+import io.prestosql.spi.function.BuiltInFunctionHandle;
+import io.prestosql.spi.function.FunctionHandle;
+import io.prestosql.spi.function.FunctionKind;
+import io.prestosql.spi.function.Signature;
+import io.prestosql.spi.relation.CallExpression;
+import io.prestosql.spi.relation.InputReferenceExpression;
+import io.prestosql.spi.relation.RowExpression;
+import io.prestosql.spi.type.Type;
+
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.omnidata.OmniDataUtils;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.OmniDataPredicate;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpUdfEnum;
+import org.apache.hadoop.hive.ql.plan.VectorGroupByDesc;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Optional;
+
+/**
+ * OmniData Aggregation
+ *
+ * @since 2021-12-07
+ */
+public class OmniDataAggregation {
+
+    private static final Logger LOG = LoggerFactory.getLogger(OmniDataAggregation.class);
+
+    private OmniDataPredicate predicateInfo;
+
+    private boolean isPushDownAgg = true;
+
+    private Map<String, AggregationInfo.AggregateFunction> aggregationMap = new LinkedHashMap<>();
+
+    public OmniDataAggregation(OmniDataPredicate predicateInfo) {
+        this.predicateInfo = predicateInfo;
+    }
+
+    public AggregationInfo getAggregation(VectorGroupByDesc aggVectorsDesc) {
+        List<RowExpression> groupingKeys = new ArrayList<>();
+        for (VectorExpression groupExpression : aggVectorsDesc.getKeyExpressions()) {
+            createGroupingKey(groupExpression, groupingKeys);
+        }
+        for (VectorAggregationDesc aggregateFunction : aggVectorsDesc.getVecAggrDescs()) {
+            createAggregateFunction(aggregateFunction);
+        }
+        return isPushDownAgg ? new AggregationInfo(aggregationMap, groupingKeys) : null;
+    }
+
+    private void createGroupingKey(VectorExpression groupExpression, List<RowExpression> groupingKeys) {
+        if (IdentityExpression.isColumnOnly(groupExpression)) {
+            IdentityExpression identityExpression = (IdentityExpression) groupExpression;
+            int outputColumnId = identityExpression.getOutputColumnNum();
+            if (!predicateInfo.addProjectionsByGroupByKey(outputColumnId)) {
+                isPushDownAgg = false;
+                LOG.info("Aggregation failed to push down, since outputColumnId is not exists");
+                return;
+            }
+            int projectId = predicateInfo.getColName2ProjectId()
+                    .get(predicateInfo.getColId2ColName().get(outputColumnId));
+            Type omniDataType = OmniDataUtils.transOmniDataType(identityExpression.getOutputTypeInfo().getTypeName());
+            groupingKeys.add(new InputReferenceExpression(projectId, omniDataType));
+        } else {
+            LOG.info("Aggregation failed to push down, since unsupported this [{}]", groupExpression.getClass());
+            isPushDownAgg = false;
+        }
+    }
+
+    private void createAggregateFunction(VectorAggregationDesc aggregateFunction) {
+        String operatorName = aggregateFunction.getAggrDesc().getGenericUDAFName().toLowerCase(Locale.ENGLISH);
+        switch (operatorName) {
+            case "count":
+                parseCountAggregation(aggregateFunction);
+                break;
+            case "max":
+            case "min":
+            case "sum":
+                parseMaxMinSumAggregation(aggregateFunction);
+                break;
+            default:
+                isPushDownAgg = false;
+        }
+    }
+
+    private void parseCountAggregation(VectorAggregationDesc aggregateFunction) {
+        List<RowExpression> arguments = new ArrayList<>();
+        Signature signature;
+        if (aggregateFunction.getInputExpression() != null) {
+            // count(col_1)
+            Type omniDataInputType = OmniDataUtils.transOmniDataType(
+                    aggregateFunction.getInputTypeInfo().getTypeName());
+            IdentityExpression identityExpression = (IdentityExpression) aggregateFunction.getInputExpression();
+            int columnId = identityExpression.getOutputColumnNum();
+            predicateInfo.addProjectionsByAggCount(columnId);
+            int projectId = predicateInfo.getColName2ProjectId().get(predicateInfo.getColId2ColName().get(columnId));
+            arguments.add(new InputReferenceExpression(projectId, predicateInfo.getTypes().get(projectId)));
+            signature = new Signature(QualifiedObjectName.valueOfDefaultFunction("count"), AGGREGATE,
+                    BIGINT.getTypeSignature(), omniDataInputType.getTypeSignature());
+        } else {
+            // count(*)
+            signature = new Signature(QualifiedObjectName.valueOfDefaultFunction("count"), AGGREGATE,
+                    BIGINT.getTypeSignature());
+            predicateInfo.getDecodeTypes().add(BIGINT.toString());
+            predicateInfo.getDecodeTypesWithAgg().add(false);
+        }
+        FunctionHandle functionHandle = new BuiltInFunctionHandle(signature);
+        CallExpression callExpression = new CallExpression("count", functionHandle, BIGINT, arguments,
+                Optional.empty());
+        aggregationMap.put(String.format("%s_%s", "count", aggregationMap.size()),
+                new AggregationInfo.AggregateFunction(callExpression, false));
+    }
+
+    private void parseMaxMinSumAggregation(VectorAggregationDesc aggregateFunction) {
+        if (IdentityExpression.isColumnOnly(aggregateFunction.getInputExpression())) {
+            String operatorName = aggregateFunction.getAggrDesc().getGenericUDAFName();
+            IdentityExpression identityExpression = (IdentityExpression) aggregateFunction.getInputExpression();
+            Type omniDataInputType = OmniDataUtils.transOmniDataType(
+                    aggregateFunction.getInputTypeInfo().getTypeName());
+            Type omniDataOutputType = OmniDataUtils.transOmniDataType(
+                    aggregateFunction.getOutputTypeInfo().getTypeName());
+            int columnId = identityExpression.getOutputColumnNum();
+            predicateInfo.addProjectionsByAgg(columnId);
+            if (!predicateInfo.isPushDown()) {
+                isPushDownAgg = false;
+                return;
+            }
+            int projectId = predicateInfo.getColName2ProjectId().get(predicateInfo.getColId2ColName().get(columnId));
+            if (omniDataInputType != omniDataOutputType) {
+                // The input and output types of the OmniData are the same.
+                castAggOutputType(projectId, omniDataInputType, omniDataOutputType);
+            }
+            FunctionHandle functionHandle = new BuiltInFunctionHandle(
+                    new Signature(QualifiedObjectName.valueOfDefaultFunction(operatorName), AGGREGATE,
+                            omniDataOutputType.getTypeSignature(), omniDataOutputType.getTypeSignature()));
+            List<RowExpression> arguments = new ArrayList<>();
+            arguments.add(new InputReferenceExpression(projectId, predicateInfo.getTypes().get(projectId)));
+            CallExpression callExpression = new CallExpression(operatorName, functionHandle, omniDataOutputType,
+                    arguments, Optional.empty());
+            aggregationMap.put(String.format("%s_%s", operatorName, projectId),
+                    new AggregationInfo.AggregateFunction(callExpression, false));
+        } else {
+            LOG.info("Aggregation failed to push down, since unsupported this [{}]",
+                    aggregateFunction.getInputExpression().getClass());
+            isPushDownAgg = false;
+        }
+    }
+
+    private void castAggOutputType(int projectId, Type omniDataInputType, Type omniDataOutputType) {
+        List<RowExpression> rowArguments = new ArrayList<>();
+        rowArguments.add(predicateInfo.getProjections().remove(projectId));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.CAST.getOperatorName()), FunctionKind.SCALAR,
+                omniDataOutputType.getTypeSignature(), omniDataInputType.getTypeSignature());
+        predicateInfo.getProjections()
+                .add(projectId, new CallExpression(NdpUdfEnum.CAST.getSignatureName(), new BuiltInFunctionHandle(signature),
+                        omniDataOutputType, rowArguments, Optional.empty()));
+        predicateInfo.getTypes().set(projectId, omniDataOutputType);
+        predicateInfo.getDecodeTypes().set(projectId, omniDataOutputType.toString());
+        predicateInfo.getDecodeTypesWithAgg().set(projectId, true);
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpArithmeticEnum.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpArithmeticEnum.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpArithmeticEnum.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpArithmeticEnum.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.enums;
+
+/**
+ * Ndp NdpOperator Type Enum
+ *
+ * @since 2022-01-27
+ */
+public enum NdpArithmeticEnum {
+    // *
+    MULTIPLY,
+    // +
+    ADD,
+    // -
+    SUBTRACT,
+    // /
+    DIVIDE,
+    // %
+    MODULUS,
+    CAST,
+    CAST_IDENTITY,
+    UNSUPPORTED;
+
+    public static NdpArithmeticEnum getArithmeticByClass(Class arithmeticClass) {
+        String arithmeticClassName = arithmeticClass.getSimpleName();
+        if (arithmeticClassName.contains("Multiply")) {
+            return MULTIPLY;
+        } else if (arithmeticClassName.contains("Add")) {
+            return ADD;
+        } else if (arithmeticClassName.contains("Subtract")) {
+            return SUBTRACT;
+        } else if (arithmeticClassName.contains("Divide")) {
+            return DIVIDE;
+        } else if (arithmeticClassName.contains("Modulo")) {
+            return MODULUS;
+        } else if (arithmeticClassName.contains("IdentityExpression")) {
+            return CAST_IDENTITY;
+        } else if (arithmeticClassName.contains("Cast")) {
+            return CAST;
+        } else {
+            return UNSUPPORTED;
+        }
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpHiveOperatorEnum.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpHiveOperatorEnum.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpHiveOperatorEnum.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpHiveOperatorEnum.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,106 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.enums;
+
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpPlanChecker;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.udf.UDFLike;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
+
+/**
+ * Hive operators supported by push down
+ *
+ * @since 2022-02-24
+ */
+public enum NdpHiveOperatorEnum {
+    // Supported push-down hive operator
+    AND("and", GenericUDFOPAnd.class),
+    BETWEEN("between", GenericUDFBetween.class),
+    EQUAL("=", GenericUDFOPEqual.class),
+    GREATER_THAN(">", GenericUDFOPGreaterThan.class),
+    GREATER_THAN_OR_EQUAL(">=", GenericUDFOPEqualOrGreaterThan.class),
+    IN("in", GenericUDFIn.class),
+    LESS_THAN("<", GenericUDFOPLessThan.class),
+    LESS_THAN_OR_EQUAL("<=", GenericUDFOPEqualOrLessThan.class),
+    LIKE("like", UDFLike.class),
+    NOT("not", GenericUDFOPNot.class),
+    NOT_EQUAL("!=", GenericUDFOPNotEqual.class),
+    NOT_NULL("isnotnull", GenericUDFOPNotNull.class),
+    NULL("isnull", GenericUDFOPNull.class),
+    OR("or", GenericUDFOPOr.class),
+    UNSUPPORTED("unsupported", null);
+
+    private String hiveOpName;
+
+    private Class hiveOpClass;
+
+    NdpHiveOperatorEnum(String hiveOpName, Class hiveOpClass) {
+        this.hiveOpName = hiveOpName;
+        this.hiveOpClass = hiveOpClass;
+    }
+
+    public String getHiveOpName() {
+        return hiveOpName;
+    }
+
+    public void setHiveOpName(String hiveOpName) {
+        this.hiveOpName = hiveOpName;
+    }
+
+    public Class getHiveOpClass() {
+        return hiveOpClass;
+    }
+
+    public void setHiveOpClass(Class hiveOpClass) {
+        this.hiveOpClass = hiveOpClass;
+    }
+
+    public static NdpHiveOperatorEnum getNdpHiveOperator(ExprNodeGenericFuncDesc funcDesc) {
+        NdpHiveOperatorEnum resOperator = NdpHiveOperatorEnum.UNSUPPORTED;
+        Class operator = (funcDesc.getGenericUDF() instanceof GenericUDFBridge)
+            ? ((GenericUDFBridge) funcDesc.getGenericUDF()).getUdfClass()
+            : funcDesc.getGenericUDF().getClass();
+        for (NdpHiveOperatorEnum operatorEnum : NdpHiveOperatorEnum.values()) {
+            // operator needs white list verification
+            if (operatorEnum.getHiveOpClass() == operator && NdpPlanChecker.checkOperatorByWhiteList(operatorEnum)) {
+                resOperator = operatorEnum;
+                break;
+            }
+        }
+        return resOperator;
+    }
+
+    public static boolean checkNotSupportedOperator(NdpHiveOperatorEnum operator) {
+        return !(operator.equals(NOT) || operator.equals(AND) || operator.equals(OR) || operator.equals(UNSUPPORTED));
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpUdfEnum.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpUdfEnum.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpUdfEnum.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/enums/NdpUdfEnum.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,130 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.enums;
+
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpPlanChecker;
+import org.apache.hadoop.hive.ql.udf.UDFReplace;
+import org.apache.hadoop.hive.ql.udf.UDFSubstr;
+import org.apache.hadoop.hive.ql.udf.UDFToBoolean;
+import org.apache.hadoop.hive.ql.udf.UDFToByte;
+import org.apache.hadoop.hive.ql.udf.UDFToDouble;
+import org.apache.hadoop.hive.ql.udf.UDFToFloat;
+import org.apache.hadoop.hive.ql.udf.UDFToInteger;
+import org.apache.hadoop.hive.ql.udf.UDFToLong;
+import org.apache.hadoop.hive.ql.udf.UDFToShort;
+import org.apache.hadoop.hive.ql.udf.UDFToString;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFLength;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper;
+
+/**
+ * Hive Udf Enum
+ */
+public enum NdpUdfEnum {
+    // Supported push-down Udf
+    CAST("cast", "$operator$cast"),
+    INSTR("instr", "instr"),
+    LENGTH("length", "length"),
+    LOWER("lower", "lower"),
+    REPLACE("replace", "replace"),
+    // unsupported split()
+    SPLIT("split", "split"),
+    SUBSCRIPT("SUBSCRIPT", "$operator$subscript"),
+    SUBSTR("substr", "substr"),
+    SUBSTRING("substring", "substring"),
+    UPPER("upper", "upper"),
+    UNSUPPORTED("", "");
+
+    private String signatureName;
+
+    private String operatorName;
+
+    NdpUdfEnum(String signatureName, String operatorName) {
+        this.signatureName = signatureName;
+        this.operatorName = operatorName;
+    }
+
+    public static NdpUdfEnum getUdfType(GenericUDF genericUDF) {
+        NdpUdfEnum resUdf = UNSUPPORTED;
+        Class udfClass = (genericUDF instanceof GenericUDFBridge)
+                ? ((GenericUDFBridge) genericUDF).getUdfClass()
+                : genericUDF.getClass();
+        if (isOpCast(udfClass)) {
+            resUdf = CAST;
+        } else if (udfClass == GenericUDFInstr.class) {
+            resUdf = INSTR;
+        } else if (udfClass == GenericUDFLength.class) {
+            resUdf = LENGTH;
+        } else if (udfClass == GenericUDFLower.class) {
+            resUdf = LOWER;
+        } else if (udfClass == UDFReplace.class) {
+            resUdf = REPLACE;
+        } else if (udfClass == GenericUDFIndex.class) {
+            resUdf = SUBSCRIPT;
+        } else if (udfClass == UDFSubstr.class) {
+            resUdf = SUBSTR;
+        } else if (udfClass == GenericUDFUpper.class) {
+            resUdf = UPPER;
+        }
+        // udf needs white list verification
+        return NdpPlanChecker.checkUdfByWhiteList(resUdf) ? resUdf : UNSUPPORTED;
+    }
+
+    public static boolean checkUdfSupported(GenericUDF genericUDF) {
+        return !getUdfType(genericUDF).equals(UNSUPPORTED);
+    }
+
+    /**
+     * need to support :
+     * GenericUDFTimestamp.class GenericUDFToBinary.class GenericUDFToDecimal.class GenericUDFToTimestampLocalTZ.class
+     *
+     * @param udfClass Class
+     * @return true or false
+     */
+    public static boolean isOpCast(Class udfClass) {
+        return udfClass == UDFToBoolean.class || udfClass == UDFToByte.class || udfClass == UDFToDouble.class
+                || udfClass == UDFToFloat.class || udfClass == UDFToInteger.class || udfClass == UDFToLong.class
+                || udfClass == UDFToShort.class || udfClass == UDFToString.class || udfClass == GenericUDFToVarchar.class
+                || udfClass == GenericUDFToChar.class || udfClass == GenericUDFToDate.class;
+    }
+
+    public String getSignatureName() {
+        return signatureName;
+    }
+
+    public void setSignatureName(String signatureName) {
+        this.signatureName = signatureName;
+    }
+
+    public String getOperatorName() {
+        return operatorName;
+    }
+
+    public void setOperatorName(String operatorName) {
+        this.operatorName = operatorName;
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilter.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilter.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilter.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilter.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,546 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.filter;
+
+import static org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpHiveOperatorEnum.NOT;
+
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpHiveOperatorEnum;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpUdfEnum;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicListDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.udf.UDFLike;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Ndp Filter
+ * <p>
+ * First, need to determine whether to perform partial push down. Currently,
+ * Second, to create a binary tree(NdpFilterBinaryTree.class) and convert Hive to OmniData.
+ *
+ * @since 2022-02-22
+ */
+public class NdpFilter {
+    private static final Logger LOG = LoggerFactory.getLogger(NdpFilter.class);
+
+    private NdpFilterBinaryTree binaryTreeHead;
+
+    private final List<NdpFilterLeaf> ndpFilterLeafList = new ArrayList<>();
+
+    private ExprNodeDesc pushDownFilterDesc;
+
+    private ExprNodeDesc unPushDownFilterDesc;
+
+    private final List<ExprNodeDesc> unsupportedFilterDescList = new ArrayList<>();
+
+    private final List<ExprNodeDesc> supportedFilterDescList = new ArrayList<>();
+
+    private boolean isExistsOr = false;
+
+    private NdpFilterMode mode;
+
+    public NdpFilter(ExprNodeGenericFuncDesc rawFuncDesc) {
+        // filter push down entrance
+        parseFilterOperator(rawFuncDesc);
+        // !isExistsOr: the 'OR' operator does not support partial push down
+        if (unsupportedFilterDescList.size() > 0 && supportedFilterDescList.size() > 0 && !isExistsOr) {
+            mode = NdpFilterMode.PART;
+        } else if (unsupportedFilterDescList.size() == 0 && supportedFilterDescList.size() > 0) {
+            // If no unsupported filter exists, push down all filter
+            mode = NdpFilterMode.ALL;
+            return;
+        } else {
+            mode = NdpFilterMode.NONE;
+            return;
+        }
+        // start to part push down
+        // create un push down
+        if (checkDynamicValue(unsupportedFilterDescList)) {
+            this.unPushDownFilterDesc = createNewFuncDesc(rawFuncDesc, unsupportedFilterDescList);
+        } else {
+            mode = NdpFilterMode.NONE;
+            return;
+        }
+        // create part push down
+        this.pushDownFilterDesc = createNewFuncDesc(rawFuncDesc, supportedFilterDescList);
+    }
+
+    public NdpFilterBinaryTree getBinaryTreeHead() {
+        return binaryTreeHead;
+    }
+
+    public List<NdpFilterLeaf> getNdpFilterLeafList() {
+        return ndpFilterLeafList;
+    }
+
+    public ExprNodeDesc getPushDownFuncDesc() {
+        return pushDownFilterDesc;
+    }
+
+    public ExprNodeDesc getUnPushDownFuncDesc() {
+        return unPushDownFilterDesc;
+    }
+
+    /**
+     * get filter push down mode
+     *
+     * @return NdpFilterMode
+     */
+    public NdpFilterMode getMode() {
+        if (mode == null) {
+            return NdpFilterMode.NONE;
+        } else {
+            return mode;
+        }
+    }
+
+    /**
+     * manually setting the push-down mode
+     *
+     * @param mode
+     */
+    public void setMode(NdpFilterMode mode) {
+        this.mode = mode;
+    }
+
+    /**
+     * Converting Hive Operators to FilterBinaryTree
+     *
+     * @param funcDesc Hive ExprNodeGenericFuncDesc
+     */
+    public void createNdpFilterBinaryTree(ExprNodeGenericFuncDesc funcDesc) {
+        binaryTreeHead = new NdpFilterBinaryTree(funcDesc, ndpFilterLeafList);
+    }
+
+    private boolean checkDynamicValue(List<ExprNodeDesc> exprNodeDescList) {
+        for (ExprNodeDesc desc : exprNodeDescList) {
+            for (ExprNodeDesc child : desc.getChildren()) {
+                if (child instanceof ExprNodeDynamicValueDesc || child instanceof ExprNodeDynamicListDesc) {
+                    return false;
+                }
+            }
+        }
+        return true;
+    }
+
+    /**
+     * Todo: need to optimization
+     *
+     * @param oldFuncDesc oldFuncDesc
+     * @param newChildren newChildren
+     * @return new ExprNodeGenericFuncDesc
+     */
+    public static ExprNodeDesc createNewFuncDesc(ExprNodeGenericFuncDesc oldFuncDesc, List<ExprNodeDesc> newChildren) {
+        if (newChildren.size() == 1) {
+            return newChildren.get(0);
+        } else {
+            return new ExprNodeGenericFuncDesc(oldFuncDesc.getTypeInfo(), oldFuncDesc.getGenericUDF(),
+                    oldFuncDesc.getFuncText(), newChildren);
+        }
+    }
+
+    private void parseFilterOperator(ExprNodeGenericFuncDesc funcDesc) {
+        NdpHiveOperatorEnum operator = NdpHiveOperatorEnum.getNdpHiveOperator(funcDesc);
+        switch (operator) {
+            case AND:
+                parseAndOrOperator(funcDesc);
+                break;
+            case OR:
+                isExistsOr = true;
+                parseAndOrOperator(funcDesc);
+                break;
+            case NOT:
+                // Not is only one child
+                parseNotOperator(funcDesc);
+                break;
+            case UNSUPPORTED:
+                // operator unsupported
+                LOG.info("OmniData Hive Filter do not to all push down, since unsupported this Operator class: [{}]",
+                        funcDesc.getGenericUDF().getClass().getSimpleName());
+                unsupportedFilterDescList.add(funcDesc);
+                break;
+            default:
+                if (checkUdf(funcDesc, operator)) {
+                    supportedFilterDescList.add(funcDesc);
+                } else {
+                    // udf unsupported
+                    unsupportedFilterDescList.add(funcDesc);
+                }
+        }
+    }
+
+    private void parseAndOrOperator(ExprNodeGenericFuncDesc funcDesc) {
+        List<ExprNodeDesc> children = funcDesc.getChildren();
+        for (int i = 0; i < children.size(); i++) {
+            ExprNodeDesc child = children.get(i);
+            if (child instanceof ExprNodeGenericFuncDesc) {
+                parseFilterOperator((ExprNodeGenericFuncDesc) child);
+            } else {
+                if (checkBooleanColumn(child)) {
+                    ExprNodeDesc newBooleanChild = transBooleanColumnExpression((ExprNodeColumnDesc) child.clone());
+                    children.set(i, newBooleanChild);
+                    supportedFilterDescList.add(newBooleanChild);
+                } else {
+                    unsupportedFilterDescList.add(child);
+                }
+            }
+        }
+    }
+
+    private void parseNotOperator(ExprNodeGenericFuncDesc notFuncDesc) {
+        // Not is only one child
+        if (notFuncDesc.getChildren().get(0) instanceof ExprNodeGenericFuncDesc) {
+            ExprNodeGenericFuncDesc notChild = (ExprNodeGenericFuncDesc) notFuncDesc.getChildren().get(0);
+            NdpHiveOperatorEnum operator = NdpHiveOperatorEnum.getNdpHiveOperator(notChild);
+            if (NdpHiveOperatorEnum.checkNotSupportedOperator(operator)) {
+                if (checkUdf(notChild, NOT)) {
+                    supportedFilterDescList.add(notFuncDesc);
+                    return;
+                }
+            }
+        }
+        // udf unsupported
+        unsupportedFilterDescList.add(notFuncDesc);
+    }
+
+    /**
+     * Convert 'Column[o_boolean]' to 'GenericUDFOPEqual(Column[o_boolean], Const boolean true)'
+     *
+     * @param booleanDesc Column[o_boolean]
+     * @return GenericUDFOPEqual(Column[o_boolean], Const boolean true)
+     */
+    private ExprNodeDesc transBooleanColumnExpression(ExprNodeColumnDesc booleanDesc) {
+        TypeInfo typeInfo = booleanDesc.getTypeInfo();
+        String funcText = "=";
+        ExprNodeConstantDesc constantDesc = new ExprNodeConstantDesc(typeInfo, true);
+        List<ExprNodeDesc> children = new ArrayList<>();
+        children.add(booleanDesc);
+        children.add(constantDesc);
+        return new ExprNodeGenericFuncDesc(typeInfo, new GenericUDFOPEqual(), funcText, children);
+    }
+
+    /**
+     * In Hive, there are two expressions for 'o_boolean=true':
+     * 1.GenericUDFOPEqual(Column[o_boolean], Const boolean true)
+     * 2.Column[o_boolean]
+     *
+     * @param exprNodeDesc expressions
+     * @return true or false
+     */
+    private boolean checkBooleanColumn(ExprNodeDesc exprNodeDesc) {
+        if (exprNodeDesc instanceof ExprNodeColumnDesc) {
+            ExprNodeColumnDesc columnDesc = (ExprNodeColumnDesc) exprNodeDesc;
+            return "boolean".equals(columnDesc.getTypeInfo().getTypeName());
+        }
+        return false;
+    }
+
+    private boolean checkUdf(ExprNodeGenericFuncDesc funcDesc, NdpHiveOperatorEnum operator) {
+        int argumentIndex = 0;
+        if (operator.equals(NdpHiveOperatorEnum.BETWEEN)) {
+            // Between not support ExprNodeDynamicValueDesc
+            if (!checkBetween(funcDesc)) {
+                return false;
+            }
+            // first argument for BETWEEN should be boolean type
+            argumentIndex = 1;
+        }
+        ExprNodeDesc udfFuncDesc = funcDesc;
+        while (udfFuncDesc.getChildren() != null) {
+            udfFuncDesc = udfFuncDesc.getChildren().get(argumentIndex);
+            argumentIndex = 0;
+            if (udfFuncDesc instanceof ExprNodeGenericFuncDesc) {
+                // check whether the UDF supports push down
+                if (!NdpUdfEnum.checkUdfSupported(((ExprNodeGenericFuncDesc) udfFuncDesc).getGenericUDF())) {
+                    LOG.info("OmniData Hive Filter failed to all push down, since unsupported this UDF class: [{}]",
+                            ((ExprNodeGenericFuncDesc) udfFuncDesc).getGenericUDF());
+                    return false;
+                }
+            }
+        }
+        return true;
+    }
+
+    private boolean checkBetween(ExprNodeGenericFuncDesc funcDesc) {
+        return funcDesc.getChildren().get(2) instanceof ExprNodeConstantDesc && funcDesc.getChildren()
+                .get(3) instanceof ExprNodeConstantDesc;
+    }
+
+    /**
+     * Converting Hive Operators to Omnidata Operators
+     * <p>
+     * For example sql: 'select a,b,c from table1 where a=1 and b=2 and c=3;' .as follows:
+     * Hive Operators:
+     * And
+     * /    |   \
+     * a=1   b=2  c=3
+     * ||
+     * V
+     * OmniData Operators:
+     * And
+     * /    |
+     * a=1  And
+     * |    \
+     * b=2  c=3
+     *
+     * @since 2021-11-23
+     */
+    public static class NdpFilterBinaryTree {
+
+        private final Logger LOG = LoggerFactory.getLogger(NdpFilterBinaryTree.class);
+
+        private NdpOperator ndpOperator;
+
+        private NdpLeafOperator ndpLeafOperator;
+
+        private NdpFilterBinaryTree leftChild;
+
+        private NdpFilterBinaryTree rightChild;
+
+        private int leaf = -1;
+
+        public NdpFilterBinaryTree(NdpOperator ndpOperator, NdpLeafOperator ndpLeafOperator, int leaf) {
+            this.ndpOperator = ndpOperator;
+            this.ndpLeafOperator = ndpLeafOperator;
+            this.leaf = leaf;
+        }
+
+        private NdpFilterBinaryTree(NdpOperator ndpOperator, List<ExprNodeGenericFuncDesc> exprNodes,
+                                    List<NdpFilterLeaf> ndpFilterLeafList) {
+            if (exprNodes.size() >= 2) {
+                this.ndpOperator = ndpOperator;
+                leftChild = new NdpFilterBinaryTree(exprNodes.get(0), ndpFilterLeafList);
+                if (exprNodes.size() == 2) {
+                    rightChild = new NdpFilterBinaryTree(exprNodes.get(1), ndpFilterLeafList);
+                } else {
+                    exprNodes.remove(0);
+                    rightChild = new NdpFilterBinaryTree(ndpOperator, exprNodes, ndpFilterLeafList);
+                }
+            } else {
+                LOG.info("OmniData Hive Filter failed to push down, since unsupported exprNodes.size() < 2");
+                setNdpLeafOperatorUnsupported();
+            }
+        }
+
+        public NdpFilterBinaryTree(ExprNodeGenericFuncDesc exprNode, List<NdpFilterLeaf> ndpFilterLeafList) {
+            parseFilterOperator(exprNode);
+            if (ndpOperator.equals(NdpOperator.AND) || ndpOperator.equals(NdpOperator.OR)) {
+                // and,or
+                parseAndOrOperator(exprNode, ndpFilterLeafList);
+            } else if (ndpOperator.equals(NdpOperator.NOT)) {
+                // not
+                parseNotOperator(exprNode, ndpFilterLeafList);
+            } else if (ndpOperator.equals(NdpOperator.LEAF)) {
+                // leaf
+                parseLeafOperator(exprNode, ndpFilterLeafList);
+            }
+        }
+
+        public NdpOperator getNdpOperator() {
+            return ndpOperator;
+        }
+
+        public int getLeaf() {
+            return leaf;
+        }
+
+        public NdpFilterBinaryTree getLeftChild() {
+            return leftChild;
+        }
+
+        public NdpFilterBinaryTree getRightChild() {
+            return rightChild;
+        }
+
+        private void parseAndOrOperator(ExprNodeGenericFuncDesc exprNode, List<NdpFilterLeaf> ndpFilterLeafList) {
+            ExprNodeDesc leftExpr = exprNode.getChildren().remove(0);
+            if (leftExpr instanceof ExprNodeGenericFuncDesc) {
+                // process left filter
+                leftChild = new NdpFilterBinaryTree((ExprNodeGenericFuncDesc) leftExpr, ndpFilterLeafList);
+                List<ExprNodeGenericFuncDesc> restChildren = new ArrayList<>();
+                for (ExprNodeDesc expr : exprNode.getChildren()) {
+                    if (expr instanceof ExprNodeGenericFuncDesc) {
+                        restChildren.add((ExprNodeGenericFuncDesc) expr);
+                    } else {
+                        LOG.info(
+                                "OmniData Hive Filter failed to push down, since Method parseAndOrOperator() unsupported this [{}]",
+                                expr.getClass());
+                        setNdpLeafOperatorUnsupported();
+                    }
+                }
+                // process right filter
+                if (restChildren.size() == 1) {
+                    rightChild = new NdpFilterBinaryTree(restChildren.get(0), ndpFilterLeafList);
+                } else {
+                    rightChild = new NdpFilterBinaryTree(ndpOperator, restChildren, ndpFilterLeafList);
+                }
+            } else {
+                LOG.info(
+                        "OmniData Hive Filter failed to push down, since Method parseAndOrOperator() unsupported this [{}]",
+                        leftExpr.getClass());
+                setNdpLeafOperatorUnsupported();
+            }
+        }
+
+        private void parseNotOperator(ExprNodeGenericFuncDesc exprNode, List<NdpFilterLeaf> ndpFilterLeafList) {
+            if (ndpLeafOperator == null) {
+                // GenericUDFOPNot
+                ExprNodeDesc expr = exprNode.getChildren().get(0);
+                if (expr instanceof ExprNodeGenericFuncDesc) {
+                    leftChild = new NdpFilterBinaryTree((ExprNodeGenericFuncDesc) expr, ndpFilterLeafList);
+                } else {
+                    LOG.info(
+                            "OmniData Hive Filter failed to push down, since Method parseNotOperator() unsupported this [{}]",
+                            expr.getClass());
+                    setNdpLeafOperatorUnsupported();
+                }
+            } else {
+                // EQUAL IS_NULL LEAF
+                leftChild = new NdpFilterBinaryTree(NdpOperator.LEAF, ndpLeafOperator, ndpFilterLeafList.size());
+                NdpFilterLeaf ndpFilterLeaf = new NdpFilterLeaf(ndpLeafOperator);
+                ndpFilterLeaf.parseExprLeaf(exprNode);
+                ndpFilterLeafList.add(ndpFilterLeaf);
+            }
+        }
+
+        private void parseLeafOperator(ExprNodeGenericFuncDesc exprNode, List<NdpFilterLeaf> ndpFilterLeafList) {
+            leaf = ndpFilterLeafList.size();
+            NdpFilterLeaf ndpFilterLeaf = new NdpFilterLeaf(ndpLeafOperator);
+            ndpFilterLeaf.parseExprLeaf(exprNode);
+            ndpFilterLeafList.add(ndpFilterLeaf);
+        }
+
+        private void setNdpLeafOperatorUnsupported() {
+            ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+        }
+
+        public void parseFilterOperator(ExprNodeGenericFuncDesc exprNode) {
+            Class operator = (exprNode.getGenericUDF() instanceof GenericUDFBridge)
+                    ? ((GenericUDFBridge) exprNode.getGenericUDF()).getUdfClass()
+                    : exprNode.getGenericUDF().getClass();
+            if (operator == GenericUDFOPAnd.class) {
+                ndpOperator = NdpOperator.AND;
+            } else if (operator == GenericUDFOPOr.class) {
+                ndpOperator = NdpOperator.OR;
+            } else if (operator == GenericUDFOPNot.class) {
+                ndpOperator = NdpOperator.NOT;
+            } else if (operator == GenericUDFOPNotEqual.class) {
+                ndpOperator = NdpOperator.NOT;
+                ndpLeafOperator = NdpLeafOperator.EQUAL;
+            } else if (operator == GenericUDFOPEqual.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.EQUAL;
+            } else if (operator == GenericUDFOPNotNull.class) {
+                ndpOperator = NdpOperator.NOT;
+                ndpLeafOperator = NdpLeafOperator.IS_NULL;
+            } else if (operator == GenericUDFOPNull.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.IS_NULL;
+            } else if (operator == GenericUDFOPGreaterThan.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.GREATER_THAN;
+            } else if (operator == GenericUDFOPLessThan.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.LESS_THAN;
+            } else if (operator == GenericUDFOPEqualOrGreaterThan.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.GREATER_THAN_OR_EQUAL;
+            } else if (operator == GenericUDFOPEqualOrLessThan.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.LESS_THAN_OR_EQUAL;
+            } else if (operator == GenericUDFBetween.class) {
+                ndpOperator = (Boolean) ((ExprNodeConstantDesc) exprNode.getChildren().get(0)).getValue()
+                        ? NdpOperator.NOT
+                        : NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.BETWEEN;
+            } else if (operator == GenericUDFIn.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.IN;
+            } else if (operator == UDFLike.class) {
+                ndpOperator = NdpOperator.LEAF;
+                ndpLeafOperator = NdpLeafOperator.LIKE;
+            } else {
+                ndpOperator = NdpOperator.LEAF;
+                LOG.info(
+                        "OmniData Hive Filter failed to push down, since Method parseFilterOperator() unsupported this [{}]",
+                        operator);
+                setNdpLeafOperatorUnsupported();
+            }
+        }
+    }
+
+    public enum NdpFilterMode {
+        /**
+         * All filter push down
+         */
+        ALL,
+        /**
+         * part filter push down
+         */
+        PART,
+        /**
+         * no filter push down
+         */
+        NONE
+    }
+
+    public enum NdpOperator {
+        OR,
+        AND,
+        NOT,
+        LEAF
+    }
+
+    public enum NdpLeafOperator {
+        BETWEEN,
+        IN,
+        LESS_THAN,
+        GREATER_THAN,
+        LESS_THAN_OR_EQUAL,
+        GREATER_THAN_OR_EQUAL,
+        EQUAL,
+        LIKE,
+        IS_NULL,
+        UNSUPPORTED
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilterLeaf.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilterLeaf.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilterLeaf.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpFilterLeaf.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,277 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.filter;
+
+import org.apache.hadoop.hive.ql.omnidata.operator.filter.NdpFilter.*;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Ndp Filter Leaf
+ * <p>
+ * Supported ExprNodeDesc:
+ * 1.ExprNodeConstantDesc
+ * 2.ExprNodeColumnDesc
+ * 3.ExprNodeGenericFuncDesc
+ * <p>
+ * Example: SELECT * FROM table1,table2 where table1.a = table2.b;
+ * ndpLeafOperator : =
+ * leftFilterFunc : table1.a
+ * rightFilterFunc : table2.b
+ *
+ * @since 2021-11-23
+ */
+public class NdpFilterLeaf {
+    private static final Logger LOG = LoggerFactory.getLogger(NdpFilterLeaf.class);
+
+    private NdpLeafOperator ndpLeafOperator;
+
+    private List<Object> constantList = new ArrayList<>();
+
+    private NdpFilterFunc leftFilterFunc;
+
+    private NdpFilterFunc rightFilterFunc;
+
+    public NdpFilterLeaf(NdpLeafOperator ndpLeafOperator) {
+        this.ndpLeafOperator = ndpLeafOperator;
+    }
+
+    public void parseExprLeaf(ExprNodeGenericFuncDesc exprLeaf) {
+        if (ndpLeafOperator.equals(NdpLeafOperator.UNSUPPORTED)) {
+            return;
+        }
+        int argumentIndex = 0;
+        if (ndpLeafOperator.equals(NdpLeafOperator.BETWEEN)) {
+            // first argument for BETWEEN should be boolean type
+            argumentIndex = 1;
+        }
+        ExprNodeDesc exprNodeDesc = exprLeaf.getChildren().get(argumentIndex);
+        if (exprNodeDesc instanceof ExprNodeColumnDesc) {
+            setLeftFilterFunc((ExprNodeColumnDesc) exprNodeDesc);
+        } else if (exprNodeDesc instanceof ExprNodeGenericFuncDesc) {
+            // exists Udf
+            parseLeftUdfExpr((ExprNodeGenericFuncDesc) exprNodeDesc);
+        } else {
+            LOG.info("OmniData Hive Filter [{}] failed to push down, since unsupported this class: [{}]",
+                this.ndpLeafOperator.name(), exprNodeDesc.getClass());
+            this.ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+            return;
+        }
+        switch (ndpLeafOperator) {
+            case IN:
+                parseInExprValue(exprLeaf);
+                break;
+            case BETWEEN:
+                parseBetweenExprValue(exprLeaf);
+                break;
+            case EQUAL:
+            case GREATER_THAN:
+            case LESS_THAN:
+            case GREATER_THAN_OR_EQUAL:
+            case LESS_THAN_OR_EQUAL:
+            case LIKE:
+                parseCompareExprValue(exprLeaf);
+                break;
+        }
+    }
+
+    public NdpLeafOperator getNdpLeafOperator() {
+        return ndpLeafOperator;
+    }
+
+    public NdpFilterFunc getLeftFilterFunc() {
+        return leftFilterFunc;
+    }
+
+    public void setLeftFilterFunc(String returnType, ExprNodeColumnDesc columnDesc, ExprNodeDesc udfExpression) {
+        leftFilterFunc = new NdpFilterFunc(returnType, columnDesc.getColumn(),
+            columnDesc.getIsPartitionColOrVirtualCol(), udfExpression);
+    }
+
+    public void setLeftFilterFunc(ExprNodeColumnDesc columnDesc) {
+        leftFilterFunc = new NdpFilterFunc(columnDesc.getTypeString(), columnDesc.getColumn(),
+            columnDesc.getIsPartitionColOrVirtualCol(), null);
+    }
+
+    public NdpFilterFunc getRightFilterFunc() {
+        return rightFilterFunc;
+    }
+
+    public void setRightFilterFunc(String returnType, ExprNodeColumnDesc columnDesc, ExprNodeDesc udfExpression) {
+        rightFilterFunc = new NdpFilterFunc(returnType, columnDesc.getColumn(),
+            columnDesc.getIsPartitionColOrVirtualCol(), udfExpression);
+    }
+
+    public void setRightFilterFunc(ExprNodeColumnDesc columnDesc) {
+        rightFilterFunc = new NdpFilterFunc(columnDesc.getTypeString(), columnDesc.getColumn(),
+            columnDesc.getIsPartitionColOrVirtualCol(), null);
+    }
+
+    public List<Object> getConstantList() {
+        return constantList;
+    }
+
+    private void parseUdfExpr(ExprNodeGenericFuncDesc udfExpr, boolean isLeftFilterFunc) {
+        ExprNodeDesc exprNodeDesc = udfExpr;
+        while (exprNodeDesc.getChildren() != null) {
+            exprNodeDesc = exprNodeDesc.getChildren().get(0);
+        }
+        if (exprNodeDesc instanceof ExprNodeColumnDesc) {
+            String udfReturnType = udfExpr.getTypeString();
+            if (isLeftFilterFunc) {
+                setLeftFilterFunc(udfReturnType, (ExprNodeColumnDesc) exprNodeDesc, udfExpr);
+            } else {
+                setRightFilterFunc(udfReturnType, (ExprNodeColumnDesc) exprNodeDesc, udfExpr);
+            }
+        } else {
+            LOG.info("OmniData Hive Filter [{}] failed to push down, since unsupported this class: [{}]",
+                this.ndpLeafOperator.name(), exprNodeDesc.getClass());
+            this.ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+        }
+    }
+
+    private void parseLeftUdfExpr(ExprNodeGenericFuncDesc udfExpr) {
+        parseUdfExpr(udfExpr, true);
+    }
+
+    private void parseRightUdfExpr(ExprNodeGenericFuncDesc udfExpr) {
+        parseUdfExpr(udfExpr, false);
+    }
+
+    /**
+     * name = "in"
+     * _FUNC_(val1, val2...) - returns true if test equals any valN
+     */
+    private void parseInExprValue(ExprNodeGenericFuncDesc inExpr) {
+        // start at index 1, since at 0 is the variable from table column
+        for (ExprNodeDesc expr : inExpr.getChildren().subList(1, inExpr.getChildren().size())) {
+            if (expr instanceof ExprNodeConstantDesc) {
+                constantList.add(((ExprNodeConstantDesc) expr).getValue());
+            } else {
+                LOG.info("OmniData Hive Filter [IN] failed to push down, since unsupported this class: [{}]",
+                    expr.getClass());
+                this.ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+                return;
+            }
+        }
+    }
+
+    /**
+     * name = "between"
+     * _FUNC_ a [NOT] BETWEEN b AND c - evaluate if a is [not] in between b and c
+     */
+    private void parseBetweenExprValue(ExprNodeGenericFuncDesc betweenExpr) {
+        if (betweenExpr.getChildren().get(2) instanceof ExprNodeConstantDesc && betweenExpr.getChildren()
+            .get(3) instanceof ExprNodeConstantDesc) {
+            ExprNodeConstantDesc leftValueInfo = (ExprNodeConstantDesc) betweenExpr.getChildren().get(2);
+            ExprNodeConstantDesc rightValueInfo = (ExprNodeConstantDesc) betweenExpr.getChildren().get(3);
+            constantList.add(leftValueInfo.getValue());
+            constantList.add(rightValueInfo.getValue());
+        } else {
+            LOG.info("OmniData Hive Filter [BETWEEN] failed to push down, since unsupported this class: [{}] or [{}]",
+                betweenExpr.getChildren().get(2).getClass(), betweenExpr.getChildren().get(3).getClass());
+            this.ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+        }
+    }
+
+    private void parseCompareExprValue(ExprNodeGenericFuncDesc compareExpr) {
+        ExprNodeDesc exprNodeDesc = compareExpr.getChildren().get(1);
+        if (exprNodeDesc instanceof ExprNodeConstantDesc) {
+            // On the right is a constant expression
+            constantList.add(((ExprNodeConstantDesc) exprNodeDesc).getValue());
+        } else if (exprNodeDesc instanceof ExprNodeColumnDesc) {
+            // On the right is a column
+            setRightFilterFunc((ExprNodeColumnDesc) exprNodeDesc);
+        } else if (exprNodeDesc instanceof ExprNodeGenericFuncDesc) {
+            // On the right is a UDF expression
+            parseRightUdfExpr((ExprNodeGenericFuncDesc) exprNodeDesc);
+        } else {
+            LOG.info("OmniData Hive Filter [{}] failed to push down, since unsupported this class: [{}]",
+                this.ndpLeafOperator.name(), exprNodeDesc.getClass());
+            this.ndpLeafOperator = NdpLeafOperator.UNSUPPORTED;
+        }
+    }
+
+    public static class NdpFilterFunc {
+        /**
+         * if Func exists UDF, returnType is UDF's return type
+         */
+        private String returnType;
+
+        private String columnName;
+
+        private Boolean isPartitionColumn;
+
+        private ExprNodeDesc udfExpression;
+
+        public NdpFilterFunc(String returnType, String columnName, Boolean isPartitionColumn,
+            ExprNodeDesc udfExpression) {
+            this.returnType = returnType;
+            this.columnName = columnName;
+            this.isPartitionColumn = isPartitionColumn;
+            this.udfExpression = udfExpression;
+        }
+
+        public String getReturnType() {
+            return returnType;
+        }
+
+        public void setReturnType(String returnType) {
+            this.returnType = returnType;
+        }
+
+        public String getColumnName() {
+            return columnName;
+        }
+
+        public void setColumnName(String columnName) {
+            this.columnName = columnName;
+        }
+
+        public Boolean getPartitionColumn() {
+            return isPartitionColumn;
+        }
+
+        public void setPartitionColumn(Boolean partitionColumn) {
+            isPartitionColumn = partitionColumn;
+        }
+
+        public ExprNodeDesc getUdfExpression() {
+            return udfExpression;
+        }
+
+        public void setUdfExpression(ExprNodeDesc udfExpression) {
+            this.udfExpression = udfExpression;
+        }
+
+        public Boolean isExistsUdf() {
+            return udfExpression != null;
+        }
+
+    }
+
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpUdfExpression.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpUdfExpression.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpUdfExpression.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/NdpUdfExpression.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,334 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.filter;
+
+import static io.prestosql.spi.type.BigintType.BIGINT;
+
+import io.prestosql.spi.connector.QualifiedObjectName;
+import io.prestosql.spi.function.BuiltInFunctionHandle;
+import io.prestosql.spi.function.FunctionKind;
+import io.prestosql.spi.function.Signature;
+import io.prestosql.spi.relation.CallExpression;
+import io.prestosql.spi.relation.InputReferenceExpression;
+import io.prestosql.spi.relation.RowExpression;
+import io.prestosql.spi.type.Type;
+import io.prestosql.spi.type.TypeSignature;
+
+import org.apache.hadoop.hive.ql.omnidata.OmniDataUtils;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpUdfEnum;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.OmniDataPredicate;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Optional;
+
+/**
+ * Hive Udf Expression
+ */
+public class NdpUdfExpression {
+
+    private static final Logger LOG = LoggerFactory.getLogger(NdpUdfExpression.class);
+
+    private final OmniDataPredicate predicateInfo;
+
+    private boolean isPushDownUdf = true;
+
+    public NdpUdfExpression(OmniDataPredicate predicateInfo) {
+        this.predicateInfo = predicateInfo;
+    }
+
+    public boolean isPushDownUdf() {
+        return isPushDownUdf;
+    }
+
+    public void createNdpUdf(ExprNodeDesc desc, List<RowExpression> arguments) {
+        if (!(desc instanceof ExprNodeGenericFuncDesc)) {
+            isPushDownUdf = false;
+            LOG.info("OmniData Hive UDF failed to push down, since unsupported this ExprNodeDesc: [{}]",
+                    desc.getClass());
+            return;
+        }
+        ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;
+        GenericUDF genericUDF = funcDesc.getGenericUDF();
+        switch (NdpUdfEnum.getUdfType(genericUDF)) {
+            case CAST:
+                createNdpCast(funcDesc, arguments);
+                break;
+            case INSTR:
+                createNdpInstr(funcDesc, arguments);
+                break;
+            case LENGTH:
+                createNdpLength(funcDesc, arguments);
+                break;
+            case LOWER:
+                createNdpLower(funcDesc, arguments);
+                break;
+            case REPLACE:
+                createNdpReplace(funcDesc, arguments);
+                break;
+            case SPLIT:
+                createNdpSplit(funcDesc, arguments);
+                break;
+            case SUBSCRIPT:
+                createNdpGetarrayitem(funcDesc, arguments);
+                break;
+            case SUBSTR:
+            case SUBSTRING:
+                createNdpSubstr(funcDesc, arguments);
+                break;
+            case UPPER:
+                createNdpUpper(funcDesc, arguments);
+                break;
+            case UNSUPPORTED:
+            default:
+                isPushDownUdf = false;
+                LOG.info("OmniData Hive UDF failed to push down, since unsupported this genericUDF: [{}]", genericUDF.getClass());
+        }
+    }
+
+    private void createNdpSplit(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String operatorName = NdpUdfEnum.SPLIT.getSignatureName();
+        Type strType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(0).getTypeString());
+        Type regexType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(1).getTypeString());
+        Type returnType = OmniDataUtils.transOmniDataType(funcDesc.getTypeString());
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, strType, rowArguments);
+
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren()
+                        .get(1)).getValue().toString(), regexType));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.SPLIT.getOperatorName()), FunctionKind.SCALAR,
+                new TypeSignature(returnType.toString()), new TypeSignature(strType.toString()),
+                new TypeSignature(regexType.toString()));
+        CallExpression callExpression = new CallExpression(operatorName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    /**
+     * name = "substr,substring"
+     * _FUNC_(str, pos[, len]) - returns the substring of str that starts at pos and is of length len or
+     * _FUNC_(bin, pos[, len]) - returns the slice of byte array that starts at pos and is of length len
+     * pos is a 1-based index. If pos<0 the starting position is determined by counting backwards from the end of str.
+     * Example: SELECT _FUNC_('Facebook', 5) FROM src LIMIT 1;
+     * result: book
+     * SELECT _FUNC_('Facebook', -5) FROM src LIMIT 1;
+     * result: ebook
+     * SELECT _FUNC_('Facebook', 5, 1) FROM src LIMIT 1;
+     * result: b
+     */
+    private void createNdpSubstr(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String operatorName = NdpUdfEnum.SUBSTR.getSignatureName();
+        Type strType = OmniDataUtils.transOmniDataUdfType(
+                funcDesc.getChildren().get(0).getTypeInfo());
+        Type lenType = BIGINT;
+        Type posType = BIGINT;
+        Type returnType = strType;
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, strType, rowArguments);
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren().get(1)).getValue().toString(),
+                lenType));
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren().get(2)).getValue().toString(),
+                posType));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.SUBSTR.getOperatorName()), FunctionKind.SCALAR,
+                returnType.getTypeSignature(), strType.getTypeSignature(), posType.getTypeSignature(),
+                lenType.getTypeSignature());
+        CallExpression callExpression = new CallExpression(operatorName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    /**
+     * name = "length"
+     * FUNC_(str | binary) - Returns the length of str or number of bytes in binary data
+     * Example: SELECT _FUNC_('Facebook') FROM src LIMIT 1;
+     * result: 8
+     */
+    private void createNdpLength(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String signatureName = NdpUdfEnum.LENGTH.getSignatureName();
+        Type childType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(0).getTypeString());
+        Type returnType = BIGINT;
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, childType, rowArguments);
+
+        Signature signature = new Signature(QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.LENGTH.getOperatorName()),
+                FunctionKind.SCALAR, new TypeSignature(returnType.toString()), new TypeSignature(childType.toString()));
+        CallExpression callExpression = new CallExpression(signatureName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+
+    }
+
+    /**
+     * name = "replace"
+     * _FUNC_(str, search, rep) - replace all substrings of 'str' that match 'search' with 'rep'
+     * Example: SELECT _FUNC_('Facebook') FROM src LIMIT 1;
+     * result: 'FACEBOOK'
+     */
+    private void createNdpReplace(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String operatorName = NdpUdfEnum.REPLACE.getSignatureName();
+        Type srcType = OmniDataUtils.transOmniDataUdfType(
+                funcDesc.getChildren().get(0).getTypeInfo());
+        Type searchType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(1).getTypeString());
+        Type replaceType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(2).getTypeString());
+        Type returnType = OmniDataUtils.transOmniDataType(funcDesc.getTypeString());
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, srcType, rowArguments);
+
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren()
+                        .get(1)).getValue().toString(), searchType));
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren()
+                        .get(2)).getValue().toString(), replaceType));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.REPLACE.getOperatorName()), FunctionKind.SCALAR,
+                returnType.getTypeSignature(), srcType.getTypeSignature(),
+                searchType.getTypeSignature(), replaceType.getTypeSignature());
+        CallExpression callExpression = new CallExpression(operatorName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    /**
+     * name = "upper,ucase"
+     * _FUNC_(str) - Returns str with all characters changed to uppercase
+     * Example: SELECT _FUNC_('Facebook') FROM src LIMIT 1;
+     * result: 'FACEBOOK'
+     */
+    private void createNdpUpper(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        createNdpSingleParameter(NdpUdfEnum.UPPER, funcDesc, arguments);
+    }
+
+    /**
+     * name = "lower,ucase"
+     * _FUNC_(str) - Returns str with all characters changed to lowercase
+     * Example: SELECT _FUNC_('Facebook') FROM src LIMIT 1;
+     * result: 'facebook'
+     */
+    private void createNdpLower(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        createNdpSingleParameter(NdpUdfEnum.LOWER, funcDesc, arguments);
+    }
+
+    private void createNdpCast(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        createNdpSingleParameter(NdpUdfEnum.CAST, funcDesc, arguments);
+    }
+
+    /**
+     * name = "index"
+     * _FUNC_(a, n) - Returns the n-th element of a
+     */
+    private void createNdpGetarrayitem(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String operatorName = NdpUdfEnum.SUBSCRIPT.getSignatureName();
+        Type strType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(0).getTypeString());
+        Type ordinalType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(1).getTypeString());
+        Type returnType = OmniDataUtils.transOmniDataType(funcDesc.getTypeString());
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, strType, rowArguments);
+
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren()
+                        .get(1)).getValue().toString(), ordinalType));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.SUBSCRIPT.getOperatorName()), FunctionKind.SCALAR,
+                new TypeSignature(returnType.toString()), new TypeSignature(strType.toString()),
+                new TypeSignature(ordinalType.toString()));
+        CallExpression callExpression = new CallExpression(operatorName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    /**
+     * name = "instr"
+     * _FUNC_(str, substr) - Returns the index of the first occurance of substr in str
+     * Example: SELECT _FUNC_('Facebook', 'boo') FROM src LIMIT 1;
+     * result: 5
+     */
+    private void createNdpInstr(ExprNodeGenericFuncDesc funcDesc, List<RowExpression> arguments) {
+        String operatorName = NdpUdfEnum.INSTR.getSignatureName();
+        Type strType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(0).getTypeString());
+        Type substrType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(1).getTypeString());
+        Type returnType = OmniDataUtils.transOmniDataType(funcDesc.getTypeString());
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, strType, rowArguments);
+
+        rowArguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                ((ExprNodeConstantDesc) funcDesc.getChildren()
+                        .get(1)).getValue().toString(), substrType));
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction(NdpUdfEnum.INSTR.getOperatorName()), FunctionKind.SCALAR,
+                new TypeSignature(returnType.toString()), new TypeSignature(strType.toString()),
+                new TypeSignature(substrType.toString()));
+        CallExpression callExpression = new CallExpression(operatorName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    private void createNdpSingleParameter(NdpUdfEnum udfEnum, ExprNodeGenericFuncDesc funcDesc,
+                                          List<RowExpression> arguments) {
+        String signatureName = udfEnum.getSignatureName();
+        Type childType = OmniDataUtils.transOmniDataType(
+                funcDesc.getChildren().get(0).getTypeString());
+        Type returnType = OmniDataUtils.transOmniDataType(funcDesc.getTypeString());
+        List<RowExpression> rowArguments = new ArrayList<>();
+        checkAttributeReference(funcDesc, childType, rowArguments);
+
+        Signature signature = new Signature(QualifiedObjectName.valueOfDefaultFunction(udfEnum.getOperatorName()),
+                FunctionKind.SCALAR, new TypeSignature(returnType.toString()), new TypeSignature(childType.toString()));
+        CallExpression callExpression = new CallExpression(signatureName, new BuiltInFunctionHandle(signature),
+                returnType, rowArguments, Optional.empty());
+        arguments.add(callExpression);
+    }
+
+    private void checkAttributeReference(ExprNodeDesc desc, Type childType, List<RowExpression> rowArguments) {
+        ExprNodeDesc childDesc = desc.getChildren().get(0);
+        if (childDesc instanceof ExprNodeGenericFuncDesc) {
+            createNdpUdf(childDesc, rowArguments);
+        } else if (childDesc instanceof ExprNodeColumnDesc) {
+            int colIndex = predicateInfo.getColName2ColIndex().get(((ExprNodeColumnDesc) childDesc).getColumn());
+            rowArguments.add(new InputReferenceExpression(colIndex, childType));
+        } else {
+            isPushDownUdf = false;
+            LOG.info("OmniData Hive UDF failed to push down, since unsupported this ExprNodeDesc: [{}]",
+                    childDesc.getClass());
+        }
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/OmniDataFilter.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/OmniDataFilter.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/OmniDataFilter.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/filter/OmniDataFilter.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,295 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.filter;
+
+import org.apache.hadoop.hive.ql.omnidata.operator.filter.NdpFilter.*;
+
+import static io.prestosql.spi.function.Signature.internalOperator;
+import static io.prestosql.spi.type.BooleanType.BOOLEAN;
+import static io.prestosql.spi.type.LikePatternType.LIKE_PATTERN;
+import static io.prestosql.spi.type.VarcharType.VARCHAR;
+
+import com.google.common.collect.ImmutableList;
+
+import io.prestosql.spi.connector.QualifiedObjectName;
+import io.prestosql.spi.function.BuiltInFunctionHandle;
+import io.prestosql.spi.function.FunctionKind;
+import io.prestosql.spi.function.OperatorType;
+import io.prestosql.spi.function.Signature;
+import io.prestosql.spi.relation.CallExpression;
+import io.prestosql.spi.relation.ConstantExpression;
+import io.prestosql.spi.relation.InputReferenceExpression;
+import io.prestosql.spi.relation.RowExpression;
+import io.prestosql.spi.relation.SpecialForm;
+import io.prestosql.spi.type.Type;
+
+import org.apache.hadoop.hive.ql.omnidata.OmniDataUtils;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.OmniDataPredicate;
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpPlanChecker;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.udf.UDFToString;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+import java.util.Optional;
+
+/**
+ * Filter Expression
+ *
+ * @since 2021-11-15
+ */
+public class OmniDataFilter {
+
+    private static final Logger LOG = LoggerFactory.getLogger(OmniDataFilter.class);
+
+    private List<NdpFilterLeaf> ndpFilterLeafList;
+
+    private OmniDataPredicate predicateInfo;
+
+    private boolean isPushDownFilter = true;
+
+    public OmniDataFilter(OmniDataPredicate predicateInfo) {
+        this.predicateInfo = predicateInfo;
+    }
+
+    public RowExpression getFilterExpression(ExprNodeGenericFuncDesc funcDesc, NdpFilter ndpFilter) {
+        ndpFilter.createNdpFilterBinaryTree(funcDesc);
+        this.ndpFilterLeafList = ndpFilter.getNdpFilterLeafList();
+        RowExpression filterRowExpression = extractExpressionTree(ndpFilter.getBinaryTreeHead());
+        return isPushDownFilter ? filterRowExpression : null;
+    }
+
+    private List<RowExpression> getArguments(NdpFilterBinaryTree expressionTree) {
+        List<RowExpression> arguments = new ArrayList<>();
+        arguments.add(extractExpressionTree(expressionTree.getLeftChild()));
+        if (expressionTree.getRightChild() != null) {
+            arguments.add(extractExpressionTree(expressionTree.getRightChild()));
+        }
+        return arguments;
+    }
+
+    private RowExpression extractExpressionTree(NdpFilterBinaryTree expressionTree) {
+        if (expressionTree == null) {
+            return null;
+        }
+        switch (expressionTree.getNdpOperator()) {
+            case AND:
+                return new SpecialForm(SpecialForm.Form.AND, BOOLEAN, getArguments(expressionTree));
+            case OR:
+                return new SpecialForm(SpecialForm.Form.OR, BOOLEAN, getArguments(expressionTree));
+            case NOT:
+                Signature signature = new Signature(QualifiedObjectName.valueOfDefaultFunction("not"),
+                        FunctionKind.SCALAR, BOOLEAN.getTypeSignature(), BOOLEAN.getTypeSignature());
+                return new CallExpression("not", new BuiltInFunctionHandle(signature), BOOLEAN,
+                        getArguments(expressionTree), Optional.empty());
+            case LEAF:
+                return getLeafRowExpression(expressionTree);
+            default:
+                return null;
+        }
+    }
+
+    private RowExpression getLeafRowExpression(NdpFilterBinaryTree expressionTree) {
+        int leaf = expressionTree.getLeaf();
+        if (!(leaf >= 0 && leaf < ndpFilterLeafList.size())) {
+            LOG.error("OmniData Hive Filter failed to push down, leaf operator out of Index");
+            isPushDownFilter = false;
+            return null;
+        }
+        NdpFilterLeaf ndpFilterLeaf = ndpFilterLeafList.get(leaf);
+        NdpLeafOperator leafOperator = ndpFilterLeaf.getNdpLeafOperator();
+        // leaf operator needs white list verification
+        if (!NdpPlanChecker.checkLeafOperatorByWhiteList(leafOperator)) {
+            isPushDownFilter = false;
+            return null;
+        }
+        switch (leafOperator) {
+            case EQUAL:
+            case GREATER_THAN:
+            case GREATER_THAN_OR_EQUAL:
+            case LESS_THAN:
+            case LESS_THAN_OR_EQUAL:
+                return getCompareExpression(ndpFilterLeaf);
+            case BETWEEN:
+                return getBetweenExpression(ndpFilterLeaf);
+            case IN:
+                return getInExpression(ndpFilterLeaf);
+            case IS_NULL:
+                return getIsNullExpression(ndpFilterLeaf);
+            case LIKE:
+                return getLikeExpression(ndpFilterLeaf);
+            default:
+                isPushDownFilter = false;
+                return null;
+        }
+    }
+
+    private RowExpression getBetweenExpression(NdpFilterLeaf ndpFilterLeaf) {
+        List<RowExpression> betweenArguments = new ArrayList<>();
+        // Only the left expression exists BETWEEN.
+        NdpFilterLeaf.NdpFilterFunc leftFilterFunc = ndpFilterLeaf.getLeftFilterFunc();
+        if (leftFilterFunc.isExistsUdf()) {
+            addUdfExpression(leftFilterFunc, betweenArguments);
+        } else {
+            addColumnExpression(leftFilterFunc, betweenArguments);
+        }
+        addConstantExpression(leftFilterFunc, ndpFilterLeaf.getConstantList(), betweenArguments, -1);
+        return new SpecialForm(SpecialForm.Form.BETWEEN, BOOLEAN, betweenArguments);
+    }
+
+    private RowExpression getInExpression(NdpFilterLeaf ndpFilterLeaf) {
+        int charLength = -1;
+        List<RowExpression> inArguments = new ArrayList<>();
+        // Only the left expression exists IN.
+        NdpFilterLeaf.NdpFilterFunc leftFilterFunc = ndpFilterLeaf.getLeftFilterFunc();
+        if (leftFilterFunc.isExistsUdf()) {
+            addUdfExpression(leftFilterFunc, inArguments);
+            charLength = getCharLength(leftFilterFunc.getUdfExpression());
+        } else {
+            addColumnExpression(leftFilterFunc, inArguments);
+        }
+        addConstantExpression(leftFilterFunc, ndpFilterLeaf.getConstantList(), inArguments, charLength);
+        return new SpecialForm(SpecialForm.Form.IN, BOOLEAN, inArguments);
+    }
+
+    /**
+     * The Hive char type needs to be processed in the 'in' operator before being converted to the OmniData Server.
+     *
+     * @param exprNodeDesc Udf Expression
+     * @return char length
+     */
+    private int getCharLength(ExprNodeDesc exprNodeDesc) {
+        int charLength = -1;
+        if (exprNodeDesc instanceof ExprNodeGenericFuncDesc) {
+            ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) exprNodeDesc;
+            GenericUDF genericUDF = funcDesc.getGenericUDF();
+            Class udfClass = (genericUDF instanceof GenericUDFBridge)
+                    ? ((GenericUDFBridge) genericUDF).getUdfClass()
+                    : genericUDF.getClass();
+            if (udfClass == UDFToString.class && funcDesc.getChildren().get(0) instanceof ExprNodeColumnDesc) {
+                ExprNodeColumnDesc columnDesc = (ExprNodeColumnDesc) funcDesc.getChildren().get(0);
+                if (columnDesc.getTypeInfo() instanceof CharTypeInfo) {
+                    charLength = ((CharTypeInfo) columnDesc.getTypeInfo()).getLength();
+                }
+            }
+        }
+        return charLength;
+    }
+
+    private RowExpression getIsNullExpression(NdpFilterLeaf ndpFilterLeaf) {
+        Type omniDataType = OmniDataUtils.transOmniDataType(ndpFilterLeaf.getLeftFilterFunc().getReturnType());
+        List<RowExpression> isNullArguments = new ArrayList<>();
+        isNullArguments.add(new InputReferenceExpression(
+                predicateInfo.getColName2ColIndex().get(ndpFilterLeaf.getLeftFilterFunc().getColumnName()), omniDataType));
+        return new SpecialForm(SpecialForm.Form.IS_NULL, BOOLEAN, isNullArguments);
+    }
+
+    private RowExpression getLikeExpression(NdpFilterLeaf ndpFilterLeaf) {
+        Type omniDataType = OmniDataUtils.transOmniDataType(ndpFilterLeaf.getLeftFilterFunc().getReturnType());
+        Signature signatureCast = internalOperator(OperatorType.CAST, LIKE_PATTERN, ImmutableList.of(VARCHAR));
+        List<RowExpression> castArguments = new ArrayList<>();
+        castArguments.add(
+                OmniDataUtils.transOmniDataConstantExpr(ndpFilterLeaf.getConstantList().get(0).toString(), omniDataType));
+        CallExpression castCallExpression = new CallExpression("CAST", new BuiltInFunctionHandle(signatureCast),
+                LIKE_PATTERN, castArguments);
+        Signature signatureLike = new Signature(QualifiedObjectName.valueOfDefaultFunction("LIKE"), FunctionKind.SCALAR,
+                BOOLEAN.getTypeSignature(), omniDataType.getTypeSignature(), LIKE_PATTERN.getTypeSignature());
+        List<RowExpression> likeArguments = new ArrayList<>();
+        likeArguments.add(new InputReferenceExpression(
+                predicateInfo.getColName2ColIndex().get(ndpFilterLeaf.getLeftFilterFunc().getColumnName()), omniDataType));
+        likeArguments.add(castCallExpression);
+        return new CallExpression("LIKE", new BuiltInFunctionHandle(signatureLike), BOOLEAN, likeArguments);
+    }
+
+    private RowExpression getCompareExpression(NdpFilterLeaf ndpFilterLeaf) {
+        int charLength = -1;
+        List<RowExpression> compareArguments = new ArrayList<>();
+        NdpFilterLeaf.NdpFilterFunc leftFilterFunc = ndpFilterLeaf.getLeftFilterFunc();
+        // On the left is column or UDF expression.
+        if (leftFilterFunc.isExistsUdf()) {
+            addUdfExpression(leftFilterFunc, compareArguments);
+            charLength = getCharLength(leftFilterFunc.getUdfExpression());
+        } else {
+            addColumnExpression(leftFilterFunc, compareArguments);
+        }
+        // Whether there is a column or UDF expression on the right
+        if (ndpFilterLeaf.getRightFilterFunc() != null) {
+            NdpFilterLeaf.NdpFilterFunc rightFilterFunc = ndpFilterLeaf.getRightFilterFunc();
+            if (rightFilterFunc.isExistsUdf()) {
+                addUdfExpression(rightFilterFunc, compareArguments);
+            } else {
+                addColumnExpression(rightFilterFunc, compareArguments);
+            }
+        } else {
+            // On the right is a constant expression
+            addConstantExpression(leftFilterFunc, ndpFilterLeaf.getConstantList(), compareArguments, charLength);
+        }
+        Type omniDataType = OmniDataUtils.transOmniDataType(leftFilterFunc.getReturnType());
+        String signatureName = OmniDataUtils.transOmniDataOperator(ndpFilterLeaf.getNdpLeafOperator().name());
+        Signature signature = new Signature(
+                QualifiedObjectName.valueOfDefaultFunction("$operator$" + signatureName.toLowerCase(Locale.ENGLISH)),
+                FunctionKind.SCALAR, BOOLEAN.getTypeSignature(), omniDataType.getTypeSignature(),
+                omniDataType.getTypeSignature());
+        return new CallExpression(signatureName, new BuiltInFunctionHandle(signature), BOOLEAN, compareArguments);
+    }
+
+    private void addColumnExpression(NdpFilterLeaf.NdpFilterFunc ndpFilterFunc, List<RowExpression> arguments) {
+        Type omniDataType = OmniDataUtils.transOmniDataType(ndpFilterFunc.getReturnType());
+        arguments.add(
+                new InputReferenceExpression(predicateInfo.getColName2ColIndex().get(ndpFilterFunc.getColumnName()),
+                        omniDataType));
+    }
+
+    private void addUdfExpression(NdpFilterLeaf.NdpFilterFunc ndpFilterFunc, List<RowExpression> arguments) {
+        NdpUdfExpression ndpUdfExpression = new NdpUdfExpression(predicateInfo);
+        ndpUdfExpression.createNdpUdf(ndpFilterFunc.getUdfExpression(), arguments);
+        if (!ndpUdfExpression.isPushDownUdf()) {
+            isPushDownFilter = false;
+        }
+    }
+
+    private void addConstantExpression(NdpFilterLeaf.NdpFilterFunc ndpFilterFunc, List<Object> constantList,
+                                       List<RowExpression> arguments, int charLength) {
+        Type omniDataType = OmniDataUtils.transOmniDataType(ndpFilterFunc.getReturnType());
+        constantList.forEach(constant -> {
+            if (constant == null) {
+                // Handle the case : where a = null
+                arguments.add(new ConstantExpression(null, omniDataType));
+            } else {
+                if (charLength < 0) {
+                    arguments.add(OmniDataUtils.transOmniDataConstantExpr(constant.toString(), omniDataType));
+                } else {
+                    // for example: char in ('aa','bb')
+                    arguments.add(OmniDataUtils.transOmniDataConstantExpr(
+                            String.format("%-" + charLength + "s", (constant.toString())), omniDataType));
+                }
+            }
+        });
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/limit/OmniDataLimit.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/limit/OmniDataLimit.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/limit/OmniDataLimit.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/limit/OmniDataLimit.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,33 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.limit;
+
+import java.util.OptionalLong;
+
+/**
+ * OmniData Limit
+ *
+ * @since 2022-02-18
+ */
+public class OmniDataLimit {
+    public static OptionalLong getOmniDataLimit(long limit) {
+        return OptionalLong.of(limit);
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/NdpPredicateInfo.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/NdpPredicateInfo.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/NdpPredicateInfo.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/NdpPredicateInfo.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,163 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.predicate;
+
+import com.huawei.boostkit.omnidata.model.Predicate;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpVectorizedRowBatchCtx;
+
+import java.io.Serializable;
+import java.util.List;
+
+/**
+ * Ndp Predicate Info
+ *
+ * @since 2022-01-27
+ */
+public class NdpPredicateInfo implements Serializable {
+
+    private static final long serialVersionUID = 1L;
+
+    private boolean isPushDown = false;
+
+    private boolean isPushDownAgg = false;
+
+    private boolean isPushDownFilter = false;
+
+    private boolean hasPartitionColumn = false;
+
+    private Predicate predicate;
+
+    private List<Integer> outputColumns;
+
+    private List<String> decodeTypes;
+
+    private List<Boolean> decodeTypesWithAgg;
+
+    private NdpVectorizedRowBatchCtx ndpVectorizedRowBatchCtx;
+
+    private String dataFormat;
+
+    public NdpPredicateInfo() {
+    }
+
+    public NdpPredicateInfo(boolean isPushDown) {
+        this.isPushDown = isPushDown;
+    }
+
+    @JsonCreator
+    public NdpPredicateInfo(@JsonProperty("isPushDown") boolean isPushDown,
+                            @JsonProperty("isPushDownAgg") boolean isPushDownAgg,
+                            @JsonProperty("isPushDownFilter") boolean isPushDownFilter,
+                            @JsonProperty("hasPartitionColumn") boolean hasPartitionColumn, @JsonProperty("predicate") Predicate predicate,
+                            @JsonProperty("outputColumns") List<Integer> outputColumns,
+                            @JsonProperty("decodeTypes") List<String> decodeTypes,
+                            @JsonProperty("decodeTypesWithAgg") List<Boolean> decodeTypesWithAgg,
+                            @JsonProperty("ndpVectorizedRowBatchCtx") NdpVectorizedRowBatchCtx ndpVectorizedRowBatchCtx,
+                            @JsonProperty("dataFormat") String dataFormat) {
+        this.isPushDown = isPushDown;
+        this.isPushDownAgg = isPushDownAgg;
+        this.isPushDownFilter = isPushDownFilter;
+        this.hasPartitionColumn = hasPartitionColumn;
+        this.predicate = predicate;
+        this.outputColumns = outputColumns;
+        this.decodeTypes = decodeTypes;
+        this.decodeTypesWithAgg = decodeTypesWithAgg;
+        this.ndpVectorizedRowBatchCtx = ndpVectorizedRowBatchCtx;
+        this.dataFormat = dataFormat;
+    }
+
+    @JsonProperty
+    public boolean getIsPushDown() {
+        return isPushDown;
+    }
+
+    @JsonProperty
+    public boolean getIsPushDownAgg() {
+        return isPushDownAgg;
+    }
+
+    @JsonProperty
+    public boolean getIsPushDownFilter() {
+        return isPushDownFilter;
+    }
+
+    @JsonProperty
+    public boolean getHasPartitionColumn() {
+        return hasPartitionColumn;
+    }
+
+    @JsonProperty
+    public Predicate getPredicate() {
+        return predicate;
+    }
+
+    @JsonProperty
+    public List<Integer> getOutputColumns() {
+        return outputColumns;
+    }
+
+    @JsonProperty
+    public List<String> getDecodeTypes() {
+        return decodeTypes;
+    }
+
+    @JsonProperty
+    public List<Boolean> getDecodeTypesWithAgg() {
+        return decodeTypesWithAgg;
+    }
+
+    @JsonProperty
+    public NdpVectorizedRowBatchCtx getNdpVectorizedRowBatchCtx() {
+        return ndpVectorizedRowBatchCtx;
+    }
+
+    @JsonProperty
+    public String getDataFormat() {
+        return dataFormat;
+    }
+
+    public void setPredicate(Predicate predicate) {
+        this.predicate = predicate;
+    }
+
+    public void setOutputColumns(List<Integer> outputColumns) {
+        this.outputColumns = outputColumns;
+    }
+
+    public void setDecodeTypes(List<String> decodeTypes) {
+        this.decodeTypes = decodeTypes;
+    }
+
+    public void setDecodeTypesWithAgg(List<Boolean> decodeTypesWithAgg) {
+        this.decodeTypesWithAgg = decodeTypesWithAgg;
+    }
+
+    public void setNdpVectorizedRowBatchCtx(NdpVectorizedRowBatchCtx ndpVectorizedRowBatchCtx) {
+        this.ndpVectorizedRowBatchCtx = ndpVectorizedRowBatchCtx;
+    }
+
+    public void setDataFormat(String dataFormat) {
+        this.dataFormat = dataFormat;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/OmniDataPredicate.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/OmniDataPredicate.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/OmniDataPredicate.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/operator/predicate/OmniDataPredicate.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,397 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.operator.predicate;
+
+import static io.prestosql.spi.function.FunctionKind.SCALAR;
+import static io.prestosql.spi.type.BigintType.BIGINT;
+import static java.util.stream.Collectors.toList;
+
+import com.huawei.boostkit.omnidata.model.Column;
+
+import io.prestosql.spi.connector.QualifiedObjectName;
+import io.prestosql.spi.function.BuiltInFunctionHandle;
+import io.prestosql.spi.function.FunctionHandle;
+import io.prestosql.spi.function.Signature;
+import io.prestosql.spi.relation.CallExpression;
+import io.prestosql.spi.relation.InputReferenceExpression;
+import io.prestosql.spi.relation.RowExpression;
+import io.prestosql.spi.type.Type;
+
+import org.apache.hadoop.hive.ql.exec.ColumnInfo;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.omnidata.OmniDataUtils;
+import org.apache.hadoop.hive.ql.omnidata.operator.aggregation.NdpArithmeticExpressionInfo;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpArithmeticEnum;
+import org.apache.hadoop.hive.ql.plan.VectorSelectDesc;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+
+/**
+ * OmniData Predicate Info
+ * need : columns types projections decodeTypes
+ * columns: used column information
+ * projections && types indicates the returned field.
+ *
+ * @since 2021-12-07
+ */
+public class OmniDataPredicate {
+    private static final Logger LOG = LoggerFactory.getLogger(OmniDataPredicate.class);
+
+    /**
+     * returned OmniData Columns
+     */
+    private List<Column> columns = new ArrayList<>();
+
+    /**
+     * returned OmniData Types
+     */
+    private List<Type> types = new ArrayList<>();
+
+    private List<RowExpression> projections = new ArrayList<>();
+
+    private List<String> decodeTypes = new ArrayList<>();
+
+    private List<Boolean> decodeTypesWithAgg = new ArrayList<>();
+
+    private Map<String, Integer> colName2ProjectId = new HashMap<>();
+
+    private Map<String, Integer> colName2ColIndex = new HashMap<>();
+
+    private Map<Integer, Integer> colId2ColIndex = new HashMap<>();
+
+    private Map<Integer, String> colId2ColName = new HashMap<>();
+
+    private VectorExpression[] expressions;
+
+    private boolean hasPartitionColumns = false;
+
+    private boolean isPushDown = true;
+
+    public boolean isPushDown() {
+        return isPushDown;
+    }
+
+    /**
+     * initialize needed columns
+     *
+     * @param tableScanOp TableScanOperator
+     */
+    public OmniDataPredicate(TableScanOperator tableScanOp) {
+        int index = 0;
+        String[] columnTypes = tableScanOp.getSchemaEvolutionColumnsTypes().split(",");
+        String[] columnNames = tableScanOp.getSchemaEvolutionColumns().split(",");
+        for (int columnId : tableScanOp.getConf().getNeededColumnIDs()) {
+            Type omniDataType = OmniDataUtils.transOmniDataType(columnTypes[columnId]);
+            columns.add(new Column(columnId, columnNames[columnId], omniDataType));
+            colId2ColIndex.put(columnId, index);
+            colId2ColName.put(columnId, columnNames[columnId]);
+            colName2ColIndex.put(columnNames[columnId], index);
+            index++;
+        }
+
+        // add partition column
+        List<String> referencedColumns = tableScanOp.getConf().getReferencedColumns();
+        if (referencedColumns != null && referencedColumns.size() > tableScanOp.getConf().getNeededColumnIDs().size()) {
+            List<ColumnInfo> signature = tableScanOp.getSchema().getSignature();
+            List<String> names = signature.stream().map(ColumnInfo::getInternalName).collect(toList());
+            for (String columnName : referencedColumns) {
+                List<Integer> neededColumnIds = tableScanOp.getConf().getNeededColumnIDs();
+                if (!names.contains(columnName) || neededColumnIds.contains(names.indexOf(columnName))) {
+                    continue;
+                }
+                int columnId = names.indexOf(columnName);
+                ColumnInfo columnInfo = signature.get(columnId);
+                if (columnInfo.getIsVirtualCol()) {
+                    Type omniDataType = OmniDataUtils.transOmniDataType(columnInfo.getTypeName());
+                    columns.add(new Column(columnId, columnName, omniDataType, true, null));
+                    colId2ColIndex.put(columnId, index);
+                    colId2ColName.put(columnId, columnName);
+                    colName2ColIndex.put(columnName, index);
+                    hasPartitionColumns = true;
+                    index++;
+                }
+            }
+        }
+    }
+
+    public List<Column> getColumns() {
+        return columns;
+    }
+
+    public List<Type> getTypes() {
+        return types;
+    }
+
+    public List<RowExpression> getProjections() {
+        return projections;
+    }
+
+    public List<String> getDecodeTypes() {
+        return decodeTypes;
+    }
+
+    public void setDecodeTypes(List<String> decodeTypes) {
+        this.decodeTypes = decodeTypes;
+    }
+
+    public List<Boolean> getDecodeTypesWithAgg() {
+        return decodeTypesWithAgg;
+    }
+
+    public void setDecodeTypesWithAgg(List<Boolean> decodeTypesWithAgg) {
+        this.decodeTypesWithAgg = decodeTypesWithAgg;
+    }
+
+    public Map<String, Integer> getColName2ProjectId() {
+        return colName2ProjectId;
+    }
+
+    public Map<String, Integer> getColName2ColIndex() {
+        return colName2ColIndex;
+    }
+
+    public Map<Integer, Integer> getColId2ColIndex() {
+        return colId2ColIndex;
+    }
+
+    public Map<Integer, String> getColId2ColName() {
+        return colId2ColName;
+    }
+
+    public boolean getHasPartitionColumns() {
+        return hasPartitionColumns;
+    }
+
+    public void setSelectExpressions(VectorSelectDesc vectorSelectDesc) {
+        if (vectorSelectDesc != null) {
+            expressions = vectorSelectDesc.getSelectExpressions();
+        } else {
+            expressions = new VectorExpression[] {};
+        }
+    }
+
+    public VectorExpression[] getExpressions() {
+        return expressions;
+    }
+
+    private void addProjections(int outputColumnId, boolean isAgg) {
+        Type omniDataType = columns.get(colId2ColIndex.get(outputColumnId)).getType();
+        types.add(omniDataType);
+        colName2ProjectId.put(columns.get(colId2ColIndex.get(outputColumnId)).getName(), projections.size());
+        projections.add(new InputReferenceExpression(colId2ColIndex.get(outputColumnId), omniDataType));
+        decodeTypes.add(omniDataType.toString());
+        decodeTypesWithAgg.add(isAgg);
+    }
+
+    /**
+     * Use the Hive TableScanOperator to add OmniData projections
+     *
+     * @param tableScanOp Hive TableScanOperator
+     */
+    public void addProjectionsByTableScan(TableScanOperator tableScanOp) {
+        tableScanOp.getConf().getNeededColumnIDs().forEach(c -> addProjections(c, false));
+    }
+
+    public void addProjectionsByAgg(int outputColumnId) {
+        checkAndClearColumnInfo(outputColumnId);
+        if (checkColumnAlreadyExists(outputColumnId)) {
+            addProjections(outputColumnId, true);
+        } else {
+            addProjectionsByVectorExpression(outputColumnId, true);
+        }
+    }
+
+    public boolean addProjectionsByGroupByKey(int outputColumnId) {
+        if (checkColumnAlreadyExists(outputColumnId)) {
+            if (colId2ColIndex.containsKey(outputColumnId)) {
+                addProjections(outputColumnId, false);
+                return true;
+            }
+        }
+        return false;
+    }
+
+    /**
+     * count() need to add bigint decode type
+     */
+    public void addProjectionsByAggCount(int outputColumnId) {
+        checkAndClearColumnInfo(outputColumnId);
+        if (checkColumnAlreadyExists(outputColumnId)) {
+            Type omniDataType = columns.get(colId2ColIndex.get(outputColumnId)).getType();
+            types.add(omniDataType);
+            colName2ProjectId.put(columns.get(colId2ColIndex.get(outputColumnId)).getName(), projections.size());
+            projections.add(new InputReferenceExpression(colId2ColIndex.get(outputColumnId), omniDataType));
+            // count() return bigint
+            decodeTypes.add(BIGINT.toString());
+            decodeTypesWithAgg.add(false);
+        } else {
+            addProjectionsByVectorExpression(outputColumnId, false);
+            // replace with bigint decode type
+            decodeTypes.remove(decodeTypes.size() - 1);
+            decodeTypes.add(BIGINT.toString());
+        }
+    }
+
+    public boolean checkColumnAlreadyExists(int outputColumnId) {
+        return colId2ColName.containsKey(outputColumnId);
+    }
+
+    /**
+     * If the column contains one or more mathematical('+ - * / %') operators
+     * and projection has been generated, clear the column information.
+     *
+     * @param columnId Hive output id
+     */
+    private void checkAndClearColumnInfo(int columnId) {
+        if (checkColumnAlreadyExists(columnId) && !colId2ColIndex.containsKey(columnId)) {
+            colName2ProjectId.remove("" + columnId);
+            colId2ColName.remove(columnId);
+        }
+    }
+
+    private void addConstantProjections(ConstantVectorExpression constantVectorExpression) {
+        Type omniDataType = OmniDataUtils.transOmniDataType(constantVectorExpression.getOutputTypeInfo().getTypeName());
+        types.add(omniDataType);
+        // constantVectorExpressionParameters(): return "val " + value. need to remove "val "
+        String constantValue = constantVectorExpression.vectorExpressionParameters().substring("val ".length());
+        projections.add(OmniDataUtils.transOmniDataConstantExpr(constantValue, omniDataType));
+        decodeTypes.add(omniDataType.toString());
+        decodeTypesWithAgg.add(false);
+    }
+
+    /**
+     * Agg Expressions with VectorExpression
+     */
+    private void addProjectionsByVectorExpression(int outputColumnId, boolean isAgg) {
+        for (VectorExpression expression : expressions) {
+            if (expression.getOutputColumnNum() == outputColumnId) {
+                // check constant expression
+                if (expression instanceof ConstantVectorExpression) {
+                    addConstantProjections((ConstantVectorExpression) expression);
+                } else {
+                    Type omniDataType = OmniDataUtils.transOmniDataType(expression.getOutputTypeInfo().getTypeName());
+                    types.add(omniDataType);
+                    // String columnName = outputColumnId + "#SelExpression";
+                    String columnName = "" + outputColumnId;
+                    colName2ProjectId.put(columnName, projections.size());
+                    colId2ColName.put(outputColumnId, columnName);
+                    projections.add(createProjection(expression));
+                    decodeTypes.add(omniDataType.toString());
+                    decodeTypesWithAgg.add(isAgg);
+                }
+                break;
+            }
+        }
+    }
+
+    private RowExpression createProjection(VectorExpression expression) {
+        Type returnType = OmniDataUtils.transOmniDataType(expression.getOutputTypeInfo().getTypeName());
+        NdpArithmeticEnum operatorType = NdpArithmeticEnum.getArithmeticByClass(expression.getClass());
+        FunctionHandle functionHandle = createFunctionHandle(expression, operatorType, returnType);
+        if (functionHandle == null) {
+            return null;
+        }
+        List<RowExpression> arguments = createArgument(expression);
+        return new CallExpression(operatorType.name(), functionHandle, returnType, arguments, Optional.empty());
+    }
+
+    private List<RowExpression> createArgument(VectorExpression expression) {
+        List<RowExpression> arguments = new ArrayList<>();
+        VectorExpression[] childExpressions = expression.getChildExpressions();
+        if (childExpressions == null || childExpressions.length == 1) {
+            int childOutputId = (childExpressions == null ? -1 : childExpressions[0].getOutputColumnNum());
+            createArgumentCore(expression, arguments, childOutputId);
+        } else {
+            for (VectorExpression childExpression : childExpressions) {
+                arguments.add(createProjection(childExpression));
+            }
+        }
+        return arguments;
+    }
+
+    private FunctionHandle createFunctionHandle(VectorExpression expression, NdpArithmeticEnum operatorType,
+        Type returnType) {
+        FunctionHandle functionHandle;
+        String tmpType;
+        switch (operatorType) {
+            case UNSUPPORTED:
+                isPushDown = false;
+                LOG.error(operatorType.name() + " is not supported");
+                return null;
+            case CAST:
+                tmpType = expression.getInputTypeInfos()[0].getTypeName();
+                functionHandle = new BuiltInFunctionHandle(
+                    new Signature(QualifiedObjectName.valueOfDefaultFunction("$operator$" + NdpArithmeticEnum.CAST),
+                        SCALAR, returnType.getTypeSignature(),
+                        OmniDataUtils.transOmniDataType(tmpType).getTypeSignature()));
+                break;
+            case CAST_IDENTITY:
+                // CAST_IDENTITY is a special type of CAST and requires special processing.
+                operatorType = NdpArithmeticEnum.CAST;
+                tmpType = expression.getInputTypeInfos()[0].getTypeName();
+                functionHandle = new BuiltInFunctionHandle(
+                    new Signature(QualifiedObjectName.valueOfDefaultFunction("$operator$" + NdpArithmeticEnum.CAST),
+                        SCALAR, OmniDataUtils.transAggType(returnType).getTypeSignature(),
+                        OmniDataUtils.transOmniDataType(tmpType).getTypeSignature()));
+                break;
+            default:
+                Type leftOmniDataType = OmniDataUtils.transOmniDataType(
+                    expression.getInputTypeInfos()[0].getTypeName());
+                Type rightOmniDataType = OmniDataUtils.transOmniDataType(
+                    expression.getInputTypeInfos()[1].getTypeName());
+                functionHandle = new BuiltInFunctionHandle(
+                    new Signature(QualifiedObjectName.valueOfDefaultFunction("$operator$" + operatorType.name()),
+                        SCALAR, returnType.getTypeSignature(), leftOmniDataType.getTypeSignature(),
+                        rightOmniDataType.getTypeSignature()));
+        }
+        return functionHandle;
+    }
+
+    private void createArgumentCore(VectorExpression expression, List<RowExpression> arguments, int childOutputId) {
+        NdpArithmeticExpressionInfo expressionInfo = new NdpArithmeticExpressionInfo(
+            expression.vectorExpressionParameters());
+        // i -> for
+        for (int i = 0; i < expression.getInputTypeInfos().length; i++) {
+            Type omniDataType = OmniDataUtils.transOmniDataType(expression.getInputTypeInfos()[i].getTypeName());
+            if (expressionInfo.getIsVal()[i]) {
+                arguments.add(OmniDataUtils.transOmniDataConstantExpr(expressionInfo.getColValue()[i], omniDataType));
+            } else if (expressionInfo.getColId()[i] == childOutputId) {
+                arguments.add(createProjection(expression.getChildExpressions()[0]));
+            } else {
+                if (expression.getClass().getName().contains("expressions.Cast")) {
+                    isPushDown = false;
+                    LOG.error("This kind of Cast is not supported.");
+                    return;
+                }
+                arguments.add(
+                    new InputReferenceExpression(colId2ColIndex.get(expressionInfo.getColId()[i]), omniDataType));
+            }
+        }
+    }
+
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanChecker.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanChecker.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanChecker.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanChecker.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,434 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.physical;
+
+import static org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpUdfEnum.*;
+import static org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpHiveOperatorEnum.*;
+
+import com.google.common.collect.ImmutableSet;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc;
+import org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpHiveOperatorEnum;
+import org.apache.hadoop.hive.ql.omnidata.operator.enums.NdpUdfEnum;
+import org.apache.hadoop.hive.ql.omnidata.operator.filter.NdpFilter.*;
+import org.apache.hadoop.hive.ql.omnidata.status.NdpStatusInfo;
+import org.apache.hadoop.hive.ql.plan.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Locale;
+import java.util.Map;
+import java.util.Optional;
+
+/**
+ * Used to check the validity of the operation during the Ndp planning phase.
+ *
+ * @since 2022-01-14
+ */
+public class NdpPlanChecker {
+    private NdpPlanChecker() {
+    }
+
+    private static final Logger LOG = LoggerFactory.getLogger(NdpPlanChecker.class);
+
+    private static final ImmutableSet<String> SUPPORTED_HIVE_TYPES = ImmutableSet.of("bigint", "boolean", "char",
+            "date", "double", "float", "int", "smallint", "string", "tinyint", "varchar");
+
+    private static final ImmutableSet<String> SUPPORTED_AGGREGATE_FUNCTIONS = ImmutableSet.of("count", "avg", "sum",
+            "max", "min");
+
+    private static final ImmutableSet<String> AVG_SUM_FUNCTION_HIVE_TYPES = ImmutableSet.of("bigint", "double", "float",
+            "int", "smallint", "tinyint");
+
+    private static final ImmutableSet<NdpUdfEnum> SUPPORTED_HIVE_UDF = ImmutableSet.of(CAST, INSTR, LENGTH, LOWER,
+            REPLACE, SPLIT, SUBSCRIPT, SUBSTRING, UPPER, SUBSTR);
+
+    // unsupported: LIKE
+    private static final ImmutableSet<NdpHiveOperatorEnum> SUPPORTED_HIVE_OPERATOR = ImmutableSet.of(AND, BETWEEN,
+            EQUAL, GREATER_THAN, GREATER_THAN_OR_EQUAL, IN, LESS_THAN, LESS_THAN_OR_EQUAL, NOT, NOT_EQUAL, NOT_NULL, NULL,
+            OR);
+
+    // NdpLeafOperator.LIKE
+    private static final ImmutableSet<NdpLeafOperator> SUPPORTED_HIVE_LEAF_OPERATOR = ImmutableSet.of(
+            NdpLeafOperator.BETWEEN, NdpLeafOperator.IN, NdpLeafOperator.LESS_THAN, NdpLeafOperator.GREATER_THAN,
+            NdpLeafOperator.LESS_THAN_OR_EQUAL, NdpLeafOperator.GREATER_THAN_OR_EQUAL, NdpLeafOperator.EQUAL,
+            NdpLeafOperator.IS_NULL);
+
+    /**
+     * Currently, 'roll up' is not supported.
+     *
+     * @param cmd hive query sql
+     * @return true or false
+     */
+    public static boolean checkRollUp(String cmd) {
+        if (cmd.replaceAll("\\s*", "").toLowerCase().contains("rollup(")) {
+            LOG.info("SQL [{}] failed to push down, since contains unsupported operator ROLLUP", cmd);
+            return false;
+        }
+        return true;
+    }
+
+    /**
+     * Currently, only one child is supported.
+     *
+     * @param tableScanOp TableScanOperator
+     * @return true or false
+     */
+    public static boolean checkTableScanNumChild(TableScanOperator tableScanOp) {
+        if (tableScanOp.getNumChild() == 1) {
+            return true;
+        } else {
+            LOG.info("Table [{}] failed to push down, since unsupported the number of TableScanOperator's child : [{}]",
+                    tableScanOp.getConf().getAlias(), tableScanOp.getNumChild());
+            return false;
+        }
+    }
+
+    /**
+     * Currently, two data formats are supported: Orc and Parquet.
+     *
+     * @param tableScanOp TableScanOperator
+     * @param work mapWork
+     * @return 'orc' and 'parquet'
+     */
+    public static Optional<String> getDataFormat(TableScanOperator tableScanOp, BaseWork work) {
+        String tableName = tableScanOp.getConf().getAlias();
+        String format = "";
+        // TableMetadata may be 'null'
+        if (tableScanOp.getConf().getTableMetadata() == null) {
+            if (work instanceof MapWork) {
+                PartitionDesc desc = ((MapWork) work).getAliasToPartnInfo().get(tableScanOp.getConf().getAlias());
+                if (desc != null) {
+                    format = desc.getInputFileFormatClass().getSimpleName();
+                } else {
+                    LOG.info("Table [{}] failed to push down, since PartitionDesc is null", tableName);
+                }
+            }
+        } else {
+            format = tableScanOp.getConf().getTableMetadata().getInputFormatClass().getSimpleName();
+        }
+        if (format.toLowerCase(Locale.ENGLISH).contains("orc")) {
+            return Optional.of("orc");
+        } else if (format.toLowerCase(Locale.ENGLISH).contains("parquet")) {
+            return Optional.of("parquet");
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Check whether host resources are available.
+     *
+     * @param ndpStatusInfoMap ndp host resource
+     * @return true or false
+     */
+    public static boolean checkHostResources(Map<String, NdpStatusInfo> ndpStatusInfoMap) {
+        if (ndpStatusInfoMap.size() == 0) {
+            LOG.info("OmniData Hive failed to push down, the number of OmniData server is 0.");
+            return false;
+        }
+        ndpStatusInfoMap.entrySet().removeIf(info -> !checkHostResources(info.getValue()));
+        if (ndpStatusInfoMap.size() == 0) {
+            LOG.info("OmniData Hive failed to push down, the number of OmniData server is 0.");
+            return false;
+        }
+        return true;
+    }
+
+    public static boolean checkHostResources(NdpStatusInfo statusInfo) {
+        if (statusInfo == null) {
+            return false;
+        }
+        if (statusInfo.getRunningTasks() > statusInfo.getMaxTasks() * statusInfo.getThreshold()) {
+            return false;
+        }
+        return true;
+    }
+
+    /**
+     * Check whether the Udf in white list
+     *
+     * @param udf hive udf
+     * @return true or false
+     */
+    public static boolean checkUdfByWhiteList(NdpUdfEnum udf) {
+        return SUPPORTED_HIVE_UDF.contains(udf);
+    }
+
+    /**
+     * Check whether the operator in white list
+     *
+     * @param operator hive operator
+     * @return true or false
+     */
+    public static boolean checkOperatorByWhiteList(NdpHiveOperatorEnum operator) {
+        return SUPPORTED_HIVE_OPERATOR.contains(operator);
+    }
+
+    /**
+     * Check whether the leaf operator in white list
+     *
+     * @param operator hive leaf operator
+     * @return true or false
+     */
+    public static boolean checkLeafOperatorByWhiteList(NdpLeafOperator operator) {
+        return SUPPORTED_HIVE_LEAF_OPERATOR.contains(operator);
+    }
+
+    public static boolean checkHiveType(String type) {
+        String lType = type.toLowerCase(Locale.ENGLISH);
+        // Keep the English letters and remove the others. like: char(11) -> char
+        if (lType.contains("char")) {
+            lType = type.replaceAll("[^a-z<>]", "");
+        }
+        return SUPPORTED_HIVE_TYPES.contains(lType);
+    }
+
+    /**
+     * Check whether the data type is supported
+     *
+     * @param tableScanOp TableScanOperator
+     * @return true or false
+     */
+    public static boolean checkHiveType(TableScanOperator tableScanOp) {
+        String[] columnTypes = tableScanOp.getSchemaEvolutionColumnsTypes().split(",");
+        for (Integer columnId : tableScanOp.getConf().getNeededColumnIDs()) {
+            if (!checkHiveType(columnTypes[columnId])) {
+                LOG.info("Table [{}] failed to push down, since unsupported this column type: [{}]",
+                        tableScanOp.getConf().getAlias(), columnTypes[columnId]);
+                return false;
+            }
+        }
+        return true;
+    }
+
+    /**
+     * Check whether the filterOperator.
+     *
+     * @param vectorFilterOperator VectorFilterOperator
+     * @return true or false
+     */
+    public static Optional<ExprNodeGenericFuncDesc> checkFilterOperator(VectorFilterOperator vectorFilterOperator) {
+        if (vectorFilterOperator == null) {
+            return Optional.empty();
+        }
+        ExprNodeDesc nodeDesc = vectorFilterOperator.getConf().getPredicate();
+        if (nodeDesc instanceof ExprNodeGenericFuncDesc) {
+            return Optional.of((ExprNodeGenericFuncDesc) nodeDesc);
+        }
+        LOG.info("FilterOperator failed to push down, since unsupported this ExprNodeDesc: [{}]",
+                nodeDesc.getClass().getSimpleName());
+        return Optional.empty();
+    }
+
+    /**
+     * Check whether the vectorSelectOperator.
+     *
+     * @param vectorSelectOperator VectorSelectOperator
+     * @return true or false
+     */
+    public static Optional<VectorSelectDesc> checkSelectOperator(VectorSelectOperator vectorSelectOperator) {
+        if (vectorSelectOperator == null) {
+            return Optional.empty();
+        }
+        SelectDesc selectDesc = vectorSelectOperator.getConf();
+        if (selectDesc.getVectorDesc() instanceof VectorSelectDesc) {
+            return Optional.of((VectorSelectDesc) selectDesc.getVectorDesc());
+        }
+        LOG.info("VectorSelectOperator failed to push down, since unsupported this SelectDesc: [{}]",
+                selectDesc.getClass().getSimpleName());
+        return Optional.empty();
+    }
+
+    /**
+     * Check whether the vectorGroupByOperator
+     *
+     * @param vectorGroupByOperator VectorGroupByOperator
+     * @return true or false
+     */
+    public static Optional<VectorGroupByDesc> checkGroupByOperator(VectorGroupByOperator vectorGroupByOperator) {
+        if (vectorGroupByOperator != null && vectorGroupByOperator.getVectorDesc() instanceof VectorGroupByDesc) {
+            VectorGroupByDesc aggVectorsDesc = (VectorGroupByDesc) vectorGroupByOperator.getVectorDesc();
+            // Agg or groupby can be pushed down only when agg or groupby exists.
+            if (aggVectorsDesc.getKeyExpressions().length > 0 || aggVectorsDesc.getVecAggrDescs().length > 0) {
+                for (VectorAggregationDesc agg : aggVectorsDesc.getVecAggrDescs()) {
+                    if (!checkAggregationDesc(agg.getAggrDesc())) {
+                        return Optional.empty();
+                    }
+                }
+                return Optional.of(aggVectorsDesc);
+            }
+        }
+        LOG.info("VectorGroupByOperator failed to push down");
+        return Optional.empty();
+    }
+
+    /**
+     * Check whether Limit offset > 0
+     *
+     * @param vectorLimitOperator VectorLimitOperator
+     * @return true or false
+     */
+    public static Optional<LimitDesc> checkLimitOperator(VectorLimitOperator vectorLimitOperator) {
+        if (vectorLimitOperator == null) {
+            return Optional.empty();
+        }
+        LimitDesc limitDesc = vectorLimitOperator.getConf();
+        if (limitDesc.getOffset() == null && limitDesc.getLimit() > 0) {
+            return Optional.of(limitDesc);
+        }
+        LOG.info("VectorLimitOperator failed to push down, since unsupported Limit offset > 0");
+        return Optional.empty();
+    }
+
+    public static boolean checkAggregationDesc(AggregationDesc agg) {
+        if (!SUPPORTED_AGGREGATE_FUNCTIONS.contains(agg.getGenericUDAFName())) {
+            LOG.info("Aggregation failed to push down, since unsupported this [{}]", agg.getGenericUDAFName());
+            return false;
+        }
+        switch (agg.getGenericUDAFName()) {
+            case "count":
+                return checkCountFunction(agg);
+            case "avg":
+                return checkAvgFunction(agg);
+            case "sum":
+                return checkSumFunction(agg);
+            case "min":
+                return checkMinFunction(agg);
+            case "max":
+                return checkMaxFunction(agg);
+            default:
+                return false;
+        }
+    }
+
+    public static boolean checkCountFunction(AggregationDesc agg) {
+        if (agg.getDistinct()) {
+            LOG.info("Aggregation [{}] failed to push down, since unsupported [distinct]", agg.getGenericUDAFName());
+            return false;
+        }
+        for (ExprNodeDesc parameter : agg.getParameters()) {
+            if (!checkHiveType(parameter.getTypeString())) {
+                return false;
+            }
+        }
+        return true;
+    }
+
+    public static boolean checkAvgFunction(AggregationDesc agg) {
+        return checkSumFunction(agg);
+    }
+
+    public static boolean checkSumFunction(AggregationDesc agg) {
+        if (agg.getDistinct()) {
+            LOG.info("Aggregation [{}] failed to push down, since unsupported [distinct]", agg.getGenericUDAFName());
+            return false;
+        }
+        boolean isConstant = false;
+        for (ExprNodeDesc parameter : agg.getParameters()) {
+            // check whether a parameter is a constant, If all are constants, do not push down.
+            if (!(parameter instanceof ExprNodeConstantDesc)) {
+                isConstant = true;
+                if ((!checkHiveType(parameter.getTypeString())) || (!AVG_SUM_FUNCTION_HIVE_TYPES.contains(
+                        parameter.getTypeString()))) {
+                    LOG.info("Aggregation [{}] failed to push down, since unsupported this column type: [{}]",
+                            agg.getGenericUDAFName(), parameter.getTypeString());
+                    return false;
+                }
+            }
+        }
+        return isConstant;
+    }
+
+    public static boolean checkMinFunction(AggregationDesc agg) {
+        if (agg.getDistinct()) {
+            LOG.info("Aggregation [{}] failed to push down, since unsupported [distinct]", agg.getGenericUDAFName());
+            return false;
+        }
+        boolean isConstant = false;
+        for (ExprNodeDesc parameter : agg.getParameters()) {
+            // check whether a parameter is a constant, If all are constants, do not push down.
+            if (!(parameter instanceof ExprNodeConstantDesc)) {
+                isConstant = true;
+                if (!checkHiveType(parameter.getTypeString())) {
+                    return false;
+                }
+            }
+        }
+        return isConstant;
+    }
+
+    public static boolean checkMaxFunction(AggregationDesc agg) {
+        return checkMinFunction(agg);
+    }
+
+    /**
+     * Check whether the filter selectivity is supported
+     *
+     * @param tableScanOp TableScanOperator
+     * @param conf OmniDataConf
+     * @return true or false
+     */
+    public static boolean checkSelectivity(TableScanOperator tableScanOp, Configuration conf) {
+        if (OmniDataConf.getOmniDataFilterSelectivityEnabled(conf)) {
+            double currentSelectivity = NdpStatisticsUtils.getSelectivity(tableScanOp);
+            double filterSelectivity = OmniDataConf.getOmniDataFilterSelectivity(conf);
+            if (currentSelectivity > filterSelectivity) {
+                LOG.info("Table [{}] failed to push down, since selectivity[{}] > threshold[{}]",
+                        tableScanOp.getConf().getAlias(), currentSelectivity, filterSelectivity);
+                return false;
+            } else {
+                LOG.info("Table [{}] selectivity is {}", tableScanOp.getConf().getAlias(), currentSelectivity);
+                return true;
+            }
+        } else {
+            LOG.info("Table [{}] filter selectivity is unenabled", tableScanOp.getConf().getAlias());
+            return true;
+        }
+    }
+
+    /**
+     * Check whether the table size is supported
+     *
+     * @param tableScanOp TableScanOperator
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static boolean checkTableSize(TableScanOperator tableScanOp, Configuration conf) {
+        if (tableScanOp.getConf().getStatistics() == null) {
+            return false;
+        }
+        long currentTableSize = tableScanOp.getConf().getStatistics().getDataSize();
+        if (currentTableSize < OmniDataConf.getOmniDataTablesSizeThreshold(conf)) {
+            LOG.info("Table [{}] failed to push down, since table size[{}] < threshold[{}]",
+                    tableScanOp.getConf().getAlias(), currentTableSize, OmniDataConf.getOmniDataTablesSizeThreshold(conf));
+            return false;
+        }
+        return true;
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanResolver.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanResolver.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanResolver.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpPlanResolver.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,475 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.physical;
+
+import com.huawei.boostkit.omnidata.model.AggregationInfo;
+import com.huawei.boostkit.omnidata.model.Predicate;
+
+import com.google.common.collect.ImmutableMap;
+
+import io.prestosql.spi.relation.RowExpression;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc;
+import org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.lib.TaskGraphWalker;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.hive.ql.omnidata.operator.aggregation.OmniDataAggregation;
+import org.apache.hadoop.hive.ql.omnidata.operator.filter.NdpFilter;
+import org.apache.hadoop.hive.ql.omnidata.operator.filter.OmniDataFilter;
+import org.apache.hadoop.hive.ql.omnidata.operator.limit.OmniDataLimit;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.OmniDataPredicate;
+import org.apache.hadoop.hive.ql.omnidata.serialize.NdpSerializationUtils;
+import org.apache.hadoop.hive.ql.omnidata.status.NdpStatusInfo;
+import org.apache.hadoop.hive.ql.omnidata.status.NdpStatusManager;
+import org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext;
+import org.apache.hadoop.hive.ql.optimizer.physical.PhysicalPlanResolver;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.BaseWork;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.plan.LimitDesc;
+import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.TezWork;
+import org.apache.hadoop.hive.ql.plan.VectorGroupByDesc;
+import org.apache.hadoop.hive.ql.plan.VectorSelectDesc;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.Serializable;
+import java.util.*;
+import java.util.stream.Collectors;
+
+/**
+ * Ndp Physical plan optimization
+ */
+public class NdpPlanResolver implements PhysicalPlanResolver {
+    private static final Logger LOG = LoggerFactory.getLogger(NdpPlanResolver.class);
+
+    private HiveConf hiveConf;
+
+    private Context context;
+
+    private ParseContext parseContext;
+
+    private boolean isPushDownFilter = false;
+
+    private boolean isPushDownPartFilter = false;
+
+    private boolean isPushDownSelect = false;
+
+    private boolean isPushDownAgg = false;
+
+    private boolean isPushDownLimit = false;
+
+    /**
+     * Hive filter expression
+     */
+    private ExprNodeGenericFuncDesc filterDesc;
+
+    /**
+     * Hive does not support push-down filter expressions.
+     */
+    private ExprNodeDesc unsupportedFilterDesc;
+
+    /**
+     * Hive select expression
+     */
+    private VectorSelectDesc selectDesc;
+
+    /**
+     * Hive agg && group by expression
+     */
+    private VectorGroupByDesc aggDesc;
+
+    /**
+     * Hive limit expression
+     */
+    private LimitDesc limitDesc;
+
+    /**
+     * Table data format
+     */
+    private String dataFormat = "";
+
+    /**
+     * If a table in an SQL statement is pushed down, this parameter is set to true.
+     */
+    private boolean existsTablePushDown = false;
+
+    private NdpVectorizedRowBatchCtx ndpCtx;
+
+    @Override
+    public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
+        this.hiveConf = pctx.getConf();
+        this.context = pctx.getContext();
+        this.parseContext = pctx.getParseContext();
+        Dispatcher dispatcher = new NdpDispatcher();
+        TaskGraphWalker walker = new TaskGraphWalker(dispatcher);
+        List<Node> topNodes = new ArrayList<>(pctx.getRootTasks());
+        walker.startWalking(topNodes, null);
+        return pctx;
+    }
+
+    private class NdpDispatcher implements Dispatcher {
+        @Override
+        public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs) throws SemanticException {
+            if (nodeOutputs == null || nodeOutputs.length == 0) {
+                throw new SemanticException("No Dispatch Context");
+            }
+            // OmniData Hive unsupported operator 'ROLLUP'
+            if (!NdpPlanChecker.checkRollUp(context.getCmd())) {
+                return null;
+            }
+
+            // host resources status map
+            Map<String, NdpStatusInfo> ndpStatusInfoMap = new HashMap<>(NdpStatusManager.getNdpZookeeperData(hiveConf));
+            if (!NdpPlanChecker.checkHostResources(ndpStatusInfoMap)) {
+                return null;
+            }
+
+            Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+
+            if (!currTask.isMapRedLocalTask() && !currTask.isMapRedTask()) {
+                return null;
+            }
+
+            // key: work name
+            Map<String, MapWork> mapWorkMap = new HashMap<>();
+            Map<String, TableScanOperator> tableScanOperatorMap = new HashMap<>();
+            // use TreeMap, sort by reduce name, for example: Reduce1, Reduce2...
+            Map<String, ReduceWork> reduceWorkMap = new TreeMap<>();
+            if (currTask.getWork() instanceof TezWork) {
+                NdpStatisticsUtils.generateMapReduceWork((TezWork) currTask.getWork(), mapWorkMap, tableScanOperatorMap,
+                        reduceWorkMap);
+            } else {
+                // unsupported
+                return null;
+            }
+            // deal with each TableScanOperator
+            tableScanOperatorMap.forEach((workName, tableScanOp) -> {
+                NdpPredicateInfo ndpPredicateInfo = new NdpPredicateInfo(false);
+                // set table's selectivity
+                tableScanOp.getConf().setOmniDataSelectivity(NdpStatisticsUtils.getSelectivity(tableScanOp));
+                // check TableScanOp
+                if (checkTableScanOp(tableScanOp, mapWorkMap.get(workName))) {
+                    Optional<RowExpression> filter = Optional.empty();
+                    Optional<AggregationInfo> aggregation = Optional.empty();
+                    OptionalLong limit = OptionalLong.empty();
+                    OmniDataPredicate omniDataPredicate = new OmniDataPredicate(tableScanOp);
+                    omniDataPredicate.setSelectExpressions(selectDesc);
+                    // get OmniData filter expression
+                    if (isPushDownFilter) {
+                        filter = getOmniDataFilter(omniDataPredicate, tableScanOp);
+                        if (!filter.isPresent()) {
+                            isPushDownFilter = false;
+                            isPushDownAgg = false;
+                        }
+                    }
+                    // get OmniData agg expression
+                    if (isPushDownAgg) {
+                        // The output column of the aggregation needs to be processed separately.
+                        aggregation = getOmniDataAggregation(omniDataPredicate);
+                        if (!aggregation.isPresent()) {
+                            isPushDownAgg = false;
+                        }
+                    }
+                    // get OmniData select expression
+                    if (isPushDownSelect && !isPushDownAgg) {
+                        omniDataPredicate = new OmniDataPredicate(tableScanOp);
+                        omniDataPredicate.addProjectionsByTableScan(tableScanOp);
+                    }
+                    // get OmniData limit expression
+                    if (isPushDownLimit) {
+                        limit = getOmniDataLimit();
+                    }
+                    // the decode type must exist
+                    if ((isPushDownFilter || isPushDownAgg) && omniDataPredicate.getDecodeTypes().size() > 0) {
+                        replaceTableScanOp(tableScanOp, mapWorkMap.get(workName));
+                        Predicate predicate = new Predicate(omniDataPredicate.getTypes(),
+                                omniDataPredicate.getColumns(), filter, omniDataPredicate.getProjections(),
+                                ImmutableMap.of(), ImmutableMap.of(), aggregation, limit);
+                        ndpPredicateInfo = new NdpPredicateInfo(true, isPushDownAgg, isPushDownFilter,
+                                omniDataPredicate.getHasPartitionColumns(), predicate,
+                                tableScanOp.getConf().getNeededColumnIDs(), omniDataPredicate.getDecodeTypes(),
+                                omniDataPredicate.getDecodeTypesWithAgg(), ndpCtx, dataFormat);
+                        // print OmniData Hive log information
+                        printPushDownInfo(tableScanOp.getConf().getAlias(), ndpStatusInfoMap);
+                        existsTablePushDown = true;
+                    }
+                }
+                // set ndpPredicateInfo after serialization
+                tableScanOp.getConf()
+                        .setNdpPredicateInfoStr(NdpSerializationUtils.serializeNdpPredicateInfo(ndpPredicateInfo));
+                initPushDown();
+            });
+            if (existsTablePushDown) {
+                // set OmniData hosts info to Hive conf
+                NdpStatusManager.setOmniDataHostToConf(hiveConf, ndpStatusInfoMap);
+                // OmniData reduce work optimize
+                if (OmniDataConf.getOmniDataReduceOptimizedEnabled(hiveConf)) {
+                    NdpStatisticsUtils.optimizeReduceWork(tableScanOperatorMap, reduceWorkMap, hiveConf, parseContext);
+                }
+            }
+            OmniDataConf.setOmniDataExistsTablePushDown(hiveConf, existsTablePushDown);
+            return null;
+        }
+
+        private boolean checkTableScanOp(TableScanOperator tableScanOp, BaseWork work) {
+            Optional<String> format = NdpPlanChecker.getDataFormat(tableScanOp, work);
+            if (format.isPresent()) {
+                dataFormat = format.get();
+            } else {
+                LOG.info("Table [{}] failed to push down, only orc and parquet are supported",
+                        tableScanOp.getConf().getAlias());
+                return false;
+            }
+            if (NdpPlanChecker.checkTableScanNumChild(tableScanOp) && NdpPlanChecker.checkHiveType(tableScanOp)
+                    && NdpPlanChecker.checkTableSize(tableScanOp, hiveConf) && NdpPlanChecker.checkSelectivity(tableScanOp,
+                    hiveConf)) {
+                // scan operator: select agg filter limit
+                scanTableScanChildOperators(tableScanOp.getChildOperators());
+            }
+            return isPushDownAgg || isPushDownFilter;
+        }
+
+        /**
+         * support : FilterOperator -> VectorSelectOperator -> VectorGroupByOperator
+         * support : FilterOperator -> VectorSelectOperator -> VectorLimitOperator
+         *
+         * @param operators operator
+         */
+        private void scanTableScanChildOperators(List<Operator<? extends OperatorDesc>> operators) {
+            if (operators == null || operators.size() != 1) {
+                return;
+            }
+            Operator<? extends OperatorDesc> operator = operators.get(0);
+            if (operator instanceof VectorFilterOperator) {
+                // filter push down
+                Optional<ExprNodeGenericFuncDesc> tmpFilterDesc = NdpPlanChecker.checkFilterOperator(
+                        (VectorFilterOperator) operator);
+                if (tmpFilterDesc.isPresent()) {
+                    filterDesc = tmpFilterDesc.get();
+                    isPushDownFilter = true;
+                }
+                scanTableScanChildOperators(operator.getChildOperators());
+            } else if (operator instanceof VectorSelectOperator) {
+                // check select
+                Optional<VectorSelectDesc> tmpSelectDesc = NdpPlanChecker.checkSelectOperator(
+                        (VectorSelectOperator) operator);
+                if (tmpSelectDesc.isPresent()) {
+                    selectDesc = tmpSelectDesc.get();
+                    isPushDownSelect = true;
+                }
+                scanTableScanChildOperators(operator.getChildOperators());
+            } else if (operator instanceof VectorGroupByOperator) {
+                // check agg
+                Optional<VectorGroupByDesc> tmpAggDesc = NdpPlanChecker.checkGroupByOperator(
+                        (VectorGroupByOperator) operator);
+                if (tmpAggDesc.isPresent()) {
+                    aggDesc = tmpAggDesc.get();
+                    isPushDownAgg = true;
+                }
+                scanTableScanChildOperators(operator.getChildOperators());
+            } else if (operator instanceof VectorLimitOperator) {
+                // check limit
+                Optional<LimitDesc> tmpLimitDesc = NdpPlanChecker.checkLimitOperator((VectorLimitOperator) operator);
+                if (tmpLimitDesc.isPresent()) {
+                    limitDesc = tmpLimitDesc.get();
+                    isPushDownLimit = true;
+                }
+                scanTableScanChildOperators(operator.getChildOperators());
+            }
+        }
+
+        /**
+         * Converts the filter expression of Hive to that of the OmniData Server.
+         *
+         * @param omniDataPredicate OmniData Predicate
+         * @return Filter expression of OmniData Filter
+         */
+        private Optional<RowExpression> getOmniDataFilter(OmniDataPredicate omniDataPredicate,
+                                                          TableScanOperator tableScanOperator) {
+            NdpFilter ndpFilter = new NdpFilter(filterDesc);
+            NdpFilter.NdpFilterMode mode = ndpFilter.getMode();
+            if (mode.equals(NdpFilter.NdpFilterMode.PART)) {
+                if (!NdpStatisticsUtils.evaluatePartFilterSelectivity(tableScanOperator,
+                        ndpFilter.getUnPushDownFuncDesc(), parseContext, hiveConf)) {
+                    return Optional.empty();
+                }
+                isPushDownPartFilter = true;
+                unsupportedFilterDesc = ndpFilter.getUnPushDownFuncDesc();
+                filterDesc = (ExprNodeGenericFuncDesc) ndpFilter.getPushDownFuncDesc();
+                // The AGG does not support part push down
+                isPushDownAgg = false;
+            } else if (mode.equals(NdpFilter.NdpFilterMode.NONE)) {
+                return Optional.empty();
+            }
+            OmniDataFilter omniDataFilter = new OmniDataFilter(omniDataPredicate);
+            // ExprNodeGenericFuncDesc need to clone
+            return Optional.ofNullable(
+                    omniDataFilter.getFilterExpression((ExprNodeGenericFuncDesc) filterDesc.clone(), ndpFilter));
+        }
+
+        /**
+         * Converts the aggregation expression of Hive to that of the OmniData Server.
+         *
+         * @param omniDataPredicate OmniData Predicate
+         * @return Aggregation expression of OmniData Server
+         */
+        private Optional<AggregationInfo> getOmniDataAggregation(OmniDataPredicate omniDataPredicate) {
+            OmniDataAggregation omniDataAggregation = new OmniDataAggregation(omniDataPredicate);
+            return Optional.ofNullable(omniDataAggregation.getAggregation(aggDesc));
+        }
+
+        private OptionalLong getOmniDataLimit() {
+            return OmniDataLimit.getOmniDataLimit(limitDesc.getLimit());
+        }
+
+        private void replaceTableScanOp(TableScanOperator tableScanOp, BaseWork work){
+            if (isPushDownFilter) {
+                if (isPushDownPartFilter) {
+                    replaceRawVectorizedRowBatchCtx(tableScanOp, work);
+                } else {
+                    removeTableScanRawFilter(tableScanOp.getChildOperators());
+                }
+                ndpCtx = new NdpVectorizedRowBatchCtx(work.getVectorizedRowBatchCtx());
+                // set rowColumnTypeInfos to TableScanDesc
+                tableScanOp.getConf().setRowColumnTypeInfos(work.getVectorizedRowBatchCtx().getRowColumnTypeInfos());
+            }
+            if (isPushDownAgg) {
+                removeTableScanRawAggregation(tableScanOp.getChildOperators(), work);
+                removeTableScanRawSelect(tableScanOp.getChildOperators());
+            }
+            // set whether to push down
+            tableScanOp.getConf().setPushDownFilter(isPushDownFilter);
+            tableScanOp.getConf().setPushDownAgg(isPushDownAgg);
+        }
+
+        /**
+         * The outputColumnId is added to VectorizedRowBatchCtx because a new filter expression is replaced.
+         *
+         * @param tableScanOp TableScanOperator
+         * @param work BaseWork
+         */
+        private void replaceRawVectorizedRowBatchCtx(TableScanOperator tableScanOp, BaseWork work) {
+            VectorizedRowBatchCtx oldCtx = work.getVectorizedRowBatchCtx();
+            VectorizedRowBatchCtx newCtx = new VectorizedRowBatchCtx(oldCtx.getRowColumnNames(),
+                    oldCtx.getRowColumnTypeInfos(), oldCtx.getRowdataTypePhysicalVariations(), oldCtx.getDataColumnNums(),
+                    oldCtx.getPartitionColumnCount(), oldCtx.getVirtualColumnCount(), oldCtx.getNeededVirtualColumns(),
+                    tableScanOp.getOutputVectorizationContext().getScratchColumnTypeNames(),
+                    tableScanOp.getOutputVectorizationContext().getScratchDataTypePhysicalVariations());
+            work.setVectorizedRowBatchCtx(newCtx);
+        }
+
+        private void removeTableScanRawFilter(List<Operator<? extends OperatorDesc>> operators) {
+            try {
+                for (Operator<? extends OperatorDesc> child : operators) {
+                    if (child instanceof VectorFilterOperator) {
+                        // remove raw VectorFilterOperator
+                        child.getParentOperators().get(0).removeChildAndAdoptItsChildren(child);
+                        return;
+                    }
+                    removeTableScanRawFilter(child.getChildOperators());
+                }
+            } catch (SemanticException e) {
+                LOG.error("Exception when trying to remove partition predicates: fail to find child from parent", e);
+            }
+        }
+
+        private void removeTableScanRawAggregation(List<Operator<? extends OperatorDesc>> operators, BaseWork work) {
+            try {
+                for (Operator<? extends OperatorDesc> child : operators) {
+                    if (child instanceof VectorGroupByOperator) {
+                        // remove raw VectorGroupByOperator
+                        child.getParentOperators().get(0).removeChildAndAdoptItsChildren(child);
+                        return;
+                    }
+                    removeTableScanRawAggregation(child.getChildOperators(), work);
+                }
+            } catch (SemanticException e) {
+                LOG.error("Exception when trying to remove partition predicates: fail to find child from parent", e);
+            }
+        }
+
+        private void removeTableScanRawSelect(List<Operator<? extends OperatorDesc>> operators) {
+            for (Operator<? extends OperatorDesc> child : operators) {
+                if (child instanceof VectorSelectOperator) {
+                    // remove raw VectorExpression
+                    ((VectorSelectOperator) child).setvExpressions(new VectorExpression[] {});
+                    return;
+                }
+                removeTableScanRawSelect(child.getChildOperators());
+            }
+        }
+
+        private void printPushDownInfo(String tableName, Map<String, NdpStatusInfo> ndpStatusInfoMap) {
+            String pushDownInfo = String.format(
+                    "Table [%s] Push Down Info [ Select:[%s], Filter:[%s], Aggregation:[%s], Group By:[%s], Limit:[%s], Raw Filter:[%s], Host Map:[%s]",
+                    tableName, ((selectDesc != null) && isPushDownAgg) ? Arrays.stream(selectDesc.getSelectExpressions())
+                            .map(VectorExpression::toString)
+                            .collect(Collectors.joining(", ")) : "", isPushDownFilter ? filterDesc.toString() : "",
+                    isPushDownAgg ? Arrays.stream(aggDesc.getVecAggrDescs())
+                            .map(VectorAggregationDesc::toString)
+                            .collect(Collectors.joining(", ")) : "",
+                    (isPushDownAgg && aggDesc.getKeyExpressions() != null) ? Arrays.stream(aggDesc.getKeyExpressions())
+                            .map(VectorExpression::toString)
+                            .collect(Collectors.joining(", ")) : "", isPushDownLimit ? limitDesc.getLimit() : "",
+                    (unsupportedFilterDesc != null) ? unsupportedFilterDesc.toString() : "",
+                    ((ndpStatusInfoMap.size() > 0) ? ndpStatusInfoMap.entrySet()
+                            .stream()
+                            .map(s -> s.getValue().getDatanodeHost() + " -> " + s.getKey())
+                            .limit(100)
+                            .collect(Collectors.joining(", ")) : ""));
+            LOG.info(pushDownInfo);
+            System.out.println(pushDownInfo);
+        }
+
+        private void initPushDown() {
+            isPushDownFilter = false;
+            isPushDownPartFilter = false;
+            isPushDownSelect = false;
+            isPushDownAgg = false;
+            isPushDownLimit = false;
+            filterDesc = null;
+            unsupportedFilterDesc = null;
+            selectDesc = null;
+            aggDesc = null;
+            limitDesc = null;
+            ndpCtx = null;
+            dataFormat = "";
+        }
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpStatisticsUtils.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpStatisticsUtils.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpStatisticsUtils.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpStatisticsUtils.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,493 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.physical;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.ColumnInfo;
+import org.apache.hadoop.hive.ql.exec.FilterOperator;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
+import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.GraphWalker;
+import org.apache.hadoop.hive.ql.lib.LevelOrderWalker;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.lib.NodeProcessor;
+import org.apache.hadoop.hive.ql.lib.Rule;
+import org.apache.hadoop.hive.ql.lib.RuleRegExp;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx;
+import org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory;
+import org.apache.hadoop.hive.ql.parse.ColumnStatsList;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.BaseWork;
+import org.apache.hadoop.hive.ql.plan.ColStatistics;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.Statistics;
+import org.apache.hadoop.hive.ql.plan.TezWork;
+import org.apache.hadoop.hive.ql.stats.StatsUtils;
+import org.apache.tez.mapreduce.grouper.TezSplitGrouper;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.math.BigDecimal;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+
+/**
+ * OmniData Hive statistics tool
+ * Used to collect statistics for optimization
+ *
+ * @since 2022-07-12
+ */
+public class NdpStatisticsUtils {
+    private static final Logger LOG = LoggerFactory.getLogger(NdpStatisticsUtils.class);
+
+    /**
+     * basic reduce quantity and protection measures
+     */
+    public static final int BASED_REDUCES = 50;
+
+    /**
+     * basic selectivity quantity and protection measures
+     */
+    public static final double BASED_SELECTIVITY = 0.025d;
+
+    /**
+     * Ratio of the length of the needed column to the total length, the value cannot be less than the basic value
+     */
+    public static final double BASED_COL_LENGTH_PROPORTION = 0.025d;
+
+    /**
+     * Optimization can be performed only when the selectivity is lower than this threshold
+     */
+    public static final double SELECTIVITY_THRESHOLD = 0.5d;
+
+    /**
+     * Number of decimal places reserved
+     */
+    public static final int OMNIDATA_SCALE = 4;
+
+    /**
+     * Calculate the estimated selectivity based on the total number of Table and the total number of Filter
+     *
+     * @param tableScanOp TableScanOperator
+     * @return selectivity
+     */
+    public static double getSelectivity(TableScanOperator tableScanOp) {
+        double selectivity = 1.0d;
+        if (tableScanOp.getConf().getStatistics() == null) {
+            return selectivity;
+        }
+        long tableCount = tableScanOp.getConf().getStatistics().getNumRows();
+        long filterCount = tableScanOp.getChildOperators().get(0).getConf().getStatistics().getNumRows();
+        if (tableCount > 0) {
+            BigDecimal tableCountBig = new BigDecimal(tableCount);
+            BigDecimal filterCountBig = new BigDecimal(filterCount);
+            selectivity = filterCountBig.divide(tableCountBig, OMNIDATA_SCALE, BigDecimal.ROUND_HALF_UP).doubleValue();
+        }
+        return selectivity;
+    }
+
+    /**
+     * Through TezWork, get MapWork, ReduceWork and TableScanOperator
+     *
+     * @param tezWork TezWork
+     * @param mapWorkMap MapWork's map
+     * @param tableScanOperatorMap TableScanOperator's map
+     * @param reduceWorkMap ReduceWork's map
+     */
+    public static void generateMapReduceWork(TezWork tezWork, Map<String, MapWork> mapWorkMap,
+                                             Map<String, TableScanOperator> tableScanOperatorMap, Map<String, ReduceWork> reduceWorkMap) {
+        for (BaseWork work : tezWork.getAllWork()) {
+            if (work instanceof MapWork) {
+                MapWork mapWork = (MapWork) work;
+                if (mapWork.getAliasToWork().size() != 1) {
+                    continue;
+                }
+                // support only one operator
+                generateMapWork(mapWork, mapWorkMap, tableScanOperatorMap);
+            }
+            if (work instanceof ReduceWork) {
+                reduceWorkMap.put(work.getName(), (ReduceWork) work);
+            }
+        }
+    }
+
+    /**
+     * Analyze the table in advance to collect statistics.
+     * Estimate the optimized size based on the selectivity and needed column proportion.
+     *
+     * @param tableScanOp tableScanOp
+     * @param parseContext The current parse context.
+     * @return optimized size
+     */
+    public static long estimateOptimizedSize(TableScanOperator tableScanOp, ParseContext parseContext) {
+        long dataSize = tableScanOp.getConf().getStatistics().getDataSize();
+        long avgRowSize = tableScanOp.getConf().getStatistics().getAvgRowSize();
+        double omniDataSelectivity = Math.max(tableScanOp.getConf().getOmniDataSelectivity(), BASED_SELECTIVITY);
+        // get the average length of the needed column
+        double totalAvgColLen = getTotalAvgColLen(tableScanOp, parseContext);
+        if (totalAvgColLen <= 0 || avgRowSize <= 0) {
+            return (long) ((double) dataSize * omniDataSelectivity);
+        } else {
+            double colLenProportion = Math.max(totalAvgColLen / (double) avgRowSize, BASED_COL_LENGTH_PROPORTION);
+            return (long) ((double) dataSize * omniDataSelectivity * colLenProportion);
+        }
+    }
+
+    /**
+     * get the average length of the needed column
+     *
+     * @param tableScanOp tableScanOp
+     * @param parseContext parseContext
+     * @return table's average length
+     */
+    public static double getTotalAvgColLen(TableScanOperator tableScanOp, ParseContext parseContext) {
+        Table table = tableScanOp.getConf().getTableMetadata();
+        List<ColStatistics> colStats;
+        if (table.isPartitioned()) {
+            Optional<Statistics> statistics = collectStatistics(parseContext, tableScanOp);
+            if (statistics.isPresent()) {
+                colStats = statistics.get().getColumnStats();
+            } else {
+                colStats = new ArrayList<>();
+            }
+        } else {
+            // Get table level column statistics from metastore for needed columns
+            colStats = StatsUtils.getTableColumnStats(tableScanOp.getConf().getTableMetadata(),
+                    tableScanOp.getSchema().getSignature(), tableScanOp.getConf().getNeededColumns(), null);
+        }
+        double totalAvgColLen = 0d;
+        // add the avgColLen for each column, colStats's size may be null
+        for (ColStatistics colStat : colStats) {
+            totalAvgColLen += colStat.getAvgColLen();
+        }
+        return totalAvgColLen;
+    }
+
+    /**
+     * Statistics: Describes the output of an operator in terms of size, rows, etc  based on estimates.
+     * use ParseContext to collect Table's statistics
+     *
+     * @param pctx The current parse context.
+     * @param tableScanOp TableScanOperator
+     * @return Statistics
+     */
+    public static Optional<Statistics> collectStatistics(ParseContext pctx, TableScanOperator tableScanOp) {
+        try {
+            AnnotateStatsProcCtx aspCtx = new AnnotateStatsProcCtx(pctx);
+            PrunedPartitionList partList = aspCtx.getParseContext().getPrunedPartitions(tableScanOp);
+            ColumnStatsList colStatsCached = aspCtx.getParseContext().getColStatsCached(partList);
+            Table table = tableScanOp.getConf().getTableMetadata();
+            // column level statistics are required only for the columns that are needed
+            List<ColumnInfo> schema = tableScanOp.getSchema().getSignature();
+            List<String> neededColumns = tableScanOp.getNeededColumns();
+            List<String> referencedColumns = tableScanOp.getReferencedColumns();
+            // gather statistics for the first time and the attach it to table scan operator
+            return Optional.of(
+                    StatsUtils.collectStatistics(aspCtx.getConf(), partList, table, schema, neededColumns, colStatsCached,
+                            referencedColumns, true));
+        } catch (HiveException e) {
+            LOG.error("OmniData Hive failed to retrieve stats ", e);
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * estimate reducer number
+     *
+     * @param totalInputFileSize total input size
+     * @param bytesPerReducer Size of bytes available for each reduce
+     * @param oldReducers old reduce number
+     * @return new reduce number
+     */
+    public static int estimateReducersByFileSize(long totalInputFileSize, long bytesPerReducer, int oldReducers) {
+        if (totalInputFileSize <= 0) {
+            return 0;
+        }
+        double bytes = Math.max(totalInputFileSize, bytesPerReducer);
+        int newReducers = (int) Math.ceil(bytes / (double) bytesPerReducer);
+        // The new reduce number cannot be less than the basic reduce number
+        newReducers = Math.max(BASED_REDUCES, newReducers);
+        // The new reduce number cannot be greater than the old reduce number
+        newReducers = Math.min(oldReducers, newReducers);
+        return newReducers;
+    }
+
+    /**
+     * Update TezEdgeProperty and set reduce tasks in ReduceWork
+     *
+     * @param reduceWork ReduceWork
+     * @param optimizedNumReduces optimized reduce numbers
+     * @param bytesPerReducer size per reducer
+     */
+    public static void setOptimizedNumReduceTasks(ReduceWork reduceWork, int optimizedNumReduces,
+                                                  long bytesPerReducer) {
+        // limit for reducers
+        final int maxReducers = reduceWork.getNumReduceTasks();
+        if (!reduceWork.isAutoReduceParallelism()) {
+            // set optimized reduce number
+            reduceWork.setNumReduceTasks(Math.min(optimizedNumReduces, maxReducers));
+            return;
+        }
+        // tez auto reduce parallelism should be set to the 'minPartition' and 'maxPartition'
+        float minPartitionFactor = 0.5f;
+        float maxPartitionFactor = 1.0f;
+
+        // min we allow tez to pick
+        int minPartition = Math.max(1, (int) ((float) optimizedNumReduces * minPartitionFactor));
+        minPartition = Math.min(minPartition, maxReducers);
+
+        // max we allow tez to pick
+        int maxPartition = Math.max(1, (int) ((float) optimizedNumReduces * maxPartitionFactor));
+        maxPartition = Math.min(maxPartition, maxReducers);
+
+        // set optimized reduce number
+        reduceWork.setNumReduceTasks(optimizedNumReduces);
+        reduceWork.setMinReduceTasks(minPartition);
+        reduceWork.setMaxReduceTasks(maxPartition);
+
+        // update TezEdgeProperty
+        reduceWork.getEdgePropRef()
+                .setAutoReduce(reduceWork.getEdgePropRef().getHiveConf(), true, minPartition, maxPartition,
+                        bytesPerReducer);
+    }
+
+    /**
+     * The ReduceWork entry is optimized
+     *
+     * @param tableScanOperatorMap TableScanOperator's map
+     * @param reduceWorkMap ReduceWork's map
+     * @param hiveConf hive conf
+     * @param pctx The current parse context.
+     */
+    public static void optimizeReduceWork(Map<String, TableScanOperator> tableScanOperatorMap,
+                                          Map<String, ReduceWork> reduceWorkMap, HiveConf hiveConf, ParseContext pctx) {
+        // use 'tez.grouping.max-size' to optimize bytesPerReducer
+        long bytesPerReducer = Math.max(hiveConf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER),
+                TezSplitGrouper.TEZ_GROUPING_SPLIT_MAX_SIZE_DEFAULT);
+        for (ReduceWork reduceWork : reduceWorkMap.values()) {
+            if (!reduceWork.isAutoReduceParallelism() || reduceWork.getTagToInput().values().size() <= 0) {
+                continue;
+            }
+            // supported 'isAutoReduceParallelism' is true
+            int optimizedNumReduces = estimateNumReducesByInput(reduceWork, tableScanOperatorMap, reduceWorkMap, pctx,
+                    bytesPerReducer);
+            if (optimizedNumReduces > 0) {
+                NdpStatisticsUtils.setOptimizedNumReduceTasks(reduceWork, optimizedNumReduces, bytesPerReducer);
+            }
+        }
+    }
+
+    /**
+     * The LengthPerGroup is optimized based on the selectivity of Agg or Filter
+     * to increase the number of splits processed by a Tez Task.
+     *
+     * @param conf hive conf
+     * @return optimized LengthPerGroup
+     */
+    public static double optimizeLengthPerGroup(Configuration conf) {
+        double selectivity = OmniDataConf.getOmniDataTableOptimizedSelectivity(conf);
+        double coefficient = 1d;
+        double configCoefficient = OmniDataConf.getOmniDataGroupOptimizedCoefficient(conf);
+        if (OmniDataConf.getOmniDataAggOptimizedEnabled(conf)) {
+            if (configCoefficient > 0) {
+                LOG.info("Desired OmniData optimized coefficient overridden by config to: {}", configCoefficient);
+                return configCoefficient;
+            }
+            // OmniData agg optimized
+            if (selectivity <= SELECTIVITY_THRESHOLD) {
+                int maxAggCoefficient = 10;
+                // 1 / selectivity + 1
+                coefficient = Math.min((new BigDecimal("1").divide(new BigDecimal(selectivity), OMNIDATA_SCALE,
+                        BigDecimal.ROUND_HALF_UP)).add(new BigDecimal("1")).doubleValue(), maxAggCoefficient);
+            }
+        } else if (OmniDataConf.getOmniDataFilterOptimizedEnabled(conf)) {
+            if (configCoefficient > 0) {
+                LOG.info("Desired OmniData optimized coefficient overridden by config to: {}", configCoefficient);
+                return configCoefficient;
+            }
+            // OmniData filter optimized
+            if (selectivity <= SELECTIVITY_THRESHOLD) {
+                int maxFilterCoefficient = 10;
+                // 0.5 / selectivity + 0.5
+                coefficient = Math.min((new BigDecimal("0.5").divide(new BigDecimal(selectivity), OMNIDATA_SCALE,
+                        BigDecimal.ROUND_HALF_UP)).add(new BigDecimal("0.5")).doubleValue(), maxFilterCoefficient);
+            }
+        } else {
+            if (OmniDataConf.getOmniDataGroupOptimizedEnabled(conf)) {
+                double maxCoefficient = 2.5d;
+                // 0.2 / selectivity + 1
+                coefficient = Math.min((new BigDecimal("0.2").divide(new BigDecimal(selectivity), OMNIDATA_SCALE,
+                        BigDecimal.ROUND_HALF_UP)).add(new BigDecimal("1")).doubleValue(), maxCoefficient);
+            }
+        }
+        return coefficient;
+    }
+
+    /**
+     * The Filter in TableScanOperator is changed. The Stats of the Filter needs to be updated.
+     *
+     * @param pctx The current parse context.
+     * @param tableScanOp TableScanOperator
+     * @throws SemanticException Exception from SemanticAnalyzer.
+     */
+    public static void updateFilterStats(ParseContext pctx, TableScanOperator tableScanOp) throws SemanticException {
+        AnnotateStatsProcCtx aspCtx = new AnnotateStatsProcCtx(pctx);
+
+        // create a walker which walks the tree in a BFS manner while maintaining the
+        // operator stack. The dispatcher generates the plan from the operator tree
+        Map<Rule, NodeProcessor> opRules = new LinkedHashMap<>();
+        opRules.put(new RuleRegExp("FIL", FilterOperator.getOperatorName() + "%"),
+                StatsRulesProcFactory.getFilterRule());
+
+        // The dispatcher fires the processor corresponding to the closest matching
+        // rule and passes the context along
+        Dispatcher dispatcher = new DefaultRuleDispatcher(StatsRulesProcFactory.getDefaultRule(), opRules, aspCtx);
+        GraphWalker ogw = new LevelOrderWalker(dispatcher, 0);
+
+        // Create a list of topOp nodes
+        ArrayList<Node> topNodes = new ArrayList<>();
+        topNodes.add(tableScanOp);
+        ogw.startWalking(topNodes, null);
+    }
+
+    /**
+     * Evaluate whether the part pushdown selectivity meets the requirements. If yes, update the filter and selectivity.
+     *
+     * @param tableScanOp TableScanOperator
+     * @param newExprDesc new Filter ExprNodeDesc
+     * @param parseContext The current parse context.
+     * @param conf hive conf
+     * @return true or false
+     */
+    public static boolean evaluatePartFilterSelectivity(TableScanOperator tableScanOp, ExprNodeDesc newExprDesc,
+                                                        ParseContext parseContext, Configuration conf) {
+        Operator<? extends OperatorDesc> operator = tableScanOp.getChildOperators().get(0);
+        if (!(operator instanceof VectorFilterOperator)) {
+            return false;
+        }
+        VectorFilterOperator vectorFilterOp = (VectorFilterOperator) operator;
+        VectorExpression oldExpr = vectorFilterOp.getPredicateExpression();
+        ExprNodeDesc oldExprNodeDesc = vectorFilterOp.getConf().getPredicate();
+        try {
+            VectorExpression ndpExpr = tableScanOp.getOutputVectorizationContext()
+                    .getVectorExpression(newExprDesc, VectorExpressionDescriptor.Mode.FILTER);
+            // set new filter expression
+            vectorFilterOp.setFilterCondition(ndpExpr);
+            // set new filter desc
+            vectorFilterOp.getConf().setPredicate(newExprDesc);
+            // update filter's statistics
+            NdpStatisticsUtils.updateFilterStats(parseContext, tableScanOp);
+        } catch (HiveException e) {
+            LOG.error("OmniData Hive update filter stats failed", e);
+            return false;
+        }
+        // Calculate the new selection rate
+        double newSelectivity = tableScanOp.getConf().getOmniDataSelectivity() / NdpStatisticsUtils.getSelectivity(
+                tableScanOp);
+        if (newSelectivity <= OmniDataConf.getOmniDataFilterSelectivity(conf)) {
+            // set table's selectivity
+            tableScanOp.getConf().setOmniDataSelectivity(newSelectivity);
+            LOG.info("Table [{}] part selectivity is {}", tableScanOp.getConf().getAlias(), newSelectivity);
+            return true;
+        }
+        vectorFilterOp.setFilterCondition(oldExpr);
+        vectorFilterOp.getConf().setPredicate(oldExprNodeDesc);
+        try {
+            NdpStatisticsUtils.updateFilterStats(parseContext, tableScanOp);
+        } catch (SemanticException e) {
+            LOG.error("OmniData Hive update filter stats failed", e);
+            return false;
+        }
+        LOG.info("Table [{}] failed to part push down, since selectivity[{}] > threshold[{}]",
+                tableScanOp.getConf().getAlias(), newSelectivity, OmniDataConf.getOmniDataFilterSelectivity(conf));
+        return false;
+    }
+
+    private static void generateMapWork(MapWork mapWork, Map<String, MapWork> mapWorkMap,
+                                        Map<String, TableScanOperator> tableScanOperatorMap) {
+        mapWork.getAliasToWork().values().forEach(operator -> {
+            if (operator instanceof TableScanOperator) {
+                tableScanOperatorMap.put(mapWork.getName(), (TableScanOperator) operator);
+                mapWorkMap.put(mapWork.getName(), mapWork);
+            }
+        });
+    }
+
+    private static int estimateNumReducesByInput(ReduceWork reduceWork,
+                                                 Map<String, TableScanOperator> tableScanOperatorMap, Map<String, ReduceWork> reduceWorkMap, ParseContext pctx,
+                                                 long bytesPerReducer) {
+        boolean isSupported = true;
+        long totalOptimizedSize = 0L;
+        int reduces = 0;
+        int totalReduces = 0;
+        // we need to add up all the estimates from reduceWork's input
+        for (String inputWorkName : reduceWork.getTagToInput().values()) {
+            if (tableScanOperatorMap.containsKey(inputWorkName)) {
+                TableScanOperator tableScanOp = tableScanOperatorMap.get(inputWorkName);
+                if (tableScanOp.getConf().getStatistics() != null) {
+                    // add the optimized input sizes
+                    totalOptimizedSize += NdpStatisticsUtils.estimateOptimizedSize(tableScanOp, pctx);
+                } else {
+                    isSupported = false;
+                    break;
+                }
+            } else if (reduceWorkMap.containsKey(inputWorkName)) {
+                reduces++;
+                // add the child's reduce number
+                totalReduces += reduceWorkMap.get(inputWorkName).getNumReduceTasks();
+            } else {
+                // unsupported MergeJoinWork
+                isSupported = false;
+                break;
+            }
+        }
+        int optimizedNumReduces = -1;
+        if (isSupported) {
+            optimizedNumReduces = NdpStatisticsUtils.estimateReducersByFileSize(totalOptimizedSize, bytesPerReducer,
+                    reduceWork.getNumReduceTasks());
+            // reduce work exists in the input
+            if (reduces > 0) {
+                int avgReduces = totalReduces / reduces;
+                // When the number of reduce works of a map work is less than the BASED_REDUCES,
+                // the number of reduce work can be ignored.
+                optimizedNumReduces = (optimizedNumReduces > NdpStatisticsUtils.BASED_REDUCES) ? optimizedNumReduces
+                        + avgReduces : avgReduces;
+            }
+        }
+        return optimizedNumReduces;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpVectorizedRowBatchCtx.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpVectorizedRowBatchCtx.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpVectorizedRowBatchCtx.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/physical/NdpVectorizedRowBatchCtx.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,453 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.physical;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.google.common.base.Preconditions;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
+import org.apache.hadoop.hive.common.type.Date;
+import org.apache.hadoop.hive.common.type.HiveChar;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
+import org.apache.hadoop.hive.common.type.Timestamp;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
+import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
+import org.apache.hadoop.hive.ql.io.IOPrepareCache;
+import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
+import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.PartitionDesc;
+import org.apache.hadoop.hive.serde2.io.DateWritableV2;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.mapred.FileSplit;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+/**
+ * Context for Ndp Vectorized row batch.
+ *
+ * @since 2022-05-28
+ */
+public class NdpVectorizedRowBatchCtx implements Serializable {
+
+    private static final long serialVersionUID = 1L;
+
+    private String[] rowColumnNames;
+
+    private DataTypePhysicalVariation[] rowDataTypePhysicalVariations;
+
+    private int[] dataColumnNums;
+
+    private int dataColumnCount;
+
+    private int partitionColumnCount;
+
+    private int virtualColumnCount;
+
+    private VirtualColumn[] neededVirtualColumns;
+
+    private String[] scratchColumnTypeNames;
+
+    private DataTypePhysicalVariation[] scratchDataTypePhysicalVariations;
+
+    /**
+     * Constructor for VectorizedRowBatchCtx
+     */
+    public NdpVectorizedRowBatchCtx() {
+    }
+
+    public NdpVectorizedRowBatchCtx(VectorizedRowBatchCtx vectorizedRowBatchCtx) {
+        this(vectorizedRowBatchCtx.getRowColumnNames(), vectorizedRowBatchCtx.getRowdataTypePhysicalVariations(),
+                vectorizedRowBatchCtx.getDataColumnNums(), vectorizedRowBatchCtx.getDataColumnCount(),
+                vectorizedRowBatchCtx.getPartitionColumnCount(), vectorizedRowBatchCtx.getVirtualColumnCount(),
+                vectorizedRowBatchCtx.getNeededVirtualColumns(), vectorizedRowBatchCtx.getScratchColumnTypeNames(),
+                vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations());
+    }
+
+    @JsonCreator
+    public NdpVectorizedRowBatchCtx(@JsonProperty("rowColumnNames") String[] rowColumnNames,
+                                    @JsonProperty("rowDataTypePhysicalVariations") DataTypePhysicalVariation[] rowDataTypePhysicalVariations,
+                                    @JsonProperty("dataColumnNums") int[] dataColumnNums, @JsonProperty("dataColumnCount") int dataColumnCount,
+                                    @JsonProperty("partitionColumnCount") int partitionColumnCount,
+                                    @JsonProperty("virtualColumnCount") int virtualColumnCount,
+                                    @JsonProperty("neededVirtualColumns") VirtualColumn[] neededVirtualColumns,
+                                    @JsonProperty("scratchColumnTypeNames") String[] scratchColumnTypeNames,
+                                    @JsonProperty("scratchDataTypePhysicalVariations")
+                                            DataTypePhysicalVariation[] scratchDataTypePhysicalVariations) {
+        this.rowColumnNames = rowColumnNames;
+        this.rowDataTypePhysicalVariations = rowDataTypePhysicalVariations;
+        this.dataColumnNums = dataColumnNums;
+        this.dataColumnCount = dataColumnCount;
+        this.partitionColumnCount = partitionColumnCount;
+        this.virtualColumnCount = virtualColumnCount;
+        this.neededVirtualColumns = neededVirtualColumns;
+        this.scratchColumnTypeNames = scratchColumnTypeNames;
+        this.scratchDataTypePhysicalVariations = scratchDataTypePhysicalVariations;
+    }
+
+    private ColumnVector createColumnVectorFromRowColumnTypeInfos(int columnNum, TypeInfo[] rowColumnTypeInfos) {
+        TypeInfo typeInfo = rowColumnTypeInfos[columnNum];
+        DataTypePhysicalVariation dataTypePhysicalVariation = DataTypePhysicalVariation.NONE;
+        if (rowDataTypePhysicalVariations != null) {
+            dataTypePhysicalVariation = rowDataTypePhysicalVariations[columnNum];
+        }
+        return VectorizedBatchUtil.createColumnVector(typeInfo, dataTypePhysicalVariation);
+    }
+
+    public boolean isVirtualColumnNeeded(String virtualColumnName) {
+        for (VirtualColumn neededVirtualColumn : neededVirtualColumns) {
+            if (neededVirtualColumn.getName().equals(virtualColumnName)) {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    /**
+     * Creates a Vectorized row batch and the column vectors.
+     *
+     * @return VectorizedRowBatch
+     */
+    public VectorizedRowBatch createVectorizedRowBatch(TypeInfo[] rowColumnTypeInfos) {
+        final int nonScratchColumnCount = rowColumnTypeInfos.length;
+        final int totalColumnCount = nonScratchColumnCount + scratchColumnTypeNames.length;
+        VectorizedRowBatch result = new VectorizedRowBatch(totalColumnCount);
+
+        if (dataColumnNums == null) {
+            // All data and partition columns.
+            for (int i = 0; i < nonScratchColumnCount; i++) {
+                result.cols[i] = createColumnVectorFromRowColumnTypeInfos(i, rowColumnTypeInfos);
+            }
+        } else {
+            // Create only needed/included columns data columns.
+            for (int i = 0; i < dataColumnNums.length; i++) {
+                int columnNum = dataColumnNums[i];
+                Preconditions.checkState(columnNum < nonScratchColumnCount);
+                result.cols[columnNum] = createColumnVectorFromRowColumnTypeInfos(columnNum, rowColumnTypeInfos);
+            }
+            // Always create partition and virtual columns.
+            final int partitionEndColumnNum = dataColumnCount + partitionColumnCount;
+            for (int partitionColumnNum = dataColumnCount;
+                 partitionColumnNum < partitionEndColumnNum; partitionColumnNum++) {
+                result.cols[partitionColumnNum] = VectorizedBatchUtil.createColumnVector(
+                        rowColumnTypeInfos[partitionColumnNum]);
+            }
+            final int virtualEndColumnNum = partitionEndColumnNum + virtualColumnCount;
+            for (int virtualColumnNum = partitionEndColumnNum;
+                 virtualColumnNum < virtualEndColumnNum; virtualColumnNum++) {
+                String virtualColumnName = rowColumnNames[virtualColumnNum];
+                if (!isVirtualColumnNeeded(virtualColumnName)) {
+                    continue;
+                }
+                result.cols[virtualColumnNum] = VectorizedBatchUtil.createColumnVector(
+                        rowColumnTypeInfos[virtualColumnNum]);
+            }
+        }
+
+        for (int i = 0; i < scratchColumnTypeNames.length; i++) {
+            String typeName = scratchColumnTypeNames[i];
+            DataTypePhysicalVariation dataTypePhysicalVariation = scratchDataTypePhysicalVariations[i];
+            result.cols[nonScratchColumnCount + i] = VectorizedBatchUtil.createColumnVector(typeName,
+                    dataTypePhysicalVariation);
+        }
+
+        // UNDONE: Also remember virtualColumnCount...
+        result.setPartitionInfo(dataColumnCount, partitionColumnCount);
+
+        result.reset();
+        return result;
+    }
+
+    public static void getPartitionValues(NdpVectorizedRowBatchCtx vrbCtx, Configuration hiveConf, FileSplit split,
+                                          Object[] partitionValues, TypeInfo[] rowColumnTypeInfos) {
+        MapWork mapWork = Utilities.getMapWork(hiveConf);
+        getPartitionValues(vrbCtx, mapWork, split, partitionValues, rowColumnTypeInfos);
+    }
+
+    public static void getPartitionValues(NdpVectorizedRowBatchCtx vrbCtx, MapWork mapWork, FileSplit split,
+                                          Object[] partitionValues, TypeInfo[] rowColumnTypeInfos) {
+        Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();
+        try {
+            PartitionDesc partDesc = HiveFileFormatUtils.getFromPathRecursively(pathToPartitionInfo, split.getPath(),
+                    IOPrepareCache.get().getPartitionDescMap());
+            getPartitionValues(vrbCtx, partDesc, partitionValues, rowColumnTypeInfos);
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    public static void getPartitionValues(NdpVectorizedRowBatchCtx vrbCtx, PartitionDesc partDesc,
+                                          Object[] partitionValues, TypeInfo[] rowColumnTypeInfos) {
+
+        LinkedHashMap<String, String> partSpec = partDesc.getPartSpec();
+
+        for (int i = 0; i < vrbCtx.partitionColumnCount; i++) {
+            Object objectValue;
+            if (partSpec == null) {
+                // For partition-less table, initialize partValue to empty string.
+                // We can have partition-less table even if we have partition keys
+                // when there is only only partition selected and the partition key is not
+                // part of the projection/include list.
+                objectValue = null;
+            } else {
+                String key = vrbCtx.rowColumnNames[vrbCtx.dataColumnCount + i];
+
+                // Create a Standard java object Inspector
+                TypeInfo partColTypeInfo = rowColumnTypeInfos[vrbCtx.dataColumnCount + i];
+                ObjectInspector objectInspector = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(
+                        partColTypeInfo);
+                objectValue = ObjectInspectorConverters.
+                        getConverter(PrimitiveObjectInspectorFactory.
+                                javaStringObjectInspector, objectInspector).
+                        convert(partSpec.get(key));
+                if (partColTypeInfo instanceof CharTypeInfo) {
+                    objectValue = ((HiveChar) objectValue).getStrippedValue();
+                }
+            }
+            partitionValues[i] = objectValue;
+        }
+    }
+
+    public void addPartitionColsToBatch(VectorizedRowBatch batch, Object[] partitionValues,
+                                        TypeInfo[] rowColumnTypeInfos) {
+        addPartitionColsToBatch(batch.cols, partitionValues, rowColumnTypeInfos);
+    }
+
+    public void addPartitionColsToBatch(ColumnVector[] cols, Object[] partitionValues, TypeInfo[] rowColumnTypeInfos) {
+        if (partitionValues != null) {
+            for (int i = 0; i < partitionColumnCount; i++) {
+                Object value = partitionValues[i];
+
+                int colIndex = dataColumnCount + i;
+                String partitionColumnName = rowColumnNames[colIndex];
+                PrimitiveTypeInfo primitiveTypeInfo = (PrimitiveTypeInfo) rowColumnTypeInfos[colIndex];
+                switch (primitiveTypeInfo.getPrimitiveCategory()) {
+                    case BOOLEAN: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            if (value instanceof Boolean) {
+                                lcv.fill((Boolean) value ? 1 : 0);
+                            }
+                        }
+                    }
+                    break;
+                    case BYTE: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill((Byte) value);
+                        }
+                    }
+                    break;
+                    case SHORT: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill((Short) value);
+                        }
+                    }
+                    break;
+                    case INT: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill((Integer) value);
+                        }
+                    }
+                    break;
+                    case LONG: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill((Long) value);
+                        }
+                    }
+                    break;
+                    case DATE: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill(DateWritableV2.dateToDays((Date) value));
+                        }
+                    }
+                    break;
+                    case TIMESTAMP: {
+                        TimestampColumnVector lcv = (TimestampColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill(((Timestamp) value).toSqlTimestamp());
+                        }
+                    }
+                    break;
+                    case INTERVAL_YEAR_MONTH: {
+                        LongColumnVector lcv = (LongColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(lcv);
+                        } else {
+                            lcv.fill(((HiveIntervalYearMonth) value).getTotalMonths());
+                        }
+                    }
+                    case INTERVAL_DAY_TIME: {
+                        IntervalDayTimeColumnVector icv = (IntervalDayTimeColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(icv);
+                        } else {
+                            icv.fill(((HiveIntervalDayTime) value));
+                        }
+                    }
+                    case FLOAT: {
+                        DoubleColumnVector dcv = (DoubleColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(dcv);
+                        } else {
+                            dcv.fill((Float) value);
+                        }
+                    }
+                    break;
+                    case DOUBLE: {
+                        DoubleColumnVector dcv = (DoubleColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(dcv);
+                        } else {
+                            dcv.fill((Double) value);
+                        }
+                    }
+                    break;
+                    case DECIMAL: {
+                        DecimalColumnVector dv = (DecimalColumnVector) cols[colIndex];
+                        if (value == null) {
+                            setNull(dv);
+                        } else {
+                            dv.fill((HiveDecimal) value);
+                        }
+                    }
+                    break;
+                    case BINARY: {
+                        BytesColumnVector bcv = (BytesColumnVector) cols[colIndex];
+                        byte[] bytes = (byte[]) value;
+                        if (bytes == null) {
+                            setNull(bcv);
+                        } else {
+                            bcv.fill(bytes);
+                        }
+                    }
+                    break;
+                    case STRING:
+                    case CHAR:
+                    case VARCHAR: {
+                        BytesColumnVector bcv = (BytesColumnVector) cols[colIndex];
+                        String sVal = value.toString();
+                        if (sVal == null) {
+                            setNull(bcv);
+                        } else {
+                            bcv.fill(sVal.getBytes());
+                        }
+                    }
+                    break;
+                    default:
+                        throw new RuntimeException(
+                                "Unable to recognize the partition type " + primitiveTypeInfo.getPrimitiveCategory()
+                                        + " for column " + partitionColumnName);
+                }
+            }
+        }
+    }
+
+    private void setNull(ColumnVector cv) {
+        cv.noNulls = false;
+        cv.isNull[0] = true;
+        cv.isRepeating = true;
+    }
+
+    @JsonProperty
+    public String[] getRowColumnNames() {
+        return rowColumnNames;
+    }
+
+    @JsonProperty
+    public DataTypePhysicalVariation[] getRowDataTypePhysicalVariations() {
+        return rowDataTypePhysicalVariations;
+    }
+
+    @JsonProperty
+    public int[] getDataColumnNums() {
+        return dataColumnNums;
+    }
+
+    @JsonProperty
+    public int getDataColumnCount() {
+        return dataColumnCount;
+    }
+
+    @JsonProperty
+    public int getPartitionColumnCount() {
+        return partitionColumnCount;
+    }
+
+    @JsonProperty
+    public int getVirtualColumnCount() {
+        return virtualColumnCount;
+    }
+
+    @JsonProperty
+    public VirtualColumn[] getNeededVirtualColumns() {
+        return neededVirtualColumns;
+    }
+
+    @JsonProperty
+    public String[] getScratchColumnTypeNames() {
+        return scratchColumnTypeNames;
+    }
+
+    @JsonProperty
+    public DataTypePhysicalVariation[] getScratchDataTypePhysicalVariations() {
+        return scratchDataTypePhysicalVariations;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataAdapter.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataAdapter.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataAdapter.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataAdapter.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,136 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.reader;
+
+import static org.apache.hadoop.hive.ql.omnidata.OmniDataUtils.addPartitionValues;
+
+import com.huawei.boostkit.omnidata.exception.OmniDataException;
+import com.huawei.boostkit.omnidata.model.TaskSource;
+import com.huawei.boostkit.omnidata.model.datasource.DataSource;
+import com.huawei.boostkit.omnidata.model.datasource.hdfs.HdfsOrcDataSource;
+import com.huawei.boostkit.omnidata.model.datasource.hdfs.HdfsParquetDataSource;
+import com.huawei.boostkit.omnidata.reader.impl.DataReaderImpl;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.TaskExecutionException;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.hive.ql.omnidata.decode.PageDeserializer;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.apache.hadoop.hive.ql.omnidata.status.NdpStatusManager;
+import org.apache.hadoop.mapred.FileSplit;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.Serializable;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Locale;
+import java.util.Properties;
+import java.util.Queue;
+
+/**
+ * Obtains data from OmniData through OmniDataAdapter and converts the data into Hive List<ColumnVector[]>.
+ */
+public class OmniDataAdapter implements Serializable {
+    private static final Logger LOGGER = LoggerFactory.getLogger(OmniDataAdapter.class);
+
+    /**
+     * The maximum number of retry times is 4.
+     */
+    private static final int TASK_FAILED_TIMES = 4;
+
+    private TaskSource taskSource;
+
+    private List<String> omniDataHosts;
+
+    private PageDeserializer deserializer;
+
+    private int taskTimeout;
+
+    public OmniDataAdapter(Configuration conf, FileSplit fileSplit, NdpPredicateInfo ndpPredicateInfo,
+                           PageDeserializer deserializer) {
+        this.deserializer = deserializer;
+        String path = fileSplit.getPath().toString();
+        long start = fileSplit.getStart();
+        long length = fileSplit.getLength();
+        // data source information, for connecting to data source.
+        DataSource dataSource;
+        if (ndpPredicateInfo.getDataFormat().toLowerCase(Locale.ENGLISH).contains("parquet")) {
+            dataSource = new HdfsParquetDataSource(path, start, length, false);
+        } else {
+            dataSource = new HdfsOrcDataSource(path, start, length, false);
+        }
+        if (ndpPredicateInfo.getHasPartitionColumn()) {
+            addPartitionValues(ndpPredicateInfo, path, HiveConf.getVar(conf, HiveConf.ConfVars.DEFAULTPARTITIONNAME));
+        }
+        this.omniDataHosts = NdpStatusManager.getOmniDataHosts(conf, fileSplit,
+                OmniDataConf.getOmniDataReplicationNum(conf));
+        this.taskSource = new TaskSource(dataSource, ndpPredicateInfo.getPredicate(), 1048576);
+        this.taskTimeout = OmniDataConf.getOmnidataClientTaskTimeout(conf);
+    }
+
+    public Queue<ColumnVector[]> getBatchFromOmniData() throws UnknownHostException {
+        Queue<ColumnVector[]> pages = new LinkedList<>();
+        int failedTimes = 0;
+        Properties properties = new Properties();
+        //  If the OmniData task fails due to an exception, the task will look for the next available OmniData host
+        for (String omniDataHost : omniDataHosts) {
+            String ipAddress = InetAddress.getByName(omniDataHost).getHostAddress();
+            properties.put("omnidata.client.target.list", ipAddress);
+            properties.put(OmniDataConf.OMNIDATA_CLIENT_TASK_TIMEOUT, taskTimeout);
+            DataReaderImpl<List<ColumnVector[]>> dataReader = null;
+            try {
+                dataReader = new DataReaderImpl<>(properties, taskSource, deserializer);
+                do {
+                    List<ColumnVector[]> page = dataReader.getNextPageBlocking();
+                    if (page != null) {
+                        pages.addAll(page);
+                    }
+                } while (!dataReader.isFinished());
+                dataReader.close();
+                break;
+            } catch (OmniDataException omniDataException) {
+                LOGGER.warn("OmniDataAdapter failed node info [hostname :{}]", omniDataHost);
+                failedTimes++;
+                pages.clear();
+                if (dataReader != null) {
+                    dataReader.close();
+                }
+            } catch (Exception e) {
+                LOGGER.error("OmniDataAdapter getBatchFromOmnidata() has error:", e);
+                failedTimes++;
+                pages.clear();
+                if (dataReader != null) {
+                    dataReader.close();
+                }
+            }
+        }
+        int retryTime = Math.min(TASK_FAILED_TIMES, omniDataHosts.size());
+        if (failedTimes >= retryTime) {
+            LOGGER.warn("No OmniData Server to connect, task has tried {} times.", retryTime);
+            throw new TaskExecutionException("No OmniData Server to connect");
+        }
+        return pages;
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataReader.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataReader.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataReader.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/reader/OmniDataReader.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,117 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.reader;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.omnidata.decode.PageDeserializer;
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpVectorizedRowBatchCtx;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.mapred.FileSplit;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.net.UnknownHostException;
+import java.util.LinkedList;
+import java.util.Queue;
+import java.util.concurrent.Callable;
+
+/**
+ * OmniDataReader for filter and agg optimization
+ *
+ * @since 2022-03-07
+ */
+public class OmniDataReader implements Callable<Queue<VectorizedRowBatch>> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(OmniDataReader.class);
+
+    private NdpPredicateInfo ndpPredicateInfo;
+
+    private NdpVectorizedRowBatchCtx ndpVectorizedRowBatchCtx;
+
+    private TypeInfo[] rowColumnTypeInfos;
+
+    private OmniDataAdapter omniDataAdapter;
+
+    private Object[] partitionValues = null;
+
+    public OmniDataReader(Configuration conf, FileSplit fileSplit, PageDeserializer deserializer,
+                          NdpPredicateInfo ndpPredicateInfo, TypeInfo[] rowColumnTypeInfos) {
+        this.ndpPredicateInfo = ndpPredicateInfo;
+        this.rowColumnTypeInfos = rowColumnTypeInfos;
+        this.omniDataAdapter = new OmniDataAdapter(conf, fileSplit, ndpPredicateInfo, deserializer);
+        if (ndpPredicateInfo.getNdpVectorizedRowBatchCtx() != null) {
+            this.ndpVectorizedRowBatchCtx = ndpPredicateInfo.getNdpVectorizedRowBatchCtx();
+            int partitionColumnCount = ndpVectorizedRowBatchCtx.getPartitionColumnCount();
+            if (partitionColumnCount > 0) {
+                partitionValues = new Object[partitionColumnCount];
+                // set partitionValues
+                NdpVectorizedRowBatchCtx.getPartitionValues(ndpVectorizedRowBatchCtx, conf, fileSplit, partitionValues,
+                        this.rowColumnTypeInfos);
+            }
+        }
+    }
+
+    @Override
+    public Queue<VectorizedRowBatch> call() throws UnknownHostException {
+        Queue<ColumnVector[]> pages = omniDataAdapter.getBatchFromOmniData();
+        return ndpPredicateInfo.getIsPushDownAgg()
+                ? createVectorizedRowBatchWithAgg(pages)
+                : createVectorizedRowBatch(pages);
+    }
+
+    private Queue<VectorizedRowBatch> createVectorizedRowBatchWithAgg(Queue<ColumnVector[]> pages) {
+        Queue<VectorizedRowBatch> rowBatches = new LinkedList<>();
+        while (!pages.isEmpty()) {
+            ColumnVector[] columnVectors = pages.poll();
+            int channelCount = columnVectors.length;
+            int positionCount = columnVectors[0].isNull.length;
+            VectorizedRowBatch rowBatch = new VectorizedRowBatch(channelCount, positionCount);
+            // agg: copy columnVectors to rowBatch.cols
+            System.arraycopy(columnVectors, 0, rowBatch.cols, 0, channelCount);
+            rowBatches.add(rowBatch);
+        }
+        return rowBatches;
+    }
+
+    private Queue<VectorizedRowBatch> createVectorizedRowBatch(Queue<ColumnVector[]> pages) {
+        Queue<VectorizedRowBatch> rowBatches = new LinkedList<>();
+        while (!pages.isEmpty()) {
+            ColumnVector[] columnVectors = pages.poll();
+            int channelCount = columnVectors.length;
+            int positionCount = columnVectors[0].isNull.length;
+            // creates a vectorized row batch and the column vectors
+            VectorizedRowBatch rowBatch = ndpVectorizedRowBatchCtx.createVectorizedRowBatch(rowColumnTypeInfos);
+            for (int i = 0; i < channelCount; i++) {
+                int columnId = ndpPredicateInfo.getOutputColumns().get(i);
+                rowBatch.cols[columnId] = columnVectors[i];
+            }
+            rowBatch.size = positionCount;
+            if (partitionValues != null) {
+                ndpVectorizedRowBatchCtx.addPartitionColsToBatch(rowBatch, partitionValues, rowColumnTypeInfos);
+            }
+            rowBatches.add(rowBatch);
+        }
+        return rowBatches;
+    }
+
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpObjectMapperProvider.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpObjectMapperProvider.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpObjectMapperProvider.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpObjectMapperProvider.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,114 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.serialize;
+
+import com.huawei.boostkit.omnidata.metadata.InternalTypeManager;
+import com.huawei.boostkit.omnidata.metadata.Metadata;
+import com.huawei.boostkit.omnidata.metadata.TypeDeserializer;
+import com.huawei.boostkit.omnidata.serialize.OmniDataBlockEncodingSerde;
+import com.huawei.boostkit.omnidata.serialize.OmniDataBlockJsonSerde;
+
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.core.JsonFactory;
+import com.fasterxml.jackson.databind.DeserializationFeature;
+import com.fasterxml.jackson.databind.MapperFeature;
+import com.fasterxml.jackson.databind.Module;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.SerializationFeature;
+import com.fasterxml.jackson.databind.module.SimpleModule;
+import com.fasterxml.jackson.datatype.guava.GuavaModule;
+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;
+import com.fasterxml.jackson.datatype.joda.JodaModule;
+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;
+
+import io.prestosql.spi.block.Block;
+import io.prestosql.spi.type.Type;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.HashSet;
+import java.util.Set;
+
+/**
+ * Object mapper provider
+ *
+ * @since 2020-07-31
+ */
+public class NdpObjectMapperProvider {
+    private static final Logger LOG = LoggerFactory.getLogger(NdpObjectMapperProvider.class);
+
+    private final JsonFactory jsonFactory;
+
+    private final Set<Module> modules = new HashSet<>();
+
+    public NdpObjectMapperProvider() {
+        this.jsonFactory = new JsonFactory();
+
+        modules.add(new Jdk8Module());
+        modules.add(new GuavaModule());
+        modules.add(new JavaTimeModule());
+        modules.add(new ParameterNamesModule());
+
+        SimpleModule module = new SimpleModule();
+        module.addSerializer(Block.class, new OmniDataBlockJsonSerde.Serializer(new OmniDataBlockEncodingSerde()));
+        module.addDeserializer(Block.class, new OmniDataBlockJsonSerde.Deserializer(new OmniDataBlockEncodingSerde()));
+        module.addDeserializer(Type.class, new TypeDeserializer(new InternalTypeManager(new Metadata())));
+        modules.add(module);
+
+        try {
+            getClass().getClassLoader().loadClass("org.joda.time.DateTime");
+            modules.add(new JodaModule());
+        } catch (ClassNotFoundException ignored) {
+            // ignore this exception
+            LOG.warn("Can't find class org.joda.time.DateTime.");
+        }
+    }
+
+    /**
+     * get Object mapper
+     *
+     * @return objectMapper
+     */
+    public ObjectMapper get() {
+        ObjectMapper objectMapper = new ObjectMapper(jsonFactory);
+        objectMapper.disable(DeserializationFeature.ACCEPT_FLOAT_AS_INT);
+        objectMapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
+        objectMapper.setDefaultPropertyInclusion(
+            JsonInclude.Value.construct(JsonInclude.Include.NON_ABSENT, JsonInclude.Include.ALWAYS));
+
+        objectMapper.disable(MapperFeature.AUTO_DETECT_GETTERS);
+        objectMapper.disable(MapperFeature.AUTO_DETECT_SETTERS);
+        objectMapper.disable(MapperFeature.AUTO_DETECT_CREATORS);
+        objectMapper.disable(MapperFeature.AUTO_DETECT_FIELDS);
+        objectMapper.disable(MapperFeature.USE_GETTERS_AS_SETTERS);
+        objectMapper.disable(MapperFeature.AUTO_DETECT_IS_GETTERS);
+        objectMapper.disable(MapperFeature.ALLOW_FINAL_FIELDS_AS_MUTATORS);
+        objectMapper.disable(MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS);
+        objectMapper.disable(MapperFeature.INFER_PROPERTY_MUTATORS);
+
+        for (Module module : modules) {
+            objectMapper.registerModule(module);
+        }
+
+        return objectMapper;
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpSerializationUtils.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpSerializationUtils.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpSerializationUtils.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/serialize/NdpSerializationUtils.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.serialize;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import org.apache.hadoop.hive.ql.omnidata.operator.predicate.NdpPredicateInfo;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+/**
+ * Npd serialization tool
+ *
+ * @since 2022-01-28
+ */
+public class NdpSerializationUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(NdpSerializationUtils.class);
+
+    public static String serializeNdpPredicateInfo(NdpPredicateInfo ndpPredicateInfo) {
+        try {
+            ObjectMapper mapper = new NdpObjectMapperProvider().get();
+            return mapper.writeValueAsString(ndpPredicateInfo);
+        } catch (JsonProcessingException e) {
+            LOG.error("serializeNdpPredicateInfo() failed", e);
+        }
+        return "";
+    }
+
+    public static NdpPredicateInfo deserializeNdpPredicateInfo(String predicateStr) {
+        try {
+            if (predicateStr != null && predicateStr.length() > 0) {
+                ObjectMapper mapper = new NdpObjectMapperProvider().get();
+                return mapper.readValue(predicateStr, NdpPredicateInfo.class);
+            } else {
+                return new NdpPredicateInfo(false);
+            }
+        } catch (IOException e) {
+            throw new RuntimeException("deserializeNdpPredicateInfo() failed", e);
+        }
+    }
+
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusInfo.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusInfo.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusInfo.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusInfo.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,76 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.status;
+
+/**
+ * OmniData node status
+ *
+ * @since 2021-03
+ */
+public class NdpStatusInfo {
+    private String datanodeHost;
+
+    private String version;
+
+    private double threshold;
+
+    private int runningTasks;
+
+    private int maxTasks;
+
+    public NdpStatusInfo(String datanodeHost, String version, double threshold, int runningTasks, int maxTasks) {
+        this.datanodeHost = datanodeHost;
+        this.version = version;
+        this.threshold = threshold;
+        this.runningTasks = runningTasks;
+        this.maxTasks = maxTasks;
+    }
+
+    public NdpStatusInfo() {
+    }
+
+    public String getDatanodeHost() {
+        return datanodeHost;
+    }
+
+    public String getVersion() {
+        return version;
+    }
+
+    public void setVersion(String version) {
+        this.version = version;
+    }
+
+    public double getThreshold() {
+        return threshold;
+    }
+
+    public void setThreshold(double threshold) {
+        this.threshold = threshold;
+    }
+
+    public int getRunningTasks() {
+        return runningTasks;
+    }
+
+    public int getMaxTasks() {
+        return maxTasks;
+    }
+}
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusManager.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusManager.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusManager.java	1970-01-01 08:00:00.000000000 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/omnidata/status/NdpStatusManager.java	2023-06-28 15:05:04.000000000 +0800
@@ -0,0 +1,286 @@
+/*
+ * Copyright (C) Huawei Technologies Co., Ltd. 2021-2022. All rights reserved.
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.omnidata.status;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import org.apache.curator.framework.CuratorFramework;
+import org.apache.curator.framework.CuratorFrameworkFactory;
+import org.apache.curator.framework.recipes.locks.InterProcessMutex;
+import org.apache.curator.retry.RetryForever;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.zookeeper.data.Stat;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * Operate Ndp zookeeper data
+ *
+ * @since 2021-03
+ */
+public class NdpStatusManager {
+    private NdpStatusManager() {
+    }
+
+    private static final Logger LOG = LoggerFactory.getLogger(NdpStatusManager.class);
+
+    public static final String NDP_DATANODE_HOSTNAMES = "hive.ndp.datanode.hostnames";
+
+    public static final String KRB5_LOGIN_CONF_KEY = "java.security.auth.login.config";
+
+    public static final String KRB5_CONF_KEY = "java.security.krb5.conf";
+
+    public static final String KRB5_SASL_CLIENT_CONF_KEY = "zookeeper.sasl.client";
+
+    public static final String LOGIN_CONFIG_FILE = "jaas.conf";
+
+    public static final String KRB5_CONFIG_FILE = "krb5.conf";
+
+    public static final String NDP_DATANODE_HOSTNAME_SEPARATOR = ",";
+
+    /**
+     * Get OmniData host resources data from ZooKeeper
+     *
+     * @param conf hive conf
+     * @return hostname -> ndp status
+     */
+    public static Map<String, NdpStatusInfo> getNdpZookeeperData(Configuration conf) {
+        Map<String, NdpStatusInfo> ndpMap = new HashMap<>();
+        String parentPath = OmniDataConf.getOmniDataZookeeperStatusNode(conf);
+        String quorumServer = OmniDataConf.getOmniDataZookeeperQuorumServer(conf);
+        String confPath = OmniDataConf.getOmniDataZookeeperConfPath(conf);
+        if (parentPath == null || quorumServer == null || confPath == null) {
+            LOG.error("OmniData Hive failed to get Zookeeper parameters, "
+                            + "please set the following parameters: {} {} {}",
+                    OmniDataConf.OMNIDATA_HIVE_ZOOKEEPER_QUORUM_SERVER, OmniDataConf.OMNIDATA_HIVE_ZOOKEEPER_STATUS_NODE,
+                    OmniDataConf.OMNIDATA_HIVE_ZOOKEEPER_CONF_PATH);
+            return ndpMap;
+        }
+        if (OmniDataConf.getOmniDataZookeeperSecurityEnabled(conf)) {
+            enableKrb5(confPath);
+        }
+        CuratorFramework zkClient = CuratorFrameworkFactory.builder()
+                .connectString(quorumServer)
+                .sessionTimeoutMs(OmniDataConf.getOmniDataZookeeperSessionTimeout(conf))
+                .connectionTimeoutMs(OmniDataConf.getOmniDataZookeeperConnectionTimeout(conf))
+                .retryPolicy(new RetryForever(OmniDataConf.getOmniDataZookeeperRetryInterval(conf)))
+                .build();
+        zkClient.start();
+        if (!verifyZookeeperPath(zkClient, parentPath)) {
+            return ndpMap;
+        }
+        InterProcessMutex lock = new InterProcessMutex(zkClient, parentPath);
+        try {
+            if (lock.acquire(OmniDataConf.getOmniDataZookeeperRetryInterval(conf), TimeUnit.MILLISECONDS)) {
+                List<String> childrenPaths = zkClient.getChildren().forPath(parentPath);
+                ObjectMapper mapper = new ObjectMapper();
+                for (String path : childrenPaths) {
+                    if (path.contains("-lock-")) {
+                        continue;
+                    }
+                    byte[] data = zkClient.getData().forPath(parentPath + "/" + path);
+                    NdpStatusInfo statusInfo = mapper.readValue(data, NdpStatusInfo.class);
+                    ndpMap.put(path, statusInfo);
+                }
+            }
+        } catch (Exception e) {
+            LOG.error("OmniData Hive failed to get host resources data from ZooKeeper", e);
+        } finally {
+            try {
+                lock.release();
+            } catch (Exception e) {
+                LOG.error("OmniData Hive failed to release OmniData lock from ZooKeeper", e);
+            }
+            zkClient.close();
+        }
+        return ndpMap;
+    }
+
+    private static boolean verifyZookeeperPath(CuratorFramework zkClient, String path) {
+        try {
+            // verify the path from ZooKeeper
+            Stat stat = zkClient.checkExists().forPath(path);
+            if (stat == null) {
+                LOG.error("OmniData Hive failed to get parent node from ZooKeeper");
+                return false;
+            }
+        } catch (Exception e) {
+            LOG.error("OmniData Hive failed to get host resources data from ZooKeeper", e);
+            return false;
+        }
+        return true;
+    }
+
+    private static void enableKrb5(String confPath) {
+        System.setProperty(KRB5_LOGIN_CONF_KEY, confPath + "/" + LOGIN_CONFIG_FILE);
+        System.setProperty(KRB5_CONF_KEY, confPath + "/" + KRB5_CONFIG_FILE);
+        System.setProperty(KRB5_SASL_CLIENT_CONF_KEY, "true");
+    }
+
+    /**
+     * Add the OmniData mapping relationship to the conf
+     *
+     * @param conf hive conf
+     * @param ndpStatusInfoMap hostname map:
+     * key: OmniData host
+     * value: datanode host
+     */
+    public static void setOmniDataHostToConf(Configuration conf, Map<String, NdpStatusInfo> ndpStatusInfoMap) {
+        List<String> dataNodeHosts = new ArrayList<>();
+        for (Map.Entry<String, NdpStatusInfo> info : ndpStatusInfoMap.entrySet()) {
+            String dataNodeHost = info.getValue().getDatanodeHost();
+            String omniDataHost = info.getKey();
+            // datanode host -> OmniData host
+            conf.set(dataNodeHost, omniDataHost);
+            dataNodeHosts.add(dataNodeHost);
+        }
+        conf.set(NDP_DATANODE_HOSTNAMES, String.join(NDP_DATANODE_HOSTNAME_SEPARATOR, dataNodeHosts));
+    }
+
+    /**
+     * One DataNode needs to be randomly selected from the available DataNodes and is not included in the excludeHosts.
+     *
+     * @param conf hive conf
+     * @param excludeDataNodeHosts excluded DataNode host
+     * @return random DataNode host
+     */
+    public static String getRandomAvailableDataNodeHost(Configuration conf, List<String> excludeDataNodeHosts) {
+        List<String> dataNodeHosts = new ArrayList<>(
+                Arrays.asList(conf.get(NdpStatusManager.NDP_DATANODE_HOSTNAMES).split(NDP_DATANODE_HOSTNAME_SEPARATOR)));
+        if (excludeDataNodeHosts.size() >= dataNodeHosts.size()) {
+            return "";
+        }
+        Iterator<String> dataNodeIt = dataNodeHosts.iterator();
+        while (dataNodeIt.hasNext()) {
+            String dataNode = dataNodeIt.next();
+            excludeDataNodeHosts.forEach(edn -> {
+                if (dataNode.equals(edn)) {
+                    dataNodeIt.remove();
+                }
+            });
+        }
+        int randomIndex = (int) (Math.random() * dataNodeHosts.size());
+        return dataNodeHosts.get(randomIndex);
+    }
+
+    /**
+     * If the number of Replication is not specified, the default value is 3
+     *
+     * @param conf hive config
+     * @param fileSplit fileSplit
+     * @return OmniData hosts
+     */
+    public static List<String> getOmniDataHosts(Configuration conf, FileSplit fileSplit) {
+        return getOmniDataHosts(conf, fileSplit, 3);
+    }
+
+    /**
+     * get the OmniData hosts.
+     *
+     * @param conf hive config
+     * @param fileSplit fileSplit
+     * @param ndpReplicationNum hdfs replication
+     * @return OmniData hosts
+     */
+    public static List<String> getOmniDataHosts(Configuration conf, FileSplit fileSplit, int ndpReplicationNum) {
+        List<String> omniDataHosts = new ArrayList<>();
+        List<String> dataNodeHosts = getDataNodeHosts(conf, fileSplit, ndpReplicationNum);
+        // shuffle
+        Collections.shuffle(dataNodeHosts);
+        dataNodeHosts.forEach(dn -> {
+            // possibly null
+            if (conf.get(dn) != null) {
+                omniDataHosts.add(conf.get(dn));
+            }
+        });
+        // add a random available datanode
+        String randomDataNodeHost = NdpStatusManager.getRandomAvailableDataNodeHost(conf, dataNodeHosts);
+        if (randomDataNodeHost.length() > 0 && conf.get(randomDataNodeHost) != null) {
+            omniDataHosts.add(conf.get(randomDataNodeHost));
+        }
+        return omniDataHosts;
+    }
+
+    /**
+     * get the DataNode hosts.
+     *
+     * @param conf hive config
+     * @param fileSplit fileSplit
+     * @param ndpReplicationNum hdfs replication
+     * @return DataNode hosts
+     */
+    public static List<String> getDataNodeHosts(Configuration conf, FileSplit fileSplit, int ndpReplicationNum) {
+        List<String> hosts = new ArrayList<>();
+        try {
+            BlockLocation[] blockLocations = fileSplit.getPath()
+                    .getFileSystem(conf)
+                    .getFileBlockLocations(fileSplit.getPath(), fileSplit.getStart(), fileSplit.getLength());
+            for (BlockLocation block : blockLocations) {
+                addHostsByBlock(conf, ndpReplicationNum, block, hosts);
+            }
+        } catch (IOException e) {
+            LOG.error("NdpStatusManager getDataNodeHosts() failed", e);
+        }
+        return hosts;
+    }
+
+    /**
+     * Add datanode by block
+     *
+     * @param conf hive config
+     * @param ndpReplicationNum hdfs replication
+     * @param block hdfs block
+     * @param hosts DataNode hosts
+     * @throws IOException block.getHosts()
+     */
+    public static void addHostsByBlock(Configuration conf, int ndpReplicationNum, BlockLocation block,
+                                       List<String> hosts) throws IOException {
+        for (String host : block.getHosts()) {
+            if ("localhost".equals(host)) {
+                List<String> dataNodeHosts = new ArrayList<>(Arrays.asList(
+                        conf.get(NdpStatusManager.NDP_DATANODE_HOSTNAMES)
+                                .split(NdpStatusManager.NDP_DATANODE_HOSTNAME_SEPARATOR)));
+                if (dataNodeHosts.size() > ndpReplicationNum) {
+                    hosts.addAll(dataNodeHosts.subList(0, ndpReplicationNum));
+                } else {
+                    hosts.addAll(dataNodeHosts);
+                }
+            } else {
+                hosts.add(host);
+            }
+            if (ndpReplicationNum == hosts.size()) {
+                return;
+            }
+        }
+    }
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java	2023-07-29 19:32:52.408665640 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java	2023-06-28 15:05:04.000000000 +0800
@@ -68,6 +68,7 @@
 import org.apache.hadoop.hive.ql.lib.RuleRegExp;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.omnidata.config.OmniDataConf;
 import org.apache.hadoop.hive.ql.optimizer.ConstantPropagate;
 import org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.ConstantPropagateOption;
 import org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin;
@@ -81,6 +82,7 @@
 import org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits;
 import org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer;
 import org.apache.hadoop.hive.ql.optimizer.physical.CrossProductHandler;
+import org.apache.hadoop.hive.ql.omnidata.physical.NdpPlanResolver;
 import org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile;
 import org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider;
 import org.apache.hadoop.hive.ql.optimizer.physical.LlapPreVectorizationPass;
@@ -710,8 +712,15 @@
       new AnnotateRunTimeStatsOptimizer().resolve(physicalCtx);
     }
 
+    if (OmniDataConf.getOmniDataEnabled(conf)) {
+      // Tez OmniData entrance
+      NdpPlanResolver ndpPlanResolver = new NdpPlanResolver();
+      ndpPlanResolver.resolve(physicalCtx);
+    } else {
+      LOG.debug("Skipping OmniData pushdown");
+    }
+
     perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, "optimizeTaskPlan");
-    return;
   }
 
   private static class SMBJoinOpProcContext implements NodeProcessorCtx {
diff -Nur hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
--- hive-rel-release-3.1.3/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java	2023-07-29 19:32:52.424665640 +0800
+++ hive-rel-release-3.1.3-master/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java	2023-06-28 15:05:04.000000000 +0800
@@ -18,14 +18,6 @@
 
 package org.apache.hadoop.hive.ql.plan;
 
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.BitSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
 import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.metadata.Table;
@@ -38,6 +30,14 @@
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.BitSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
 /**
  * Table Scan Descriptor Currently, data is only read from a base source as part
  * of map-reduce framework. So, nothing is stored in the descriptor. But, more
@@ -74,6 +74,11 @@
   private boolean statsReliable;
   private String tmpStatsDir;
 
+  private String ndpPredicateInfoStr;
+  private TypeInfo[] rowColumnTypeInfos;
+  private boolean isPushDownFilter = false;
+  private boolean isPushDownAgg = false;
+  private double omniDataSelectivity = 1.0d;
   private ExprNodeGenericFuncDesc filterExpr;
   private Serializable filterObject;
   private String serializedFilterExpr;
@@ -92,13 +97,13 @@
   private transient List<String> referencedColumns;
 
   public static final String FILTER_EXPR_CONF_STR =
-      "hive.io.filter.expr.serialized";
+          "hive.io.filter.expr.serialized";
 
   public static final String FILTER_TEXT_CONF_STR =
-      "hive.io.filter.text";
+          "hive.io.filter.text";
 
   public static final String FILTER_OBJECT_CONF_STR =
-      "hive.io.filter.object";
+          "hive.io.filter.object";
 
   // input file name (big) to bucket number
   private Map<String, Integer> bucketFileNameMapping;
@@ -228,15 +233,53 @@
     return PlanUtils.getExprListString(Arrays.asList(filterExpr));
   }
 
+  public String getNdpPredicateInfoStr() {
+    return ndpPredicateInfoStr;
+  }
+
+  public void setNdpPredicateInfoStr(String ndpPredicateInfoStr) {
+    this.ndpPredicateInfoStr = ndpPredicateInfoStr;
+  }
+
+  public TypeInfo[] getRowColumnTypeInfos() {
+    return rowColumnTypeInfos;
+  }
+
+  public void setRowColumnTypeInfos(TypeInfo[] rowColumnTypeInfos) {
+    this.rowColumnTypeInfos = rowColumnTypeInfos;
+  }
+
+  public boolean isPushDownFilter() {
+    return isPushDownFilter;
+  }
+
+  public void setPushDownFilter(boolean isPushDownFilter) {
+    this.isPushDownFilter = isPushDownFilter;
+  }
+
+  public boolean isPushDownAgg() {
+    return isPushDownAgg;
+  }
+
+  public void setPushDownAgg(boolean isPushDownAgg) {
+    this.isPushDownAgg = isPushDownAgg;
+  }
+
+  public double getOmniDataSelectivity() {
+    return omniDataSelectivity;
+  }
+
+  public void setOmniDataSelectivity(double omniDataSelectivity) {
+    this.omniDataSelectivity = omniDataSelectivity;
+  }
+
   // @Signature // XXX
   public ExprNodeGenericFuncDesc getFilterExpr() {
     return filterExpr;
   }
-
   public void setFilterExpr(ExprNodeGenericFuncDesc filterExpr) {
     this.filterExpr = filterExpr;
   }
-
   public Serializable getFilterObject() {
     return filterObject;
   }
@@ -482,7 +525,7 @@
     private final VectorTableScanDesc vectorTableScanDesc;
 
     public TableScanOperatorExplainVectorization(TableScanDesc tableScanDesc,
-        VectorTableScanDesc vectorTableScanDesc) {
+                                                 VectorTableScanDesc vectorTableScanDesc) {
       // Native vectorization supported.
       super(vectorTableScanDesc, true);
       this.tableScanDesc = tableScanDesc;
@@ -503,13 +546,13 @@
       }
 
       DataTypePhysicalVariation[] projectedColumnDataTypePhysicalVariations =
-          vectorTableScanDesc.getProjectedColumnDataTypePhysicalVariations();
+              vectorTableScanDesc.getProjectedColumnDataTypePhysicalVariations();
 
       return BaseExplainVectorization.getColumnAndTypes(
-          projectionColumns,
-          projectedColumnNames,
-          projectedColumnTypeInfos,
-          projectedColumnDataTypePhysicalVariations).toString();
+              projectionColumns,
+              projectedColumnNames,
+              projectedColumnTypeInfos,
+              projectedColumnDataTypePhysicalVariations).toString();
     }
   }
 
@@ -539,9 +582,9 @@
     if (getClass().getName().equals(other.getClass().getName())) {
       TableScanDesc otherDesc = (TableScanDesc) other;
       return Objects.equals(getQualifiedTable(), otherDesc.getQualifiedTable()) &&
-          ExprNodeDescUtils.isSame(getFilterExpr(), otherDesc.getFilterExpr()) &&
-          getRowLimit() == otherDesc.getRowLimit() &&
-          isGatherStats() == otherDesc.isGatherStats();
+              ExprNodeDescUtils.isSame(getFilterExpr(), otherDesc.getFilterExpr()) &&
+              getRowLimit() == otherDesc.getRowLimit() &&
+              isGatherStats() == otherDesc.isGatherStats();
     }
     return false;
   }
@@ -549,4 +592,4 @@
   public boolean isFullAcidTable() {
     return isTranscationalTable() && !getAcidOperationalProperties().isInsertOnly();
   }
-}
+}
\ No newline at end of file
diff -Nur hive-rel-release-3.1.3/ql/src/test/scripts/testgrep_win.bat hive-rel-release-3.1.3-master/ql/src/test/scripts/testgrep_win.bat
--- hive-rel-release-3.1.3/ql/src/test/scripts/testgrep_win.bat	2023-07-29 19:32:52.996665640 +0800
+++ hive-rel-release-3.1.3-master/ql/src/test/scripts/testgrep_win.bat	2023-06-28 15:05:04.000000000 +0800
@@ -1,17 +1,17 @@
-@REM Licensed to the Apache Software Foundation (ASF) under one
-@REM or more contributor license agreements.  See the NOTICE file
-@REM distributed with this work for additional information
-@REM regarding copyright ownership.  The ASF licenses this file
-@REM to you under the Apache License, Version 2.0 (the
-@REM "License"); you may not use this file except in compliance
-@REM with the License.  You may obtain a copy of the License at
-@REM
-@REM     http://www.apache.org/licenses/LICENSE-2.0
-@REM
-@REM Unless required by applicable law or agreed to in writing, software
-@REM distributed under the License is distributed on an "AS IS" BASIS,
-@REM WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-@REM See the License for the specific language governing permissions and
-@REM limitations under the License.
-
-findstr "10.*"
+@REM Licensed to the Apache Software Foundation (ASF) under one
+@REM or more contributor license agreements.  See the NOTICE file
+@REM distributed with this work for additional information
+@REM regarding copyright ownership.  The ASF licenses this file
+@REM to you under the Apache License, Version 2.0 (the
+@REM "License"); you may not use this file except in compliance
+@REM with the License.  You may obtain a copy of the License at
+@REM
+@REM     http://www.apache.org/licenses/LICENSE-2.0
+@REM
+@REM Unless required by applicable law or agreed to in writing, software
+@REM distributed under the License is distributed on an "AS IS" BASIS,
+@REM WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+@REM See the License for the specific language governing permissions and
+@REM limitations under the License.
+
+findstr "10.*"
diff -Nur hive-rel-release-3.1.3/README.md hive-rel-release-3.1.3-master/README.md
--- hive-rel-release-3.1.3/README.md	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/README.md	1970-01-01 08:00:00.000000000 +0800
@@ -1,110 +0,0 @@
-Apache Hive (TM)
-================
-[![Master Build Status](https://travis-ci.org/apache/hive.svg?branch=master)](https://travis-ci.org/apache/hive/branches)
-[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.hive/hive/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.hive%22)
-
-The Apache Hive (TM) data warehouse software facilitates reading,
-writing, and managing large datasets residing in distributed storage
-using SQL. Built on top of Apache Hadoop (TM), it provides:
-
-* Tools to enable easy access to data via SQL, thus enabling data
-  warehousing tasks such as extract/transform/load (ETL), reporting,
-  and data analysis
-
-* A mechanism to impose structure on a variety of data formats
-
-* Access to files stored either directly in Apache HDFS (TM) or in other
-  data storage systems such as Apache HBase (TM)
-
-* Query execution using Apache Hadoop MapReduce, Apache Tez
-  or Apache Spark frameworks.
-
-Hive provides standard SQL functionality, including many of the later
-2003 and 2011 features for analytics.  These include OLAP functions,
-subqueries, common table expressions, and more.  Hive's SQL can also be
-extended with user code via user defined functions (UDFs), user defined
-aggregates (UDAFs), and user defined table functions (UDTFs).
-
-Hive users have a choice of 3 runtimes when executing SQL queries.
-Users can choose between Apache Hadoop MapReduce, Apache Tez or
-Apache Spark frameworks as their execution backend. MapReduce is a
-mature framework that is proven at large scales. However, MapReduce
-is a purely batch framework, and queries using it may experience
-higher latencies (tens of seconds), even over small datasets. Apache
-Tez is designed for interactive query, and has substantially reduced
-overheads versus MapReduce. Apache Spark is a cluster computing
-framework that's built outside of MapReduce, but on top of HDFS,
-with a notion of composable and transformable distributed collection
-of items called Resilient Distributed Dataset (RDD) which allows
-processing and analysis without traditional intermediate stages that
-MapReduce introduces.
-
-Users are free to switch back and forth between these frameworks
-at any time. In each case, Hive is best suited for use cases
-where the amount of data processed is large enough to require a
-distributed system.
-
-Hive is not designed for online transaction processing. It is best used
-for traditional data warehousing tasks.  Hive is designed to maximize
-scalability (scale out with more machines added dynamically to the Hadoop
-cluster), performance, extensibility, fault-tolerance, and
-loose-coupling with its input formats.
-
-
-General Info
-============
-
-For the latest information about Hive, please visit out website at:
-
-  http://hive.apache.org/
-
-
-Getting Started
-===============
-
-- Installation Instructions and a quick tutorial:
-  https://cwiki.apache.org/confluence/display/Hive/GettingStarted
-
-- A longer tutorial that covers more features of HiveQL:
-  https://cwiki.apache.org/confluence/display/Hive/Tutorial
-
-- The HiveQL Language Manual:
-  https://cwiki.apache.org/confluence/display/Hive/LanguageManual
-
-
-Requirements
-============
-
-- Java 1.8
-
-- Hadoop 3.x
-
-
-Upgrading from older versions of Hive
-=====================================
-
-- Hive includes changes to the MetaStore schema. If
-  you are upgrading from an earlier version of Hive it is imperative
-  that you upgrade the MetaStore schema by running the appropriate
-  schema upgrade scripts located in the scripts/metastore/upgrade
-  directory.
-
-- We have provided upgrade scripts for MySQL, PostgreSQL, Oracle,
-  Microsoft SQL Server, and Derby databases. If you are using a
-  different database for your MetaStore you will need to provide
-  your own upgrade script.
-
-Useful mailing lists
-====================
-
-1. user@hive.apache.org - To discuss and ask usage questions. Send an
-   empty email to user-subscribe@hive.apache.org in order to subscribe
-   to this mailing list.
-
-2. dev@hive.apache.org - For discussions about code, design and features.
-   Send an empty email to dev-subscribe@hive.apache.org in order to
-   subscribe to this mailing list.
-
-3. commits@hive.apache.org - In order to monitor commits to the source
-   repository. Send an empty email to commits-subscribe@hive.apache.org
-   in order to subscribe to this mailing list.
diff -Nur hive-rel-release-3.1.3/RELEASE_NOTES.txt hive-rel-release-3.1.3-master/RELEASE_NOTES.txt
--- hive-rel-release-3.1.3/RELEASE_NOTES.txt	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/RELEASE_NOTES.txt	1970-01-01 08:00:00.000000000 +0800
@@ -1,10 +0,0 @@
-Release Notes - Hive - Version 3.1.3
-
-** Bug
-    * [HIVE-22405] - Add ColumnVector support for ProlepticCalendar
-    * [HIVE-22407] - Hive metastore upgrade scripts have incorrect (or outdated) comment syntax
-    * [HIVE-22410] - CachedStore Prewarm Failure NPE
-    * [HIVE-22589] - Add storage support for ProlepticCalendar
-** Improvement
-    * [HIVE-22241] - Implement UDF to interpret date/timestamp using its internal representation and Gregorian-Julian hybrid calendar
-    * [HIVE-22384] - Hive MetaStore CachedStore Funcion Cache Dev
diff -Nur hive-rel-release-3.1.3/.reviewboardrc hive-rel-release-3.1.3-master/.reviewboardrc
--- hive-rel-release-3.1.3/.reviewboardrc	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.reviewboardrc	1970-01-01 08:00:00.000000000 +0800
@@ -1,33 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-# 
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-
-# This file makes it easier to post review requests to ReviewBoard.
-# Assuming you already have RBTools installed you can post a review
-# request with the following command:
-#
-# % rbt post
-#
-# The RBTools docs (including installation instructions) are located here:
-# http://www.reviewboard.org/docs/rbtools
-#
-# Note: This may not work correctly if $HOME/.reviewboardrc exists.
-
-REPOSITORY='hive-git'
-REVIEWBOARD_URL='https://reviews.apache.org'
-TRACKING_BRANCH='origin/branch-3.1'
-TARGET_GROUPS='hive'
-GUESS_FIELDS='yes'
diff -Nur hive-rel-release-3.1.3/.travis.yml hive-rel-release-3.1.3-master/.travis.yml
--- hive-rel-release-3.1.3/.travis.yml	2023-07-29 19:32:52.300665640 +0800
+++ hive-rel-release-3.1.3-master/.travis.yml	1970-01-01 08:00:00.000000000 +0800
@@ -1,46 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# https://docs.travis-ci.com/user/ci-environment/
-# trusty - 7.5GB memory and 2 cores
-sudo: required
-dist: trusty
-
-# travis performs a shallow clone by default, in case of any issues
-# that requires full git history, enable this
-# before_install: git fetch --unshallow
-
-language: java
-jdk:
-  - oraclejdk8
-
-# disabling cache for /home/travis/.m2/repository/org/apache/hive/hive-jdbc/3.0.0-SNAPSHOT/hive-jdbc-3.0.0-SNAPSHOT-standalone.jar (Permission denied)
-#cache:
-#  directories:
-#  - $HOME/.m2
-
-env:
-  MAVEN_SKIP_RC=true
-  MAVEN_OPTS="-Xmx2g"
-
-# workaround added: https://github.com/travis-ci/travis-ci/issues/4629
-before_install:
-  - sed -i.bak -e 's|https://nexus.codehaus.org/snapshots/|https://oss.sonatype.org/content/repositories/codehaus-snapshots/|g' ~/.m2/settings.xml
-
-
-install: true
-
-script: mvn clean install -DskipTests -q -Pitests
